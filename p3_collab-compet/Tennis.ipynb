{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.10000000149011612\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.09000000171363354\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.09000000171363354\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TennismEnv(gym.Env):\n",
    "    \"\"\"Custom Tennis Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(TennismEnv, self).__init__()    # Define action and observation space\n",
    "        self.tennis_env=env\n",
    "        # They must be gym.spaces objects    # Example when using discrete actions:\n",
    "        self.action_space = [spaces.Box(shape= [4], low=-1, high=1)]    # Example for using image as input:\n",
    "#         self.observation_space = [spaces.Box(low=-15, high=15, shape=\n",
    "#                         [24], dtype=np.float64), spaces.Box(low=-20, high=20, shape=\n",
    "#                         [24], dtype=np.float64)]\n",
    "        self.observation_space = [spaces.Box(low=-20, high=20, shape=\n",
    "                        [48], dtype=np.float64)]\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = self.tennis_env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done \n",
    "#         print(next_states[0]+next_states[1])\n",
    "        return list(np.array(next_states).flatten()), np.array(rewards).astype('float'), any(dones), None\n",
    "    # Execute one time step within the environment\n",
    "    def reset(self):\n",
    "        env_info = self.tennis_env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "#         print(env_info.vector_observations)\n",
    "        return env_info.vector_observations.flatten()\n",
    "\n",
    "#     # Reset the state of the environment to an initial state\n",
    "#     def render(self, mode='human', close=False):\n",
    "#     # Render the environment to the screen\n",
    "#     ...\n",
    "len(TennismEnv(env).step(actions)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following code cell was taken and customly adjusted to support MARL from ptan library source code. GitHub link: https://github.com/Shmuma/ptan.git \n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from ptan_util.agent import BaseAgent\n",
    "from ptan_util.common import utils\n",
    "\n",
    "# one single experience step\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'done'])\n",
    "\n",
    "\n",
    "class ExperienceSource:\n",
    "    \"\"\"\n",
    "    Simple n-step experience source using single or multiple environments\n",
    "\n",
    "    Every experience contains n list of Experience entries\n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, steps_count=2, steps_delta=1, vectorized=False):\n",
    "        \"\"\"\n",
    "        Create simple experience source\n",
    "        :param env: environment or list of environments to be used\n",
    "        :param agent: callable to convert batch of states into actions to take\n",
    "        :param steps_count: count of steps to track for every experience chain\n",
    "        :param steps_delta: how many steps to do between experience items\n",
    "        :param vectorized: support of vectorized envs from OpenAI universe\n",
    "        \"\"\"\n",
    "        assert isinstance(env, (gym.Env, list, tuple))\n",
    "#         assert isinstance(agent, BaseAgent)\n",
    "        assert isinstance(steps_count, int)\n",
    "        assert steps_count >= 1\n",
    "        assert isinstance(vectorized, bool)\n",
    "        if isinstance(env, (list, tuple)):\n",
    "            self.pool = env\n",
    "        else:\n",
    "            self.pool = [env]\n",
    "        self.agent = agent\n",
    "        self.steps_count = steps_count\n",
    "        self.steps_delta = steps_delta\n",
    "        self.total_rewards = []\n",
    "        self.total_steps = []\n",
    "        self.vectorized = vectorized\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, agent_states, histories, cur_rewards, cur_steps = [], [], [], [], []\n",
    "        env_lens = []\n",
    "        for env in self.pool:\n",
    "            obs = env.reset()\n",
    "            # if the environment is vectorized, all it's output is lists of results.\n",
    "            # Details are here: https://github.com/openai/universe/blob/master/doc/env_semantics.rst\n",
    "            if self.vectorized:\n",
    "                obs_len = len(obs)\n",
    "                states.extend(obs)\n",
    "            else:\n",
    "                obs_len = 1\n",
    "                states.append(obs)\n",
    "            env_lens.append(obs_len)\n",
    "\n",
    "            for _ in range(obs_len):\n",
    "                histories.append(deque(maxlen=self.steps_count))\n",
    "                cur_rewards.append(0.0)\n",
    "                cur_steps.append(0)\n",
    "                agent_states.append(self.agent.initial_state())\n",
    "\n",
    "        iter_idx = 0\n",
    "        while True:\n",
    "            actions = [None] * len(states)\n",
    "            states_input = []\n",
    "            states_indices = []\n",
    "            for idx, state in enumerate(states):\n",
    "                if state is None:\n",
    "                    actions[idx] = self.pool[0].action_space.sample()  # assume that all envs are from the same family\n",
    "                else:\n",
    "                    states_input.append(state)\n",
    "                    states_indices.append(idx)\n",
    "            if states_input:\n",
    "                states_actions, new_agent_states = self.agent(states_input, agent_states)\n",
    "                for idx, action in enumerate(states_actions):\n",
    "                    g_idx = states_indices[idx]\n",
    "                    actions[g_idx] = action\n",
    "                    agent_states[g_idx] = new_agent_states[idx]\n",
    "            grouped_actions = _group_list(actions, env_lens)\n",
    "\n",
    "            global_ofs = 0\n",
    "            for env_idx, (env, action_n) in enumerate(zip(self.pool, grouped_actions)):\n",
    "                if self.vectorized:\n",
    "                    next_state_n, r_n, is_done_n, _ = env.step(action_n)\n",
    "                else:\n",
    "                    next_state, r, is_done, _ = env.step(action_n[0])\n",
    "                    next_state_n, r_n, is_done_n = [next_state], [r], [is_done]\n",
    "\n",
    "                for ofs, (action, next_state, r, is_done) in enumerate(zip(action_n, next_state_n, r_n, is_done_n)):\n",
    "                    idx = global_ofs + ofs\n",
    "                    state = states[idx]\n",
    "                    history = histories[idx]\n",
    "\n",
    "                    cur_rewards[idx] += r\n",
    "                    cur_steps[idx] += 1\n",
    "                    if state is not None:\n",
    "                        history.append(Experience(state=state, action=action, reward=r, done=is_done))\n",
    "                    if len(history) == self.steps_count and iter_idx % self.steps_delta == 0:\n",
    "                        yield tuple(history)\n",
    "                    states[idx] = next_state\n",
    "                    if is_done:\n",
    "                        # in case of very short episode (shorter than our steps count), send gathered history\n",
    "                        if 0 < len(history) < self.steps_count:\n",
    "                            yield tuple(history)\n",
    "                        # generate tail of history\n",
    "                        while len(history) > 1:\n",
    "                            history.popleft()\n",
    "                            yield tuple(history)\n",
    "                        self.total_rewards.append(cur_rewards[idx])\n",
    "                        self.total_steps.append(cur_steps[idx])\n",
    "                        cur_rewards[idx] = 0.0\n",
    "                        cur_steps[idx] = 0\n",
    "                        # vectorized envs are reset automatically\n",
    "                        states[idx] = env.reset() if not self.vectorized else None\n",
    "                        agent_states[idx] = self.agent.initial_state()\n",
    "                        history.clear()\n",
    "                global_ofs += len(action_n)\n",
    "            iter_idx += 1\n",
    "\n",
    "    def pop_total_rewards(self):\n",
    "        r = self.total_rewards\n",
    "        if r:\n",
    "            self.total_rewards = []\n",
    "            self.total_steps = []\n",
    "        return r\n",
    "\n",
    "    def pop_rewards_steps(self):\n",
    "        res = list(zip(self.total_rewards, self.total_steps))\n",
    "        if res:\n",
    "            self.total_rewards, self.total_steps = [], []\n",
    "        return res\n",
    "\n",
    "\n",
    "def _group_list(items, lens):\n",
    "    \"\"\"\n",
    "    Unflat the list of items by lens\n",
    "    :param items: list of items\n",
    "    :param lens: list of integers\n",
    "    :return: list of list of items grouped by lengths\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    cur_ofs = 0\n",
    "    for g_len in lens:\n",
    "        res.append(items[cur_ofs:cur_ofs+g_len])\n",
    "        cur_ofs += g_len\n",
    "    return res\n",
    "\n",
    "\n",
    "# those entries are emitted from ExperienceSourceFirstLast. Reward is discounted over the trajectory piece\n",
    "ExperienceFirstLast = collections.namedtuple('ExperienceFirstLast', ('state', 'action', 'reward', 'last_state'))\n",
    "\n",
    "\n",
    "class ExperienceSourceFirstLast(ExperienceSource):\n",
    "    \"\"\"\n",
    "    This is a wrapper around ExperienceSource to prevent storing full trajectory in replay buffer when we need\n",
    "    only first and last states. For every trajectory piece it calculates discounted reward and emits only first\n",
    "    and last states and action taken in the first state.\n",
    "\n",
    "    If we have partial trajectory at the end of episode, last_state will be None\n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, gamma, steps_count=1, steps_delta=1, vectorized=False):\n",
    "        assert isinstance(gamma, float)\n",
    "        super(ExperienceSourceFirstLast, self).__init__(env, agent, steps_count+1, steps_delta, vectorized=vectorized)\n",
    "        self.gamma = gamma\n",
    "        self.steps = steps_count\n",
    "\n",
    "    def __iter__(self):\n",
    "        for exp in super(ExperienceSourceFirstLast, self).__iter__():\n",
    "            if exp[-1].done and len(exp) <= self.steps:\n",
    "                last_state = None\n",
    "                elems = exp\n",
    "            else:\n",
    "                last_state = exp[-1].state\n",
    "                elems = exp[:-1]\n",
    "            total_reward = 0.0\n",
    "            for e in reversed(elems):\n",
    "                total_reward *= self.gamma\n",
    "                total_reward += e.reward\n",
    "            yield ExperienceFirstLast(state=exp[0].state, action=exp[0].action,\n",
    "                                      reward=total_reward, last_state=last_state)\n",
    "\n",
    "\n",
    "def discount_with_dones(rewards, dones, gamma):\n",
    "    discounted = []\n",
    "    r = 0\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        r = reward + gamma*r*(1.-done)\n",
    "        discounted.append(r)\n",
    "    return discounted[::-1]\n",
    "\n",
    "\n",
    "class ExperienceSourceRollouts:\n",
    "    \"\"\"\n",
    "    N-step rollout experience source following A3C rollouts scheme. Have to be used with agent,\n",
    "    keeping the value in its state (for example, agent.ActorCriticAgent).\n",
    "\n",
    "    Yields batches of num_envs * n_steps samples with the following arrays:\n",
    "    1. observations\n",
    "    2. actions\n",
    "    3. discounted rewards, with values approximation\n",
    "    4. values\n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, gamma, steps_count=5):\n",
    "        \"\"\"\n",
    "        Constructs the rollout experience source\n",
    "        :param env: environment or list of environments to be used\n",
    "        :param agent: callable to convert batch of states into actions\n",
    "        :param steps_count: how many steps to perform rollouts\n",
    "        \"\"\"\n",
    "        assert isinstance(env, (gym.Env, list, tuple))\n",
    "#         assert isinstance(agent, BaseAgent)\n",
    "        assert isinstance(gamma, float)\n",
    "        assert isinstance(steps_count, int)\n",
    "        assert steps_count >= 1\n",
    "\n",
    "        if isinstance(env, (list, tuple)):\n",
    "            self.pool = env\n",
    "        else:\n",
    "            self.pool = [env]\n",
    "        self.agent = agent\n",
    "        self.gamma = gamma\n",
    "        self.steps_count = steps_count\n",
    "        self.total_rewards = []\n",
    "        self.total_steps = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        pool_size = len(self.pool)\n",
    "        states = [np.array(e.reset()) for e in self.pool]\n",
    "        mb_states = np.zeros((pool_size, self.steps_count) + states[0].shape, dtype=states[0].dtype)\n",
    "        mb_rewards = np.zeros((pool_size, self.steps_count), dtype=np.float32)\n",
    "        mb_values = np.zeros((pool_size, self.steps_count), dtype=np.float32)\n",
    "        mb_actions = np.zeros((pool_size, self.steps_count), dtype=np.int64)\n",
    "        mb_dones = np.zeros((pool_size, self.steps_count), dtype=np.bool)\n",
    "        total_rewards = [0.0] * pool_size\n",
    "        total_steps = [0] * pool_size\n",
    "        agent_states = None\n",
    "        step_idx = 0\n",
    "\n",
    "        while True:\n",
    "            actions, agent_states = self.agent(states, agent_states)\n",
    "            rewards = []\n",
    "            dones = []\n",
    "            new_states = []\n",
    "            for env_idx, (e, action) in enumerate(zip(self.pool, actions)):\n",
    "                o, r, done, _ = e.step(action)\n",
    "                total_rewards[env_idx] += r\n",
    "                total_steps[env_idx] += 1\n",
    "                if done:\n",
    "                    o = e.reset()\n",
    "                    self.total_rewards.append(total_rewards[env_idx])\n",
    "                    self.total_steps.append(total_steps[env_idx])\n",
    "                    total_rewards[env_idx] = 0.0\n",
    "                    total_steps[env_idx] = 0\n",
    "                new_states.append(np.array(o))\n",
    "                dones.append(done)\n",
    "                rewards.append(r)\n",
    "            # we need an extra step to get values approximation for rollouts\n",
    "            if step_idx == self.steps_count:\n",
    "                # calculate rollout rewards\n",
    "                for env_idx, (env_rewards, env_dones, last_value) in enumerate(zip(mb_rewards, mb_dones, agent_states)):\n",
    "                    env_rewards = env_rewards.tolist()\n",
    "                    env_dones = env_dones.tolist()\n",
    "                    if not env_dones[-1]:\n",
    "                        env_rewards = discount_with_dones(env_rewards + [last_value], env_dones + [False], self.gamma)[:-1]\n",
    "                    else:\n",
    "                        env_rewards = discount_with_dones(env_rewards, env_dones, self.gamma)\n",
    "                    mb_rewards[env_idx] = env_rewards\n",
    "                yield mb_states.reshape((-1,) + mb_states.shape[2:]), mb_rewards.flatten(), mb_actions.flatten(), mb_values.flatten()\n",
    "                step_idx = 0\n",
    "            mb_states[:, step_idx] = states\n",
    "            mb_rewards[:, step_idx] = rewards\n",
    "            mb_values[:, step_idx] = agent_states\n",
    "            mb_actions[:, step_idx] = actions\n",
    "            mb_dones[:, step_idx] = dones\n",
    "            step_idx += 1\n",
    "            states = new_states\n",
    "\n",
    "    def pop_total_rewards(self):\n",
    "        r = self.total_rewards\n",
    "        if r:\n",
    "            self.total_rewards = []\n",
    "            self.total_steps = []\n",
    "        return r\n",
    "\n",
    "    def pop_rewards_steps(self):\n",
    "        res = list(zip(self.total_rewards, self.total_steps))\n",
    "        if res:\n",
    "            self.total_rewards, self.total_steps = [], []\n",
    "        return res\n",
    "\n",
    "\n",
    "class ExperienceSourceBuffer:\n",
    "    \"\"\"\n",
    "    The same as ExperienceSource, but takes episodes from the buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer, steps_count=1):\n",
    "        \"\"\"\n",
    "        Create buffered experience source\n",
    "        :param buffer: list of episodes, each is a list of Experience object\n",
    "        :param steps_count: count of steps in every entry\n",
    "        \"\"\"\n",
    "        self.update_buffer(buffer)\n",
    "        self.steps_count = steps_count\n",
    "\n",
    "    def update_buffer(self, buffer):\n",
    "        self.buffer = buffer\n",
    "        self.lens = list(map(len, buffer))\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Infinitely sample episode from the buffer and then sample item offset\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            episode = random.randrange(len(self.buffer))\n",
    "            ofs = random.randrange(self.lens[episode] - self.steps_count - 1)\n",
    "            yield self.buffer[episode][ofs:ofs+self.steps_count]\n",
    "\n",
    "\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, experience_source, buffer_size):\n",
    "        assert isinstance(experience_source, (ExperienceSource, type(None)))\n",
    "        assert isinstance(buffer_size, int)\n",
    "        self.experience_source_iter = None if experience_source is None else iter(experience_source)\n",
    "        self.buffer = []\n",
    "        self.capacity = buffer_size\n",
    "        self.pos = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Get one random batch from experience replay\n",
    "        TODO: implement sampling order policy\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(self.buffer) <= batch_size:\n",
    "            return self.buffer\n",
    "        # Warning: replace=False makes random.choice O(n)\n",
    "        keys = np.random.choice(len(self.buffer), batch_size, replace=True)\n",
    "        return [self.buffer[key] for key in keys]\n",
    "\n",
    "    def _add(self, sample):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(sample)\n",
    "        else:\n",
    "            self.buffer[self.pos] = sample\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def populate(self, samples):\n",
    "        \"\"\"\n",
    "        Populates samples into the buffer\n",
    "        :param samples: how many samples to populate\n",
    "        \"\"\"\n",
    "        for _ in range(samples):\n",
    "            entry = next(self.experience_source_iter)\n",
    "            self._add(entry)\n",
    "\n",
    "class PrioReplayBufferNaive:\n",
    "    def __init__(self, exp_source, buf_size, prob_alpha=0.6):\n",
    "        self.exp_source_iter = iter(exp_source)\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity = buf_size\n",
    "        self.pos = 0\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((buf_size, ), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def populate(self, count):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        for _ in range(count):\n",
    "            sample = next(self.exp_source_iter)\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(sample)\n",
    "            else:\n",
    "                self.buffer[self.pos] = sample\n",
    "            self.priorities[self.pos] = max_prio\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        probs = np.array(prios, dtype=np.float32) ** self.prob_alpha\n",
    "\n",
    "        probs /= probs.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=True)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ExperienceReplayBuffer):\n",
    "    def __init__(self, experience_source, buffer_size, alpha):\n",
    "        super(PrioritizedReplayBuffer, self).__init__(experience_source, buffer_size)\n",
    "        assert alpha > 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < buffer_size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = utils.SumSegmentTree(it_capacity)\n",
    "        self._it_min = utils.MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def _add(self, *args, **kwargs):\n",
    "        idx = self.pos\n",
    "        super()._add(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        for _ in range(batch_size):\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self) - 1)\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        samples = [self.buffer[idx] for idx in idxes]\n",
    "        return samples, idxes, weights\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)\n",
    "\n",
    "\n",
    "class BatchPreprocessor:\n",
    "    \"\"\"\n",
    "    Abstract preprocessor class descendants to which converts experience\n",
    "    batch to form suitable to learning.\n",
    "    \"\"\"\n",
    "    def preprocess(self, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class QLearningPreprocessor(BatchPreprocessor):\n",
    "    \"\"\"\n",
    "    Supports SimpleDQN, TargetDQN, DoubleDQN and can additionally feed TD-error back to\n",
    "    experience replay buffer.\n",
    "\n",
    "    To use different modes, use appropriate class method\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_model, use_double_dqn=False, batch_td_error_hook=None, gamma=0.99, device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.use_double_dqn = use_double_dqn\n",
    "        self.batch_dt_error_hook = batch_td_error_hook\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "    @staticmethod\n",
    "    def simple_dqn(model, **kwargs):\n",
    "        return QLearningPreprocessor(model=model, target_model=None, use_double_dqn=False, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_dqn(model, target_model, **kwards):\n",
    "        return QLearningPreprocessor(model, target_model, use_double_dqn=False, **kwards)\n",
    "\n",
    "    @staticmethod\n",
    "    def double_dqn(model, target_model, **kwargs):\n",
    "        return QLearningPreprocessor(model, target_model, use_double_dqn=True, **kwargs)\n",
    "\n",
    "    def _calc_Q(self, states_first, states_last):\n",
    "        \"\"\"\n",
    "        Calculates apropriate q values for first and last states. Way of calculate depends on our settings.\n",
    "        :param states_first: numpy array of first states\n",
    "        :param states_last: numpy array of last states\n",
    "        :return: tuple of numpy arrays of q values\n",
    "        \"\"\"\n",
    "        # here we need both first and last values calculated using our main model, so we\n",
    "        # combine both states into one batch for efficiency and separate results later\n",
    "        if self.target_model is None or self.use_double_dqn:\n",
    "            states_t = torch.tensor(np.concatenate((states_first, states_last), axis=0)).to(self.device)\n",
    "            res_both = self.model(states_t).data.cpu().numpy()\n",
    "            return res_both[:len(states_first)], res_both[len(states_first):]\n",
    "\n",
    "        # in this case we have target_model set and use_double_dqn==False\n",
    "        # so, we should calculate first_q and last_q using different models\n",
    "        states_first_v = torch.tensor(states_first).to(self.device)\n",
    "        states_last_v = torch.tensor(states_last).to(self.device)\n",
    "        q_first = self.model(states_first_v).data\n",
    "        q_last = self.target_model(states_last_v).data\n",
    "        return q_first.cpu().numpy(), q_last.cpu().numpy()\n",
    "\n",
    "    def _calc_target_rewards(self, states_last, q_last):\n",
    "        \"\"\"\n",
    "        Calculate rewards from final states according to variants from our construction:\n",
    "        1. simple DQN: max(Q(states, model))\n",
    "        2. target DQN: max(Q(states, target_model))\n",
    "        3. double DQN: Q(states, target_model)[argmax(Q(states, model)]\n",
    "        :param states_last: numpy array of last states from the games\n",
    "        :param q_last: numpy array of last q values\n",
    "        :return: vector of target rewards\n",
    "        \"\"\"\n",
    "        # in this case we handle both simple DQN and target DQN\n",
    "        if self.target_model is None or not self.use_double_dqn:\n",
    "            return q_last.max(axis=1)\n",
    "\n",
    "        # here we have target_model set and use_double_dqn==True\n",
    "        actions = q_last.argmax(axis=1)\n",
    "        # calculate Q values using target net\n",
    "        states_last_v = torch.tensor(states_last).to(self.device)\n",
    "        q_last_target = self.target_model(states_last_v).data.cpu().numpy()\n",
    "        return q_last_target[range(q_last_target.shape[0]), actions]\n",
    "\n",
    "    def preprocess(self, batch):\n",
    "        \"\"\"\n",
    "        Calculates data for Q learning from batch of observations\n",
    "        :param batch: list of lists of Experience objects\n",
    "        :return: tuple of numpy arrays:\n",
    "            1. states -- observations\n",
    "            2. target Q-values\n",
    "            3. vector of td errors for every batch entry\n",
    "        \"\"\"\n",
    "        # first and last states for every entry\n",
    "        state_0 = np.array([exp[0].state for exp in batch], dtype=np.float32)\n",
    "        state_L = np.array([exp[-1].state for exp in batch], dtype=np.float32)\n",
    "\n",
    "        q0, qL = self._calc_Q(state_0, state_L)\n",
    "        rewards = self._calc_target_rewards(state_L, qL)\n",
    "\n",
    "        td = np.zeros(shape=(len(batch),))\n",
    "\n",
    "        for idx, (total_reward, exps) in enumerate(zip(rewards, batch)):\n",
    "            # game is done, no final reward\n",
    "            if exps[-1].done:\n",
    "                total_reward = 0.0\n",
    "            for exp in reversed(exps[:-1]):\n",
    "                total_reward *= self.gamma\n",
    "                total_reward += exp.reward\n",
    "            # update total reward and calculate td error\n",
    "            act = exps[0].action\n",
    "            td[idx] = q0[idx][act] - total_reward\n",
    "            q0[idx][act] = total_reward\n",
    "\n",
    "        return state_0, q0, td\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following code cell was taken and customly adjusted to support MARL from ptan library source code. GitHub link: https://github.com/Shmuma/ptan.git \n",
    "import sys\n",
    "import time\n",
    "import operator\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RewardTracker:\n",
    "    def __init__(self, writer, min_ts_diff=1.0):\n",
    "        \"\"\"\n",
    "        Constructs RewardTracker\n",
    "        :param writer: writer to use for writing stats\n",
    "        :param min_ts_diff: minimal time difference to track speed\n",
    "        \"\"\"\n",
    "        self.writer = writer\n",
    "        self.min_ts_diff = min_ts_diff\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        self.ts_frame = 0\n",
    "        self.total_rewards = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "\n",
    "    def reward(self, reward, frame, epsilon=None):\n",
    "        self.total_rewards.append(reward)\n",
    "        mean_reward = np.mean(self.total_rewards[-100:])\n",
    "        ts_diff = time.time() - self.ts\n",
    "        if ts_diff > self.min_ts_diff:\n",
    "            speed = (frame - self.ts_frame) / ts_diff\n",
    "            self.ts_frame = frame\n",
    "            self.ts = time.time()\n",
    "            epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
    "            print(\"%d: done %d episodes, mean reward %.3f, speed %.2f f/s%s\" % (\n",
    "                frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n",
    "            ))\n",
    "            sys.stdout.flush()\n",
    "            self.writer.add_scalar(\"speed\", speed, frame)\n",
    "        if epsilon is not None:\n",
    "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "        self.writer.add_scalars(\"reward\", {'agent1': reward[0], 'agent2': reward[1]}, frame)\n",
    "        return mean_reward if len(self.total_rewards) > 30 else None\n",
    "\n",
    "\n",
    "class TBMeanTracker:\n",
    "    \"\"\"\n",
    "    TensorBoard value tracker: allows to batch fixed amount of historical values and write their mean into TB\n",
    "\n",
    "    Designed and tested with pytorch-tensorboard in mind\n",
    "    \"\"\"\n",
    "    def __init__(self, writer, batch_size):\n",
    "        \"\"\"\n",
    "        :param writer: writer with close() and add_scalar() methods\n",
    "        :param batch_size: integer size of batch to track\n",
    "        \"\"\"\n",
    "        assert isinstance(batch_size, int)\n",
    "        assert writer is not None\n",
    "        self.writer = writer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._batches = collections.defaultdict(list)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.writer.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def _as_float(value):\n",
    "        assert isinstance(value, (float, int, np.ndarray, np.generic, torch.autograd.Variable)) or torch.is_tensor(value)\n",
    "        tensor_val = None\n",
    "        if isinstance(value, torch.autograd.Variable):\n",
    "            tensor_val = value.data\n",
    "        elif torch.is_tensor(value):\n",
    "            tensor_val = value\n",
    "\n",
    "        if tensor_val is not None:\n",
    "            return tensor_val.float().mean().item()\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            return float(np.mean(value))\n",
    "        else:\n",
    "            return float(value)\n",
    "\n",
    "    def track(self, param_name, value, iter_index):\n",
    "        assert isinstance(param_name, str)\n",
    "        assert isinstance(iter_index, int)\n",
    "\n",
    "        data = self._batches[param_name]\n",
    "        data.append(self._as_float(value))\n",
    "\n",
    "        if len(data) >= self.batch_size:\n",
    "            self.writer.add_scalar(param_name, np.mean(data), iter_index)\n",
    "            data.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelActor(\n",
      "  (mu1): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (mu2): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "ModelCritic(\n",
      "  (value): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "ModelSACTwinQ(\n",
      "  (q1): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (q2): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "411: done 25 episodes, mean reward 0.001, speed 409.61 f/s\n",
      "814: done 51 episodes, mean reward -0.000, speed 402.43 f/s\n",
      "1279: done 82 episodes, mean reward -0.001, speed 447.28 f/s\n",
      "1762: done 113 episodes, mean reward -0.002, speed 477.35 f/s\n",
      "2222: done 142 episodes, mean reward -0.003, speed 459.57 f/s\n",
      "2713: done 174 episodes, mean reward -0.002, speed 485.02 f/s\n",
      "3153: done 203 episodes, mean reward -0.002, speed 435.49 f/s\n",
      "3604: done 231 episodes, mean reward -0.001, speed 442.44 f/s\n",
      "4096: done 262 episodes, mean reward -0.001, speed 479.66 f/s\n",
      "4587: done 295 episodes, mean reward -0.001, speed 490.55 f/s\n",
      "5068: done 327 episodes, mean reward -0.002, speed 474.21 f/s\n",
      "5550: done 357 episodes, mean reward -0.003, speed 481.79 f/s\n",
      "6043: done 389 episodes, mean reward -0.002, speed 482.50 f/s\n",
      "6556: done 421 episodes, mean reward -0.002, speed 502.08 f/s\n",
      "6987: done 450 episodes, mean reward -0.003, speed 430.60 f/s\n",
      "7479: done 482 episodes, mean reward -0.003, speed 487.44 f/s\n",
      "7886: done 509 episodes, mean reward -0.003, speed 405.04 f/s\n",
      "8365: done 540 episodes, mean reward -0.002, speed 475.03 f/s\n",
      "8832: done 571 episodes, mean reward -0.002, speed 459.96 f/s\n",
      "9346: done 605 episodes, mean reward -0.002, speed 505.02 f/s\n",
      "9783: done 635 episodes, mean reward -0.003, speed 431.87 f/s\n",
      "Test done in 1.11 sec, reward -0.005, steps 14\n",
      "10016: done 651 episodes, mean reward -0.004, speed 125.64 f/s\n",
      "10089: done 655 episodes, mean reward -0.003, speed 70.71 f/s\n",
      "10160: done 660 episodes, mean reward -0.004, speed 66.94 f/s\n",
      "10231: done 665 episodes, mean reward -0.004, speed 70.10 f/s\n",
      "10302: done 670 episodes, mean reward -0.004, speed 67.65 f/s\n",
      "10373: done 675 episodes, mean reward -0.004, speed 70.17 f/s\n",
      "10444: done 680 episodes, mean reward -0.004, speed 70.52 f/s\n",
      "10515: done 685 episodes, mean reward -0.004, speed 70.33 f/s\n",
      "10586: done 690 episodes, mean reward -0.004, speed 65.18 f/s\n",
      "10657: done 695 episodes, mean reward -0.004, speed 68.82 f/s\n",
      "10728: done 700 episodes, mean reward -0.004, speed 62.24 f/s\n",
      "10799: done 705 episodes, mean reward -0.004, speed 70.48 f/s\n",
      "10870: done 710 episodes, mean reward -0.004, speed 68.72 f/s\n",
      "10941: done 715 episodes, mean reward -0.004, speed 65.63 f/s\n",
      "11012: done 720 episodes, mean reward -0.004, speed 66.48 f/s\n",
      "11083: done 725 episodes, mean reward -0.004, speed 68.52 f/s\n",
      "11154: done 730 episodes, mean reward -0.004, speed 65.30 f/s\n",
      "11225: done 735 episodes, mean reward -0.004, speed 69.86 f/s\n",
      "11296: done 740 episodes, mean reward -0.004, speed 68.10 f/s\n",
      "11367: done 745 episodes, mean reward -0.004, speed 67.39 f/s\n",
      "11438: done 750 episodes, mean reward -0.004, speed 68.59 f/s\n",
      "11509: done 755 episodes, mean reward -0.005, speed 66.80 f/s\n",
      "11594: done 761 episodes, mean reward -0.005, speed 71.55 f/s\n",
      "11665: done 766 episodes, mean reward -0.005, speed 65.73 f/s\n",
      "11736: done 771 episodes, mean reward -0.005, speed 70.72 f/s\n",
      "11807: done 776 episodes, mean reward -0.005, speed 66.86 f/s\n",
      "11878: done 781 episodes, mean reward -0.005, speed 70.14 f/s\n",
      "11949: done 786 episodes, mean reward -0.005, speed 70.88 f/s\n",
      "12020: done 791 episodes, mean reward -0.005, speed 70.76 f/s\n",
      "12091: done 796 episodes, mean reward -0.005, speed 67.24 f/s\n",
      "12176: done 802 episodes, mean reward -0.005, speed 69.91 f/s\n",
      "12247: done 807 episodes, mean reward -0.005, speed 70.41 f/s\n",
      "12318: done 812 episodes, mean reward -0.005, speed 68.92 f/s\n",
      "12389: done 817 episodes, mean reward -0.005, speed 69.96 f/s\n",
      "12474: done 823 episodes, mean reward -0.005, speed 71.35 f/s\n",
      "12545: done 828 episodes, mean reward -0.005, speed 70.34 f/s\n",
      "12630: done 834 episodes, mean reward -0.005, speed 73.86 f/s\n",
      "12716: done 840 episodes, mean reward -0.005, speed 71.36 f/s\n",
      "12787: done 845 episodes, mean reward -0.005, speed 65.79 f/s\n",
      "12858: done 850 episodes, mean reward -0.005, speed 69.56 f/s\n",
      "12929: done 855 episodes, mean reward -0.005, speed 64.57 f/s\n",
      "13000: done 860 episodes, mean reward -0.005, speed 68.08 f/s\n",
      "13071: done 865 episodes, mean reward -0.005, speed 67.05 f/s\n",
      "13142: done 870 episodes, mean reward -0.005, speed 70.24 f/s\n",
      "13213: done 875 episodes, mean reward -0.005, speed 67.02 f/s\n",
      "13284: done 880 episodes, mean reward -0.005, speed 70.90 f/s\n",
      "13355: done 885 episodes, mean reward -0.005, speed 66.21 f/s\n",
      "13440: done 891 episodes, mean reward -0.005, speed 71.11 f/s\n",
      "13511: done 896 episodes, mean reward -0.005, speed 68.13 f/s\n",
      "13582: done 901 episodes, mean reward -0.005, speed 68.70 f/s\n",
      "13667: done 907 episodes, mean reward -0.005, speed 71.10 f/s\n",
      "13738: done 912 episodes, mean reward -0.005, speed 64.57 f/s\n",
      "13809: done 917 episodes, mean reward -0.005, speed 67.52 f/s\n",
      "13880: done 922 episodes, mean reward -0.005, speed 66.70 f/s\n",
      "13951: done 927 episodes, mean reward -0.005, speed 70.24 f/s\n",
      "14022: done 932 episodes, mean reward -0.005, speed 68.49 f/s\n",
      "14107: done 938 episodes, mean reward -0.005, speed 72.16 f/s\n",
      "14178: done 943 episodes, mean reward -0.005, speed 69.96 f/s\n",
      "14249: done 948 episodes, mean reward -0.005, speed 68.10 f/s\n",
      "14320: done 953 episodes, mean reward -0.005, speed 66.64 f/s\n",
      "14405: done 959 episodes, mean reward -0.005, speed 71.26 f/s\n",
      "14491: done 965 episodes, mean reward -0.005, speed 73.07 f/s\n",
      "14562: done 970 episodes, mean reward -0.005, speed 70.07 f/s\n",
      "14633: done 975 episodes, mean reward -0.005, speed 68.99 f/s\n",
      "14704: done 980 episodes, mean reward -0.005, speed 69.31 f/s\n",
      "14789: done 986 episodes, mean reward -0.005, speed 70.73 f/s\n",
      "14860: done 991 episodes, mean reward -0.005, speed 65.39 f/s\n",
      "14931: done 996 episodes, mean reward -0.005, speed 70.40 f/s\n",
      "15002: done 1001 episodes, mean reward -0.005, speed 70.08 f/s\n",
      "15073: done 1006 episodes, mean reward -0.005, speed 69.49 f/s\n",
      "15144: done 1011 episodes, mean reward -0.005, speed 65.68 f/s\n",
      "15215: done 1016 episodes, mean reward -0.005, speed 69.29 f/s\n",
      "15286: done 1021 episodes, mean reward -0.005, speed 68.62 f/s\n",
      "15357: done 1026 episodes, mean reward -0.005, speed 68.90 f/s\n",
      "15429: done 1030 episodes, mean reward -0.004, speed 65.88 f/s\n",
      "15500: done 1035 episodes, mean reward -0.004, speed 70.62 f/s\n",
      "15571: done 1040 episodes, mean reward -0.004, speed 65.36 f/s\n",
      "15656: done 1046 episodes, mean reward -0.004, speed 71.71 f/s\n",
      "15729: done 1050 episodes, mean reward -0.004, speed 67.78 f/s\n",
      "15800: done 1055 episodes, mean reward -0.004, speed 68.56 f/s\n",
      "15871: done 1060 episodes, mean reward -0.004, speed 69.62 f/s\n",
      "15942: done 1065 episodes, mean reward -0.004, speed 68.02 f/s\n",
      "16013: done 1070 episodes, mean reward -0.004, speed 67.63 f/s\n",
      "16084: done 1075 episodes, mean reward -0.004, speed 65.22 f/s\n",
      "16155: done 1080 episodes, mean reward -0.004, speed 70.31 f/s\n",
      "16226: done 1085 episodes, mean reward -0.004, speed 68.89 f/s\n",
      "16297: done 1090 episodes, mean reward -0.004, speed 69.62 f/s\n",
      "16368: done 1095 episodes, mean reward -0.004, speed 66.77 f/s\n",
      "16439: done 1100 episodes, mean reward -0.004, speed 70.17 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16510: done 1105 episodes, mean reward -0.004, speed 66.48 f/s\n",
      "16596: done 1111 episodes, mean reward -0.004, speed 72.18 f/s\n",
      "16667: done 1116 episodes, mean reward -0.004, speed 66.58 f/s\n",
      "16752: done 1122 episodes, mean reward -0.004, speed 72.96 f/s\n",
      "16823: done 1127 episodes, mean reward -0.004, speed 68.16 f/s\n",
      "16894: done 1132 episodes, mean reward -0.004, speed 66.12 f/s\n",
      "16965: done 1137 episodes, mean reward -0.004, speed 69.66 f/s\n",
      "17036: done 1142 episodes, mean reward -0.004, speed 69.90 f/s\n",
      "17107: done 1147 episodes, mean reward -0.004, speed 70.65 f/s\n",
      "17192: done 1153 episodes, mean reward -0.005, speed 70.58 f/s\n",
      "17277: done 1159 episodes, mean reward -0.005, speed 72.05 f/s\n",
      "17348: done 1164 episodes, mean reward -0.005, speed 68.51 f/s\n",
      "17419: done 1169 episodes, mean reward -0.005, speed 68.85 f/s\n",
      "17490: done 1174 episodes, mean reward -0.005, speed 67.51 f/s\n",
      "17561: done 1179 episodes, mean reward -0.005, speed 70.13 f/s\n",
      "17632: done 1184 episodes, mean reward -0.005, speed 70.24 f/s\n",
      "17717: done 1190 episodes, mean reward -0.005, speed 71.14 f/s\n",
      "17803: done 1196 episodes, mean reward -0.005, speed 72.37 f/s\n",
      "17874: done 1201 episodes, mean reward -0.005, speed 67.27 f/s\n",
      "17945: done 1206 episodes, mean reward -0.005, speed 67.84 f/s\n",
      "18016: done 1211 episodes, mean reward -0.005, speed 67.25 f/s\n",
      "18101: done 1217 episodes, mean reward -0.005, speed 72.75 f/s\n",
      "18172: done 1222 episodes, mean reward -0.005, speed 66.30 f/s\n",
      "18243: done 1227 episodes, mean reward -0.005, speed 66.91 f/s\n",
      "18314: done 1232 episodes, mean reward -0.005, speed 69.56 f/s\n",
      "18399: done 1238 episodes, mean reward -0.005, speed 71.48 f/s\n",
      "18470: done 1243 episodes, mean reward -0.005, speed 70.07 f/s\n",
      "18541: done 1248 episodes, mean reward -0.005, speed 70.45 f/s\n",
      "18612: done 1253 episodes, mean reward -0.005, speed 69.73 f/s\n",
      "18683: done 1258 episodes, mean reward -0.005, speed 68.62 f/s\n",
      "18754: done 1263 episodes, mean reward -0.005, speed 68.25 f/s\n",
      "18839: done 1269 episodes, mean reward -0.005, speed 71.29 f/s\n",
      "18910: done 1274 episodes, mean reward -0.005, speed 67.30 f/s\n",
      "18981: done 1279 episodes, mean reward -0.005, speed 68.65 f/s\n",
      "19066: done 1285 episodes, mean reward -0.005, speed 70.21 f/s\n",
      "19137: done 1290 episodes, mean reward -0.005, speed 70.61 f/s\n",
      "19208: done 1295 episodes, mean reward -0.005, speed 69.79 f/s\n",
      "19279: done 1300 episodes, mean reward -0.005, speed 65.55 f/s\n",
      "19350: done 1305 episodes, mean reward -0.005, speed 67.78 f/s\n",
      "19421: done 1310 episodes, mean reward -0.005, speed 69.62 f/s\n",
      "19507: done 1316 episodes, mean reward -0.005, speed 70.52 f/s\n",
      "19578: done 1321 episodes, mean reward -0.005, speed 68.13 f/s\n",
      "19649: done 1326 episodes, mean reward -0.005, speed 68.24 f/s\n",
      "19720: done 1331 episodes, mean reward -0.005, speed 68.15 f/s\n",
      "19805: done 1337 episodes, mean reward -0.005, speed 71.04 f/s\n",
      "19890: done 1343 episodes, mean reward -0.005, speed 72.65 f/s\n",
      "19961: done 1348 episodes, mean reward -0.005, speed 69.45 f/s\n",
      "Test done in 1.02 sec, reward -0.005, steps 13\n",
      "20016: done 1351 episodes, mean reward -0.005, speed 30.09 f/s\n",
      "20087: done 1356 episodes, mean reward -0.005, speed 69.16 f/s\n",
      "20158: done 1361 episodes, mean reward -0.005, speed 70.24 f/s\n",
      "20243: done 1367 episodes, mean reward -0.005, speed 70.69 f/s\n",
      "20314: done 1372 episodes, mean reward -0.005, speed 70.68 f/s\n",
      "20385: done 1377 episodes, mean reward -0.005, speed 70.90 f/s\n",
      "20456: done 1382 episodes, mean reward -0.005, speed 70.31 f/s\n",
      "20541: done 1388 episodes, mean reward -0.005, speed 73.12 f/s\n",
      "20612: done 1393 episodes, mean reward -0.005, speed 64.84 f/s\n",
      "20698: done 1399 episodes, mean reward -0.005, speed 70.36 f/s\n",
      "20783: done 1405 episodes, mean reward -0.005, speed 71.40 f/s\n",
      "20868: done 1411 episodes, mean reward -0.005, speed 72.49 f/s\n",
      "20939: done 1416 episodes, mean reward -0.005, speed 67.71 f/s\n",
      "21010: done 1421 episodes, mean reward -0.005, speed 68.36 f/s\n",
      "21081: done 1426 episodes, mean reward -0.005, speed 68.70 f/s\n",
      "21152: done 1431 episodes, mean reward -0.005, speed 70.34 f/s\n",
      "21223: done 1436 episodes, mean reward -0.005, speed 67.11 f/s\n",
      "21294: done 1441 episodes, mean reward -0.005, speed 64.10 f/s\n",
      "21366: done 1446 episodes, mean reward -0.005, speed 71.10 f/s\n",
      "21437: done 1451 episodes, mean reward -0.005, speed 63.96 f/s\n",
      "21522: done 1457 episodes, mean reward -0.005, speed 72.49 f/s\n",
      "21593: done 1462 episodes, mean reward -0.005, speed 68.46 f/s\n",
      "21672: done 1467 episodes, mean reward -0.005, speed 69.07 f/s\n",
      "21744: done 1472 episodes, mean reward -0.005, speed 71.30 f/s\n",
      "21843: done 1477 episodes, mean reward -0.005, speed 69.01 f/s\n",
      "21922: done 1481 episodes, mean reward -0.004, speed 75.13 f/s\n",
      "21993: done 1486 episodes, mean reward -0.004, speed 67.15 f/s\n",
      "22064: done 1491 episodes, mean reward -0.004, speed 69.66 f/s\n",
      "22135: done 1496 episodes, mean reward -0.004, speed 65.59 f/s\n",
      "22208: done 1501 episodes, mean reward -0.004, speed 68.90 f/s\n",
      "22283: done 1505 episodes, mean reward -0.004, speed 67.51 f/s\n",
      "22354: done 1510 episodes, mean reward -0.004, speed 70.94 f/s\n",
      "22426: done 1515 episodes, mean reward -0.004, speed 68.67 f/s\n",
      "22497: done 1520 episodes, mean reward -0.004, speed 70.94 f/s\n",
      "22568: done 1525 episodes, mean reward -0.004, speed 70.38 f/s\n",
      "22639: done 1530 episodes, mean reward -0.004, speed 70.10 f/s\n",
      "22710: done 1535 episodes, mean reward -0.004, speed 65.18 f/s\n",
      "22781: done 1540 episodes, mean reward -0.004, speed 70.05 f/s\n",
      "22852: done 1545 episodes, mean reward -0.004, speed 67.52 f/s\n",
      "22923: done 1550 episodes, mean reward -0.004, speed 67.62 f/s\n",
      "22994: done 1555 episodes, mean reward -0.004, speed 69.83 f/s\n",
      "23066: done 1560 episodes, mean reward -0.004, speed 69.49 f/s\n",
      "23137: done 1565 episodes, mean reward -0.004, speed 70.61 f/s\n",
      "23211: done 1570 episodes, mean reward -0.004, speed 68.94 f/s\n",
      "23295: done 1576 episodes, mean reward -0.004, speed 71.09 f/s\n",
      "23366: done 1581 episodes, mean reward -0.004, speed 69.14 f/s\n",
      "23437: done 1586 episodes, mean reward -0.004, speed 69.83 f/s\n",
      "23508: done 1591 episodes, mean reward -0.004, speed 68.39 f/s\n",
      "23593: done 1597 episodes, mean reward -0.004, speed 71.98 f/s\n",
      "23666: done 1602 episodes, mean reward -0.004, speed 67.08 f/s\n",
      "23754: done 1607 episodes, mean reward -0.004, speed 71.12 f/s\n",
      "23827: done 1611 episodes, mean reward -0.004, speed 65.35 f/s\n",
      "23902: done 1615 episodes, mean reward -0.003, speed 66.15 f/s\n",
      "23993: done 1619 episodes, mean reward -0.002, speed 72.80 f/s\n",
      "24076: done 1622 episodes, mean reward -0.001, speed 67.50 f/s\n",
      "24146: done 1627 episodes, mean reward -0.001, speed 69.73 f/s\n",
      "24237: done 1632 episodes, mean reward -0.001, speed 68.48 f/s\n",
      "24310: done 1636 episodes, mean reward -0.000, speed 67.52 f/s\n",
      "24383: done 1640 episodes, mean reward 0.000, speed 71.36 f/s\n",
      "24455: done 1644 episodes, mean reward 0.001, speed 69.36 f/s\n",
      "24531: done 1648 episodes, mean reward 0.001, speed 70.38 f/s\n",
      "24609: done 1652 episodes, mean reward 0.002, speed 67.55 f/s\n",
      "24685: done 1656 episodes, mean reward 0.002, speed 72.13 f/s\n",
      "24756: done 1661 episodes, mean reward 0.002, speed 66.04 f/s\n",
      "24826: done 1666 episodes, mean reward 0.002, speed 69.08 f/s\n",
      "24901: done 1670 episodes, mean reward 0.003, speed 68.30 f/s\n",
      "24978: done 1674 episodes, mean reward 0.003, speed 71.47 f/s\n",
      "25051: done 1678 episodes, mean reward 0.004, speed 66.66 f/s\n",
      "25141: done 1682 episodes, mean reward 0.005, speed 70.19 f/s\n",
      "25214: done 1685 episodes, mean reward 0.006, speed 67.93 f/s\n",
      "25287: done 1688 episodes, mean reward 0.007, speed 69.01 f/s\n",
      "25361: done 1692 episodes, mean reward 0.007, speed 70.18 f/s\n",
      "25429: done 1695 episodes, mean reward 0.008, speed 65.53 f/s\n",
      "25500: done 1700 episodes, mean reward 0.008, speed 68.12 f/s\n",
      "25589: done 1704 episodes, mean reward 0.009, speed 68.08 f/s\n",
      "25675: done 1709 episodes, mean reward 0.009, speed 68.31 f/s\n",
      "25761: done 1714 episodes, mean reward 0.009, speed 68.84 f/s\n",
      "25832: done 1719 episodes, mean reward 0.007, speed 67.20 f/s\n",
      "25903: done 1724 episodes, mean reward 0.006, speed 69.60 f/s\n",
      "25974: done 1729 episodes, mean reward 0.006, speed 69.59 f/s\n",
      "26059: done 1735 episodes, mean reward 0.005, speed 72.00 f/s\n",
      "26130: done 1740 episodes, mean reward 0.005, speed 69.19 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26201: done 1745 episodes, mean reward 0.004, speed 69.45 f/s\n",
      "26272: done 1750 episodes, mean reward 0.003, speed 69.34 f/s\n",
      "26357: done 1756 episodes, mean reward 0.003, speed 70.96 f/s\n",
      "26428: done 1761 episodes, mean reward 0.003, speed 66.73 f/s\n",
      "26499: done 1766 episodes, mean reward 0.003, speed 66.52 f/s\n",
      "26570: done 1771 episodes, mean reward 0.002, speed 66.18 f/s\n",
      "26641: done 1776 episodes, mean reward 0.001, speed 65.87 f/s\n",
      "26712: done 1781 episodes, mean reward 0.001, speed 67.94 f/s\n",
      "26783: done 1786 episodes, mean reward -0.001, speed 69.62 f/s\n",
      "26854: done 1791 episodes, mean reward -0.002, speed 69.32 f/s\n",
      "26925: done 1796 episodes, mean reward -0.003, speed 67.00 f/s\n",
      "26996: done 1801 episodes, mean reward -0.003, speed 70.80 f/s\n",
      "27081: done 1807 episodes, mean reward -0.004, speed 71.72 f/s\n",
      "27152: done 1812 episodes, mean reward -0.004, speed 69.72 f/s\n",
      "27223: done 1817 episodes, mean reward -0.005, speed 69.76 f/s\n",
      "27309: done 1823 episodes, mean reward -0.005, speed 72.46 f/s\n",
      "27380: done 1828 episodes, mean reward -0.005, speed 65.89 f/s\n",
      "27451: done 1833 episodes, mean reward -0.005, speed 68.86 f/s\n",
      "27522: done 1838 episodes, mean reward -0.005, speed 66.83 f/s\n",
      "27593: done 1843 episodes, mean reward -0.005, speed 69.71 f/s\n",
      "27664: done 1848 episodes, mean reward -0.005, speed 68.15 f/s\n",
      "27749: done 1854 episodes, mean reward -0.005, speed 71.30 f/s\n",
      "27820: done 1859 episodes, mean reward -0.005, speed 67.82 f/s\n",
      "27891: done 1864 episodes, mean reward -0.005, speed 66.14 f/s\n",
      "27962: done 1869 episodes, mean reward -0.005, speed 67.33 f/s\n",
      "28033: done 1874 episodes, mean reward -0.005, speed 67.30 f/s\n",
      "28118: done 1880 episodes, mean reward -0.005, speed 71.49 f/s\n",
      "28189: done 1885 episodes, mean reward -0.005, speed 67.52 f/s\n",
      "28260: done 1890 episodes, mean reward -0.005, speed 66.89 f/s\n",
      "28331: done 1895 episodes, mean reward -0.005, speed 67.36 f/s\n",
      "28402: done 1900 episodes, mean reward -0.005, speed 68.92 f/s\n",
      "28473: done 1905 episodes, mean reward -0.005, speed 65.54 f/s\n",
      "28544: done 1910 episodes, mean reward -0.005, speed 68.03 f/s\n",
      "28615: done 1915 episodes, mean reward -0.005, speed 69.56 f/s\n",
      "28686: done 1920 episodes, mean reward -0.005, speed 70.51 f/s\n",
      "28757: done 1925 episodes, mean reward -0.005, speed 66.73 f/s\n",
      "28842: done 1931 episodes, mean reward -0.005, speed 72.10 f/s\n",
      "28927: done 1937 episodes, mean reward -0.005, speed 71.15 f/s\n",
      "28998: done 1942 episodes, mean reward -0.005, speed 67.47 f/s\n",
      "29084: done 1948 episodes, mean reward -0.005, speed 70.28 f/s\n",
      "29155: done 1953 episodes, mean reward -0.005, speed 68.89 f/s\n",
      "29226: done 1958 episodes, mean reward -0.005, speed 69.22 f/s\n",
      "29297: done 1963 episodes, mean reward -0.005, speed 66.06 f/s\n",
      "29368: done 1968 episodes, mean reward -0.005, speed 69.40 f/s\n",
      "29439: done 1973 episodes, mean reward -0.005, speed 68.99 f/s\n",
      "29524: done 1979 episodes, mean reward -0.005, speed 71.26 f/s\n",
      "29595: done 1984 episodes, mean reward -0.005, speed 67.62 f/s\n",
      "29666: done 1989 episodes, mean reward -0.005, speed 67.09 f/s\n",
      "29737: done 1994 episodes, mean reward -0.005, speed 68.62 f/s\n",
      "29808: done 1999 episodes, mean reward -0.005, speed 69.09 f/s\n",
      "29879: done 2004 episodes, mean reward -0.005, speed 69.18 f/s\n",
      "29950: done 2009 episodes, mean reward -0.005, speed 67.78 f/s\n",
      "Test done in 1.01 sec, reward -0.005, steps 13\n",
      "30017: done 2013 episodes, mean reward -0.005, speed 33.53 f/s\n",
      "30088: done 2018 episodes, mean reward -0.005, speed 69.25 f/s\n",
      "30159: done 2023 episodes, mean reward -0.005, speed 67.80 f/s\n",
      "30230: done 2028 episodes, mean reward -0.005, speed 67.65 f/s\n",
      "30301: done 2033 episodes, mean reward -0.005, speed 70.37 f/s\n",
      "30386: done 2039 episodes, mean reward -0.005, speed 69.83 f/s\n",
      "30457: done 2044 episodes, mean reward -0.005, speed 65.87 f/s\n",
      "30528: done 2049 episodes, mean reward -0.005, speed 68.39 f/s\n",
      "30599: done 2054 episodes, mean reward -0.005, speed 68.31 f/s\n",
      "30670: done 2059 episodes, mean reward -0.005, speed 70.64 f/s\n",
      "30741: done 2064 episodes, mean reward -0.005, speed 68.38 f/s\n",
      "30826: done 2070 episodes, mean reward -0.005, speed 70.97 f/s\n",
      "30897: done 2075 episodes, mean reward -0.005, speed 69.74 f/s\n",
      "30982: done 2081 episodes, mean reward -0.005, speed 70.97 f/s\n",
      "31067: done 2087 episodes, mean reward -0.005, speed 71.67 f/s\n",
      "31138: done 2092 episodes, mean reward -0.005, speed 68.55 f/s\n",
      "31224: done 2098 episodes, mean reward -0.005, speed 72.79 f/s\n",
      "31295: done 2103 episodes, mean reward -0.005, speed 65.63 f/s\n",
      "31366: done 2108 episodes, mean reward -0.005, speed 68.58 f/s\n",
      "31437: done 2113 episodes, mean reward -0.005, speed 68.59 f/s\n",
      "31508: done 2118 episodes, mean reward -0.005, speed 70.89 f/s\n",
      "31579: done 2123 episodes, mean reward -0.005, speed 68.89 f/s\n",
      "31650: done 2128 episodes, mean reward -0.005, speed 67.00 f/s\n",
      "31721: done 2133 episodes, mean reward -0.005, speed 70.17 f/s\n",
      "31806: done 2139 episodes, mean reward -0.005, speed 70.93 f/s\n",
      "31877: done 2144 episodes, mean reward -0.005, speed 67.82 f/s\n",
      "31962: done 2150 episodes, mean reward -0.005, speed 71.65 f/s\n",
      "32033: done 2155 episodes, mean reward -0.005, speed 70.05 f/s\n",
      "32104: done 2160 episodes, mean reward -0.005, speed 68.78 f/s\n",
      "32189: done 2166 episodes, mean reward -0.005, speed 73.11 f/s\n",
      "32260: done 2171 episodes, mean reward -0.005, speed 66.68 f/s\n",
      "32331: done 2176 episodes, mean reward -0.005, speed 66.61 f/s\n",
      "32402: done 2181 episodes, mean reward -0.005, speed 68.65 f/s\n",
      "32473: done 2186 episodes, mean reward -0.005, speed 70.82 f/s\n",
      "32544: done 2191 episodes, mean reward -0.005, speed 67.55 f/s\n",
      "32619: done 2195 episodes, mean reward -0.004, speed 71.26 f/s\n",
      "32690: done 2200 episodes, mean reward -0.004, speed 68.23 f/s\n",
      "32763: done 2204 episodes, mean reward -0.004, speed 71.45 f/s\n",
      "32838: done 2207 episodes, mean reward -0.003, speed 65.89 f/s\n",
      "32913: done 2210 episodes, mean reward -0.002, speed 69.78 f/s\n",
      "32988: done 2213 episodes, mean reward -0.001, speed 68.36 f/s\n",
      "33060: done 2217 episodes, mean reward -0.000, speed 66.31 f/s\n",
      "33131: done 2222 episodes, mean reward -0.000, speed 70.34 f/s\n",
      "33201: done 2227 episodes, mean reward -0.000, speed 69.60 f/s\n",
      "33275: done 2231 episodes, mean reward 0.000, speed 70.11 f/s\n",
      "33346: done 2236 episodes, mean reward 0.000, speed 69.25 f/s\n",
      "33417: done 2241 episodes, mean reward 0.000, speed 70.92 f/s\n",
      "33488: done 2246 episodes, mean reward 0.000, speed 64.83 f/s\n",
      "33563: done 2250 episodes, mean reward 0.001, speed 71.49 f/s\n",
      "33634: done 2255 episodes, mean reward 0.001, speed 67.20 f/s\n",
      "33739: done 2261 episodes, mean reward 0.001, speed 70.03 f/s\n",
      "33810: done 2266 episodes, mean reward 0.001, speed 67.87 f/s\n",
      "33881: done 2271 episodes, mean reward 0.001, speed 67.16 f/s\n",
      "33956: done 2275 episodes, mean reward 0.002, speed 70.11 f/s\n",
      "34042: done 2280 episodes, mean reward 0.002, speed 69.55 f/s\n",
      "34127: done 2286 episodes, mean reward 0.002, speed 72.40 f/s\n",
      "34198: done 2291 episodes, mean reward 0.002, speed 63.68 f/s\n",
      "34258: done 2294 episodes, mean reward 0.003, speed 54.94 f/s\n",
      "34329: done 2299 episodes, mean reward 0.002, speed 62.10 f/s\n",
      "34389: done 2302 episodes, mean reward 0.003, speed 59.21 f/s\n",
      "34462: done 2306 episodes, mean reward 0.002, speed 63.17 f/s\n",
      "34533: done 2310 episodes, mean reward 0.001, speed 61.91 f/s\n",
      "34607: done 2314 episodes, mean reward 0.001, speed 63.87 f/s\n",
      "34671: done 2316 episodes, mean reward 0.002, speed 61.41 f/s\n",
      "34741: done 2321 episodes, mean reward 0.001, speed 61.23 f/s\n",
      "34833: done 2325 episodes, mean reward 0.002, speed 60.24 f/s\n",
      "34896: done 2327 episodes, mean reward 0.003, speed 58.45 f/s\n",
      "34958: done 2330 episodes, mean reward 0.003, speed 61.49 f/s\n",
      "35029: done 2335 episodes, mean reward 0.003, speed 59.54 f/s\n",
      "35090: done 2338 episodes, mean reward 0.004, speed 59.38 f/s\n",
      "35178: done 2343 episodes, mean reward 0.004, speed 58.25 f/s\n",
      "35254: done 2347 episodes, mean reward 0.005, speed 59.90 f/s\n",
      "35331: done 2350 episodes, mean reward 0.005, speed 61.19 f/s\n",
      "35390: done 2354 episodes, mean reward 0.005, speed 56.27 f/s\n",
      "35463: done 2358 episodes, mean reward 0.005, speed 58.99 f/s\n",
      "35525: done 2360 episodes, mean reward 0.006, speed 58.60 f/s\n",
      "35591: done 2362 episodes, mean reward 0.006, speed 62.91 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35667: done 2366 episodes, mean reward 0.007, speed 63.21 f/s\n",
      "35739: done 2370 episodes, mean reward 0.007, speed 60.56 f/s\n",
      "35814: done 2374 episodes, mean reward 0.008, speed 60.15 f/s\n",
      "35876: done 2377 episodes, mean reward 0.008, speed 60.71 f/s\n",
      "35951: done 2381 episodes, mean reward 0.008, speed 63.24 f/s\n",
      "36045: done 2385 episodes, mean reward 0.009, speed 62.34 f/s\n",
      "36110: done 2387 episodes, mean reward 0.010, speed 60.17 f/s\n",
      "36186: done 2391 episodes, mean reward 0.010, speed 63.13 f/s\n",
      "36263: done 2395 episodes, mean reward 0.010, speed 59.64 f/s\n",
      "36334: done 2400 episodes, mean reward 0.009, speed 63.93 f/s\n",
      "36413: done 2403 episodes, mean reward 0.010, speed 62.86 f/s\n",
      "36492: done 2407 episodes, mean reward 0.009, speed 60.29 f/s\n",
      "36562: done 2412 episodes, mean reward 0.009, speed 61.90 f/s\n",
      "36672: done 2417 episodes, mean reward 0.009, speed 64.56 f/s\n",
      "36739: done 2420 episodes, mean reward 0.010, speed 62.89 f/s\n",
      "36810: done 2421 episodes, mean reward 0.011, speed 65.72 f/s\n",
      "36885: done 2425 episodes, mean reward 0.011, speed 63.67 f/s\n",
      "36960: done 2429 episodes, mean reward 0.010, speed 66.44 f/s\n",
      "37051: done 2433 episodes, mean reward 0.011, speed 64.13 f/s\n",
      "37131: done 2436 episodes, mean reward 0.012, speed 63.78 f/s\n",
      "37207: done 2440 episodes, mean reward 0.012, speed 64.60 f/s\n",
      "37282: done 2443 episodes, mean reward 0.012, speed 62.79 f/s\n",
      "37353: done 2444 episodes, mean reward 0.014, speed 63.90 f/s\n",
      "37428: done 2447 episodes, mean reward 0.014, speed 63.45 f/s\n",
      "37501: done 2451 episodes, mean reward 0.014, speed 62.31 f/s\n",
      "37575: done 2455 episodes, mean reward 0.015, speed 63.38 f/s\n",
      "37646: done 2460 episodes, mean reward 0.013, speed 62.32 f/s\n",
      "37717: done 2465 episodes, mean reward 0.012, speed 65.60 f/s\n",
      "37804: done 2470 episodes, mean reward 0.012, speed 61.63 f/s\n",
      "37869: done 2474 episodes, mean reward 0.012, speed 62.43 f/s\n",
      "37940: done 2479 episodes, mean reward 0.011, speed 63.71 f/s\n",
      "38011: done 2484 episodes, mean reward 0.010, speed 62.48 f/s\n",
      "38122: done 2488 episodes, mean reward 0.010, speed 61.85 f/s\n",
      "38199: done 2492 episodes, mean reward 0.010, speed 61.43 f/s\n",
      "38274: done 2497 episodes, mean reward 0.010, speed 60.78 f/s\n",
      "38345: done 2502 episodes, mean reward 0.010, speed 62.45 f/s\n",
      "38417: done 2506 episodes, mean reward 0.010, speed 59.96 f/s\n",
      "38505: done 2511 episodes, mean reward 0.010, speed 65.54 f/s\n",
      "38581: done 2515 episodes, mean reward 0.010, speed 61.76 f/s\n",
      "38644: done 2518 episodes, mean reward 0.009, speed 62.05 f/s\n",
      "38733: done 2522 episodes, mean reward 0.007, speed 61.81 f/s\n",
      "38806: done 2525 episodes, mean reward 0.008, speed 64.41 f/s\n",
      "38880: done 2528 episodes, mean reward 0.009, speed 38.35 f/s\n",
      "38954: done 2532 episodes, mean reward 0.008, speed 62.66 f/s\n",
      "39051: done 2534 episodes, mean reward 0.009, speed 63.19 f/s\n",
      "39122: done 2539 episodes, mean reward 0.008, speed 61.33 f/s\n",
      "39195: done 2543 episodes, mean reward 0.007, speed 64.50 f/s\n",
      "39264: done 2548 episodes, mean reward 0.004, speed 65.13 f/s\n",
      "39339: done 2551 episodes, mean reward 0.005, speed 63.35 f/s\n",
      "39425: done 2556 episodes, mean reward 0.005, speed 64.89 f/s\n",
      "39519: done 2559 episodes, mean reward 0.006, speed 63.15 f/s\n",
      "39606: done 2564 episodes, mean reward 0.007, speed 62.53 f/s\n",
      "39694: done 2568 episodes, mean reward 0.008, speed 63.21 f/s\n",
      "39765: done 2572 episodes, mean reward 0.008, speed 63.06 f/s\n",
      "39839: done 2576 episodes, mean reward 0.008, speed 65.24 f/s\n",
      "39915: done 2579 episodes, mean reward 0.009, speed 66.30 f/s\n",
      "39987: done 2583 episodes, mean reward 0.010, speed 62.41 f/s\n",
      "Test done in 1.61 sec, reward 0.008, steps 17\n",
      "Best reward updated: -0.005 -> 0.008\n",
      "40017: done 2584 episodes, mean reward 0.010, speed 14.23 f/s\n",
      "40091: done 2588 episodes, mean reward 0.009, speed 62.29 f/s\n",
      "40161: done 2593 episodes, mean reward 0.009, speed 65.28 f/s\n",
      "40235: done 2598 episodes, mean reward 0.009, speed 64.56 f/s\n",
      "40305: done 2602 episodes, mean reward 0.009, speed 64.41 f/s\n",
      "40395: done 2606 episodes, mean reward 0.009, speed 63.62 f/s\n",
      "40466: done 2611 episodes, mean reward 0.009, speed 65.45 f/s\n",
      "40539: done 2615 episodes, mean reward 0.009, speed 64.30 f/s\n",
      "40616: done 2619 episodes, mean reward 0.010, speed 64.23 f/s\n",
      "40686: done 2624 episodes, mean reward 0.008, speed 61.39 f/s\n",
      "40756: done 2629 episodes, mean reward 0.007, speed 66.10 f/s\n",
      "40828: done 2633 episodes, mean reward 0.007, speed 63.64 f/s\n",
      "40905: done 2638 episodes, mean reward 0.006, speed 61.34 f/s\n",
      "40969: done 2641 episodes, mean reward 0.006, speed 63.50 f/s\n",
      "41042: done 2645 episodes, mean reward 0.007, speed 61.66 f/s\n",
      "41113: done 2650 episodes, mean reward 0.006, speed 61.67 f/s\n",
      "41188: done 2655 episodes, mean reward 0.006, speed 62.07 f/s\n",
      "41253: done 2657 episodes, mean reward 0.006, speed 62.49 f/s\n",
      "41325: done 2661 episodes, mean reward 0.005, speed 64.75 f/s\n",
      "41391: done 2665 episodes, mean reward 0.005, speed 64.28 f/s\n",
      "41464: done 2669 episodes, mean reward 0.005, speed 66.78 f/s\n",
      "41555: done 2673 episodes, mean reward 0.005, speed 63.22 f/s\n",
      "41630: done 2678 episodes, mean reward 0.005, speed 62.98 f/s\n",
      "41701: done 2683 episodes, mean reward 0.004, speed 64.63 f/s\n",
      "41777: done 2687 episodes, mean reward 0.005, speed 67.68 f/s\n",
      "41862: done 2690 episodes, mean reward 0.005, speed 67.02 f/s\n",
      "41933: done 2695 episodes, mean reward 0.005, speed 66.77 f/s\n",
      "42007: done 2699 episodes, mean reward 0.006, speed 69.98 f/s\n",
      "42080: done 2703 episodes, mean reward 0.006, speed 65.70 f/s\n",
      "42182: done 2709 episodes, mean reward 0.006, speed 69.33 f/s\n",
      "42274: done 2713 episodes, mean reward 0.006, speed 66.96 f/s\n",
      "42361: done 2718 episodes, mean reward 0.006, speed 67.02 f/s\n",
      "42437: done 2721 episodes, mean reward 0.007, speed 65.45 f/s\n",
      "42518: done 2724 episodes, mean reward 0.008, speed 67.64 f/s\n",
      "42591: done 2728 episodes, mean reward 0.008, speed 67.30 f/s\n",
      "42662: done 2733 episodes, mean reward 0.008, speed 70.65 f/s\n",
      "42733: done 2738 episodes, mean reward 0.008, speed 68.03 f/s\n",
      "42818: done 2744 episodes, mean reward 0.006, speed 71.08 f/s\n",
      "42889: done 2749 episodes, mean reward 0.006, speed 65.90 f/s\n",
      "42963: done 2753 episodes, mean reward 0.007, speed 70.98 f/s\n",
      "43037: done 2757 episodes, mean reward 0.007, speed 65.29 f/s\n",
      "43109: done 2761 episodes, mean reward 0.007, speed 70.15 f/s\n",
      "43182: done 2765 episodes, mean reward 0.007, speed 68.96 f/s\n",
      "43254: done 2770 episodes, mean reward 0.006, speed 69.30 f/s\n",
      "43325: done 2775 episodes, mean reward 0.005, speed 67.04 f/s\n",
      "43415: done 2779 episodes, mean reward 0.006, speed 71.33 f/s\n",
      "43486: done 2784 episodes, mean reward 0.006, speed 68.66 f/s\n",
      "43557: done 2789 episodes, mean reward 0.004, speed 66.34 f/s\n",
      "43628: done 2794 episodes, mean reward 0.004, speed 68.66 f/s\n",
      "43701: done 2798 episodes, mean reward 0.004, speed 68.65 f/s\n",
      "43787: done 2804 episodes, mean reward 0.003, speed 71.59 f/s\n",
      "43860: done 2809 episodes, mean reward 0.003, speed 67.79 f/s\n",
      "43948: done 2813 episodes, mean reward 0.003, speed 69.66 f/s\n",
      "44023: done 2816 episodes, mean reward 0.004, speed 67.56 f/s\n",
      "44098: done 2820 episodes, mean reward 0.003, speed 69.70 f/s\n",
      "44206: done 2825 episodes, mean reward 0.003, speed 69.08 f/s\n",
      "44305: done 2830 episodes, mean reward 0.003, speed 69.37 f/s\n",
      "44385: done 2834 episodes, mean reward 0.004, speed 66.85 f/s\n",
      "44458: done 2838 episodes, mean reward 0.004, speed 71.58 f/s\n",
      "44532: done 2842 episodes, mean reward 0.005, speed 69.07 f/s\n",
      "44605: done 2846 episodes, mean reward 0.005, speed 67.97 f/s\n",
      "44676: done 2851 episodes, mean reward 0.005, speed 69.79 f/s\n",
      "44763: done 2857 episodes, mean reward 0.004, speed 72.17 f/s\n",
      "44834: done 2861 episodes, mean reward 0.004, speed 69.45 f/s\n",
      "44907: done 2865 episodes, mean reward 0.004, speed 68.11 f/s\n",
      "44981: done 2869 episodes, mean reward 0.005, speed 67.98 f/s\n",
      "45052: done 2874 episodes, mean reward 0.005, speed 68.55 f/s\n",
      "45124: done 2879 episodes, mean reward 0.004, speed 66.81 f/s\n",
      "45204: done 2883 episodes, mean reward 0.004, speed 69.53 f/s\n",
      "45297: done 2888 episodes, mean reward 0.005, speed 68.87 f/s\n",
      "45371: done 2892 episodes, mean reward 0.005, speed 65.79 f/s\n",
      "45453: done 2895 episodes, mean reward 0.006, speed 70.89 f/s\n",
      "45526: done 2899 episodes, mean reward 0.006, speed 69.24 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45608: done 2904 episodes, mean reward 0.006, speed 69.68 f/s\n",
      "45681: done 2908 episodes, mean reward 0.007, speed 69.43 f/s\n",
      "45756: done 2911 episodes, mean reward 0.007, speed 68.98 f/s\n",
      "45831: done 2914 episodes, mean reward 0.007, speed 71.43 f/s\n",
      "45902: done 2919 episodes, mean reward 0.006, speed 67.91 f/s\n",
      "45991: done 2924 episodes, mean reward 0.006, speed 69.34 f/s\n",
      "46066: done 2927 episodes, mean reward 0.006, speed 66.86 f/s\n",
      "46152: done 2933 episodes, mean reward 0.004, speed 72.75 f/s\n",
      "46231: done 2937 episodes, mean reward 0.005, speed 67.72 f/s\n",
      "46304: done 2941 episodes, mean reward 0.004, speed 70.66 f/s\n",
      "46387: done 2943 episodes, mean reward 0.005, speed 68.73 f/s\n",
      "46460: done 2946 episodes, mean reward 0.006, speed 64.35 f/s\n",
      "46536: done 2950 episodes, mean reward 0.006, speed 71.95 f/s\n",
      "46608: done 2954 episodes, mean reward 0.007, speed 69.56 f/s\n",
      "46689: done 2959 episodes, mean reward 0.007, speed 68.56 f/s\n",
      "46763: done 2962 episodes, mean reward 0.007, speed 68.57 f/s\n",
      "46837: done 2966 episodes, mean reward 0.007, speed 68.75 f/s\n",
      "46919: done 2970 episodes, mean reward 0.007, speed 68.54 f/s\n",
      "47014: done 2974 episodes, mean reward 0.008, speed 69.93 f/s\n",
      "47103: done 2978 episodes, mean reward 0.009, speed 70.05 f/s\n",
      "47179: done 2981 episodes, mean reward 0.010, speed 67.66 f/s\n",
      "47260: done 2984 episodes, mean reward 0.011, speed 68.88 f/s\n",
      "47346: done 2989 episodes, mean reward 0.011, speed 67.31 f/s\n",
      "47423: done 2994 episodes, mean reward 0.010, speed 67.97 f/s\n",
      "47498: done 2997 episodes, mean reward 0.010, speed 69.81 f/s\n",
      "47575: done 3001 episodes, mean reward 0.010, speed 69.37 f/s\n",
      "47646: done 3006 episodes, mean reward 0.010, speed 70.10 f/s\n",
      "47719: done 3010 episodes, mean reward 0.010, speed 66.15 f/s\n",
      "47812: done 3014 episodes, mean reward 0.009, speed 68.53 f/s\n",
      "47884: done 3018 episodes, mean reward 0.009, speed 71.58 f/s\n",
      "47961: done 3021 episodes, mean reward 0.010, speed 68.54 f/s\n",
      "48046: done 3027 episodes, mean reward 0.010, speed 72.96 f/s\n",
      "48130: done 3031 episodes, mean reward 0.010, speed 64.44 f/s\n",
      "48204: done 3035 episodes, mean reward 0.010, speed 68.06 f/s\n",
      "48278: done 3039 episodes, mean reward 0.010, speed 67.08 f/s\n",
      "48353: done 3042 episodes, mean reward 0.011, speed 67.84 f/s\n",
      "48428: done 3044 episodes, mean reward 0.011, speed 66.23 f/s\n",
      "48501: done 3048 episodes, mean reward 0.011, speed 67.13 f/s\n",
      "48572: done 3053 episodes, mean reward 0.010, speed 70.31 f/s\n",
      "48651: done 3057 episodes, mean reward 0.010, speed 66.37 f/s\n",
      "48739: done 3061 episodes, mean reward 0.010, speed 69.49 f/s\n",
      "48811: done 3065 episodes, mean reward 0.010, speed 67.65 f/s\n",
      "48884: done 3069 episodes, mean reward 0.011, speed 71.19 f/s\n",
      "48957: done 3073 episodes, mean reward 0.011, speed 66.52 f/s\n",
      "49031: done 3077 episodes, mean reward 0.010, speed 67.12 f/s\n",
      "49104: done 3082 episodes, mean reward 0.008, speed 66.33 f/s\n",
      "49179: done 3086 episodes, mean reward 0.008, speed 70.40 f/s\n",
      "49269: done 3090 episodes, mean reward 0.008, speed 68.84 f/s\n",
      "49348: done 3094 episodes, mean reward 0.008, speed 64.22 f/s\n",
      "49421: done 3098 episodes, mean reward 0.007, speed 69.26 f/s\n",
      "49493: done 3103 episodes, mean reward 0.007, speed 69.52 f/s\n",
      "49581: done 3108 episodes, mean reward 0.008, speed 73.50 f/s\n",
      "49691: done 3112 episodes, mean reward 0.009, speed 72.45 f/s\n",
      "49765: done 3115 episodes, mean reward 0.009, speed 67.75 f/s\n",
      "49841: done 3119 episodes, mean reward 0.009, speed 71.77 f/s\n",
      "49913: done 3123 episodes, mean reward 0.009, speed 67.49 f/s\n",
      "Test done in 1.50 sec, reward 0.013, steps 20\n",
      "Best reward updated: 0.008 -> 0.013\n",
      "50016: done 3128 episodes, mean reward 0.009, speed 34.48 f/s\n",
      "50107: done 3131 episodes, mean reward 0.010, speed 66.38 f/s\n",
      "50179: done 3135 episodes, mean reward 0.010, speed 66.47 f/s\n",
      "50251: done 3139 episodes, mean reward 0.010, speed 67.02 f/s\n",
      "50340: done 3142 episodes, mean reward 0.010, speed 68.38 f/s\n",
      "50412: done 3146 episodes, mean reward 0.009, speed 68.73 f/s\n",
      "50487: done 3151 episodes, mean reward 0.009, speed 66.85 f/s\n",
      "50561: done 3154 episodes, mean reward 0.010, speed 68.47 f/s\n",
      "50648: done 3159 episodes, mean reward 0.010, speed 71.62 f/s\n",
      "50722: done 3163 episodes, mean reward 0.009, speed 69.81 f/s\n",
      "50795: done 3167 episodes, mean reward 0.009, speed 66.48 f/s\n",
      "50886: done 3171 episodes, mean reward 0.008, speed 71.75 f/s\n",
      "50962: done 3174 episodes, mean reward 0.009, speed 68.99 f/s\n",
      "51048: done 3178 episodes, mean reward 0.009, speed 68.34 f/s\n",
      "51121: done 3181 episodes, mean reward 0.010, speed 70.79 f/s\n",
      "51232: done 3185 episodes, mean reward 0.011, speed 71.87 f/s\n",
      "51320: done 3189 episodes, mean reward 0.012, speed 69.93 f/s\n",
      "51393: done 3193 episodes, mean reward 0.012, speed 70.32 f/s\n",
      "51481: done 3197 episodes, mean reward 0.012, speed 65.52 f/s\n",
      "51555: done 3201 episodes, mean reward 0.013, speed 72.42 f/s\n",
      "51640: done 3206 episodes, mean reward 0.013, speed 68.58 f/s\n",
      "51738: done 3210 episodes, mean reward 0.014, speed 71.15 f/s\n",
      "51809: done 3215 episodes, mean reward 0.011, speed 70.86 f/s\n",
      "51900: done 3219 episodes, mean reward 0.012, speed 69.97 f/s\n",
      "51981: done 3222 episodes, mean reward 0.012, speed 71.08 f/s\n",
      "52061: done 3225 episodes, mean reward 0.013, speed 70.71 f/s\n",
      "52134: done 3229 episodes, mean reward 0.013, speed 68.64 f/s\n",
      "52208: done 3233 episodes, mean reward 0.012, speed 66.58 f/s\n",
      "52282: done 3236 episodes, mean reward 0.013, speed 64.79 f/s\n",
      "52355: done 3239 episodes, mean reward 0.013, speed 71.65 f/s\n",
      "52429: done 3243 episodes, mean reward 0.013, speed 67.69 f/s\n",
      "52502: done 3246 episodes, mean reward 0.013, speed 68.53 f/s\n",
      "52582: done 3249 episodes, mean reward 0.014, speed 69.21 f/s\n",
      "52661: done 3252 episodes, mean reward 0.015, speed 68.42 f/s\n",
      "52735: done 3256 episodes, mean reward 0.014, speed 68.37 f/s\n",
      "52808: done 3260 episodes, mean reward 0.014, speed 71.27 f/s\n",
      "52881: done 3264 episodes, mean reward 0.014, speed 69.62 f/s\n",
      "52955: done 3269 episodes, mean reward 0.014, speed 70.54 f/s\n",
      "53027: done 3273 episodes, mean reward 0.014, speed 68.22 f/s\n",
      "53121: done 3278 episodes, mean reward 0.013, speed 69.62 f/s\n",
      "53206: done 3284 episodes, mean reward 0.012, speed 70.68 f/s\n",
      "53278: done 3288 episodes, mean reward 0.011, speed 69.86 f/s\n",
      "53354: done 3291 episodes, mean reward 0.011, speed 70.33 f/s\n",
      "53444: done 3295 episodes, mean reward 0.011, speed 69.57 f/s\n",
      "53529: done 3300 episodes, mean reward 0.011, speed 68.14 f/s\n",
      "53603: done 3304 episodes, mean reward 0.011, speed 70.29 f/s\n",
      "53675: done 3307 episodes, mean reward 0.012, speed 64.23 f/s\n",
      "53753: done 3310 episodes, mean reward 0.012, speed 69.40 f/s\n",
      "53835: done 3312 episodes, mean reward 0.013, speed 68.80 f/s\n",
      "53922: done 3316 episodes, mean reward 0.014, speed 69.17 f/s\n",
      "54016: done 3320 episodes, mean reward 0.014, speed 72.09 f/s\n",
      "54091: done 3324 episodes, mean reward 0.013, speed 67.87 f/s\n",
      "54184: done 3328 episodes, mean reward 0.014, speed 66.85 f/s\n",
      "54255: done 3333 episodes, mean reward 0.013, speed 66.80 f/s\n",
      "54348: done 3336 episodes, mean reward 0.014, speed 70.65 f/s\n",
      "54439: done 3340 episodes, mean reward 0.014, speed 71.44 f/s\n",
      "54510: done 3345 episodes, mean reward 0.013, speed 66.33 f/s\n",
      "54586: done 3348 episodes, mean reward 0.013, speed 69.18 f/s\n",
      "54678: done 3351 episodes, mean reward 0.014, speed 66.63 f/s\n",
      "54752: done 3355 episodes, mean reward 0.013, speed 66.34 f/s\n",
      "54834: done 3359 episodes, mean reward 0.014, speed 69.26 f/s\n",
      "54911: done 3363 episodes, mean reward 0.013, speed 66.63 f/s\n",
      "55015: done 3369 episodes, mean reward 0.014, speed 72.93 f/s\n",
      "55107: done 3372 episodes, mean reward 0.015, speed 74.31 f/s\n",
      "55180: done 3377 episodes, mean reward 0.015, speed 66.33 f/s\n",
      "55276: done 3380 episodes, mean reward 0.016, speed 69.27 f/s\n",
      "55351: done 3383 episodes, mean reward 0.017, speed 68.76 f/s\n",
      "55442: done 3387 episodes, mean reward 0.017, speed 68.46 f/s\n",
      "55517: done 3390 episodes, mean reward 0.017, speed 71.97 f/s\n",
      "55589: done 3393 episodes, mean reward 0.018, speed 67.05 f/s\n",
      "55664: done 3396 episodes, mean reward 0.018, speed 70.81 f/s\n",
      "55737: done 3400 episodes, mean reward 0.018, speed 70.20 f/s\n",
      "55823: done 3405 episodes, mean reward 0.018, speed 71.43 f/s\n",
      "55934: done 3409 episodes, mean reward 0.019, speed 70.06 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56024: done 3413 episodes, mean reward 0.017, speed 69.87 f/s\n",
      "56105: done 3416 episodes, mean reward 0.017, speed 71.94 f/s\n",
      "56222: done 3419 episodes, mean reward 0.019, speed 69.05 f/s\n",
      "56312: done 3420 episodes, mean reward 0.020, speed 66.70 f/s\n",
      "56383: done 3423 episodes, mean reward 0.020, speed 68.28 f/s\n",
      "56459: done 3426 episodes, mean reward 0.021, speed 65.31 f/s\n",
      "56535: done 3430 episodes, mean reward 0.021, speed 67.26 f/s\n",
      "56626: done 3434 episodes, mean reward 0.021, speed 68.58 f/s\n",
      "56708: done 3436 episodes, mean reward 0.022, speed 67.82 f/s\n",
      "56799: done 3440 episodes, mean reward 0.022, speed 70.78 f/s\n",
      "56875: done 3442 episodes, mean reward 0.022, speed 65.33 f/s\n",
      "56953: done 3445 episodes, mean reward 0.023, speed 70.22 f/s\n",
      "57025: done 3449 episodes, mean reward 0.022, speed 71.43 f/s\n",
      "57113: done 3454 episodes, mean reward 0.022, speed 69.40 f/s\n",
      "57187: done 3458 episodes, mean reward 0.022, speed 72.70 f/s\n",
      "57270: done 3460 episodes, mean reward 0.023, speed 69.47 f/s\n",
      "57361: done 3463 episodes, mean reward 0.024, speed 68.35 f/s\n",
      "57437: done 3466 episodes, mean reward 0.025, speed 66.40 f/s\n",
      "57505: done 3468 episodes, mean reward 0.026, speed 66.56 f/s\n",
      "57576: done 3473 episodes, mean reward 0.024, speed 67.55 f/s\n",
      "57708: done 3476 episodes, mean reward 0.026, speed 67.10 f/s\n",
      "57793: done 3478 episodes, mean reward 0.027, speed 72.59 f/s\n",
      "57884: done 3482 episodes, mean reward 0.027, speed 68.42 f/s\n",
      "57960: done 3485 episodes, mean reward 0.027, speed 67.99 f/s\n",
      "58041: done 3487 episodes, mean reward 0.028, speed 70.19 f/s\n",
      "58133: done 3491 episodes, mean reward 0.027, speed 68.44 f/s\n",
      "58210: done 3495 episodes, mean reward 0.027, speed 67.32 f/s\n",
      "58293: done 3497 episodes, mean reward 0.028, speed 68.66 f/s\n",
      "58375: done 3499 episodes, mean reward 0.029, speed 68.20 f/s\n",
      "58461: done 3504 episodes, mean reward 0.029, speed 69.47 f/s\n",
      "58551: done 3508 episodes, mean reward 0.029, speed 68.35 f/s\n",
      "58669: done 3511 episodes, mean reward 0.030, speed 69.21 f/s\n",
      "58743: done 3514 episodes, mean reward 0.029, speed 69.35 f/s\n",
      "58814: done 3515 episodes, mean reward 0.031, speed 66.59 f/s\n",
      "58888: done 3519 episodes, mean reward 0.030, speed 67.78 f/s\n",
      "58962: done 3522 episodes, mean reward 0.028, speed 69.66 f/s\n",
      "59059: done 3525 episodes, mean reward 0.028, speed 68.26 f/s\n",
      "59161: done 3528 episodes, mean reward 0.029, speed 70.23 f/s\n",
      "59265: done 3530 episodes, mean reward 0.031, speed 71.76 f/s\n",
      "59337: done 3535 episodes, mean reward 0.029, speed 66.47 f/s\n",
      "59438: done 3538 episodes, mean reward 0.029, speed 69.42 f/s\n",
      "59566: done 3542 episodes, mean reward 0.030, speed 68.64 f/s\n",
      "59682: done 3545 episodes, mean reward 0.030, speed 70.69 f/s\n",
      "59762: done 3548 episodes, mean reward 0.031, speed 68.43 f/s\n",
      "59858: done 3551 episodes, mean reward 0.032, speed 68.84 f/s\n",
      "59957: done 3554 episodes, mean reward 0.033, speed 67.57 f/s\n",
      "Test done in 2.16 sec, reward 0.032, steps 27\n",
      "Best reward updated: 0.013 -> 0.032\n",
      "60032: done 3555 episodes, mean reward 0.034, speed 22.87 f/s\n",
      "60120: done 3560 episodes, mean reward 0.033, speed 69.76 f/s\n",
      "60200: done 3562 episodes, mean reward 0.033, speed 67.54 f/s\n",
      "60290: done 3566 episodes, mean reward 0.033, speed 67.25 f/s\n",
      "60377: done 3571 episodes, mean reward 0.032, speed 70.12 f/s\n",
      "60459: done 3573 episodes, mean reward 0.034, speed 68.54 f/s\n",
      "60531: done 3577 episodes, mean reward 0.032, speed 70.33 f/s\n",
      "60604: done 3581 episodes, mean reward 0.030, speed 69.30 f/s\n",
      "60678: done 3584 episodes, mean reward 0.030, speed 69.84 f/s\n",
      "60789: done 3588 episodes, mean reward 0.030, speed 68.83 f/s\n",
      "60860: done 3593 episodes, mean reward 0.029, speed 69.47 f/s\n",
      "60932: done 3597 episodes, mean reward 0.027, speed 71.34 f/s\n",
      "61000: done 3599 episodes, mean reward 0.027, speed 67.80 f/s\n",
      "61082: done 3601 episodes, mean reward 0.028, speed 71.41 f/s\n",
      "61162: done 3604 episodes, mean reward 0.029, speed 69.76 f/s\n",
      "61237: done 3607 episodes, mean reward 0.029, speed 68.54 f/s\n",
      "61309: done 3611 episodes, mean reward 0.027, speed 68.02 f/s\n",
      "61398: done 3615 episodes, mean reward 0.025, speed 66.85 f/s\n",
      "61487: done 3617 episodes, mean reward 0.027, speed 71.15 f/s\n",
      "61560: done 3619 episodes, mean reward 0.028, speed 68.71 f/s\n",
      "61638: done 3623 episodes, mean reward 0.027, speed 67.06 f/s\n",
      "61734: done 3627 episodes, mean reward 0.026, speed 66.63 f/s\n",
      "61830: done 3630 episodes, mean reward 0.025, speed 68.54 f/s\n",
      "61902: done 3634 episodes, mean reward 0.025, speed 65.65 f/s\n",
      "61978: done 3637 episodes, mean reward 0.025, speed 69.58 f/s\n",
      "62125: done 3641 episodes, mean reward 0.027, speed 71.47 f/s\n",
      "62198: done 3645 episodes, mean reward 0.025, speed 69.10 f/s\n",
      "62289: done 3649 episodes, mean reward 0.025, speed 68.64 f/s\n",
      "62361: done 3653 episodes, mean reward 0.023, speed 66.28 f/s\n",
      "62433: done 3657 episodes, mean reward 0.021, speed 68.88 f/s\n",
      "62508: done 3660 episodes, mean reward 0.022, speed 68.61 f/s\n",
      "62611: done 3662 episodes, mean reward 0.022, speed 66.70 f/s\n",
      "62705: done 3666 episodes, mean reward 0.022, speed 69.79 f/s\n",
      "62793: done 3670 episodes, mean reward 0.023, speed 69.36 f/s\n",
      "62864: done 3675 episodes, mean reward 0.021, speed 66.06 f/s\n",
      "62961: done 3678 episodes, mean reward 0.022, speed 67.65 f/s\n",
      "63056: done 3681 episodes, mean reward 0.023, speed 69.08 f/s\n",
      "63130: done 3685 episodes, mean reward 0.022, speed 71.73 f/s\n",
      "63201: done 3690 episodes, mean reward 0.021, speed 65.69 f/s\n",
      "63289: done 3695 episodes, mean reward 0.021, speed 66.17 f/s\n",
      "63362: done 3699 episodes, mean reward 0.020, speed 69.23 f/s\n",
      "63435: done 3703 episodes, mean reward 0.018, speed 65.76 f/s\n",
      "63518: done 3709 episodes, mean reward 0.017, speed 70.41 f/s\n",
      "63614: done 3713 episodes, mean reward 0.018, speed 67.59 f/s\n",
      "63692: done 3716 episodes, mean reward 0.018, speed 67.58 f/s\n",
      "63773: done 3719 episodes, mean reward 0.016, speed 67.45 f/s\n",
      "63870: done 3722 episodes, mean reward 0.017, speed 68.14 f/s\n",
      "63961: done 3725 episodes, mean reward 0.018, speed 69.74 f/s\n",
      "64037: done 3727 episodes, mean reward 0.020, speed 66.95 f/s\n",
      "64120: done 3731 episodes, mean reward 0.019, speed 68.89 f/s\n",
      "64193: done 3735 episodes, mean reward 0.019, speed 68.38 f/s\n",
      "64265: done 3740 episodes, mean reward 0.018, speed 71.30 f/s\n",
      "64353: done 3744 episodes, mean reward 0.016, speed 68.27 f/s\n",
      "64426: done 3748 episodes, mean reward 0.016, speed 70.30 f/s\n",
      "64511: done 3750 episodes, mean reward 0.017, speed 68.67 f/s\n",
      "64584: done 3754 episodes, mean reward 0.017, speed 68.98 f/s\n",
      "64665: done 3757 episodes, mean reward 0.018, speed 67.43 f/s\n",
      "64761: done 3760 episodes, mean reward 0.018, speed 68.03 f/s\n",
      "64851: done 3764 episodes, mean reward 0.017, speed 69.12 f/s\n",
      "64940: done 3768 episodes, mean reward 0.017, speed 68.79 f/s\n",
      "65013: done 3772 episodes, mean reward 0.017, speed 72.31 f/s\n",
      "65179: done 3776 episodes, mean reward 0.019, speed 67.38 f/s\n",
      "65253: done 3780 episodes, mean reward 0.018, speed 69.65 f/s\n",
      "65379: done 3784 episodes, mean reward 0.019, speed 68.86 f/s\n",
      "65455: done 3787 episodes, mean reward 0.020, speed 69.43 f/s\n",
      "65529: done 3791 episodes, mean reward 0.021, speed 69.75 f/s\n",
      "65600: done 3794 episodes, mean reward 0.021, speed 70.69 f/s\n",
      "65690: done 3798 episodes, mean reward 0.021, speed 70.71 f/s\n",
      "65761: done 3802 episodes, mean reward 0.021, speed 70.65 f/s\n",
      "65835: done 3803 episodes, mean reward 0.023, speed 70.44 f/s\n",
      "65909: done 3807 episodes, mean reward 0.023, speed 65.67 f/s\n",
      "65994: done 3812 episodes, mean reward 0.024, speed 72.11 f/s\n",
      "66064: done 3816 episodes, mean reward 0.022, speed 69.49 f/s\n",
      "66138: done 3820 episodes, mean reward 0.020, speed 71.17 f/s\n",
      "66206: done 3824 episodes, mean reward 0.019, speed 66.34 f/s\n",
      "66283: done 3827 episodes, mean reward 0.018, speed 67.67 f/s\n",
      "66360: done 3829 episodes, mean reward 0.019, speed 70.79 f/s\n",
      "66434: done 3833 episodes, mean reward 0.019, speed 70.24 f/s\n",
      "66508: done 3836 episodes, mean reward 0.019, speed 67.55 f/s\n",
      "66582: done 3840 episodes, mean reward 0.020, speed 68.82 f/s\n",
      "66655: done 3844 episodes, mean reward 0.019, speed 71.10 f/s\n",
      "66727: done 3848 episodes, mean reward 0.019, speed 67.27 f/s\n",
      "66818: done 3853 episodes, mean reward 0.018, speed 71.92 f/s\n",
      "66888: done 3857 episodes, mean reward 0.018, speed 68.71 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66964: done 3860 episodes, mean reward 0.017, speed 68.77 f/s\n",
      "67040: done 3863 episodes, mean reward 0.018, speed 67.73 f/s\n",
      "67129: done 3867 episodes, mean reward 0.018, speed 71.11 f/s\n",
      "67205: done 3871 episodes, mean reward 0.017, speed 68.78 f/s\n",
      "67275: done 3876 episodes, mean reward 0.014, speed 68.85 f/s\n",
      "67367: done 3879 episodes, mean reward 0.015, speed 73.32 f/s\n",
      "67457: done 3883 episodes, mean reward 0.016, speed 66.83 f/s\n",
      "67532: done 3887 episodes, mean reward 0.014, speed 70.20 f/s\n",
      "67605: done 3891 episodes, mean reward 0.014, speed 67.86 f/s\n",
      "67687: done 3893 episodes, mean reward 0.015, speed 70.77 f/s\n",
      "67775: done 3896 episodes, mean reward 0.015, speed 67.98 f/s\n",
      "67845: done 3900 episodes, mean reward 0.015, speed 65.43 f/s\n",
      "67917: done 3905 episodes, mean reward 0.013, speed 69.52 f/s\n",
      "68008: done 3908 episodes, mean reward 0.014, speed 66.02 f/s\n",
      "68081: done 3911 episodes, mean reward 0.015, speed 69.52 f/s\n",
      "68154: done 3915 episodes, mean reward 0.015, speed 66.39 f/s\n",
      "68230: done 3918 episodes, mean reward 0.015, speed 71.60 f/s\n",
      "68304: done 3922 episodes, mean reward 0.016, speed 68.53 f/s\n",
      "68393: done 3925 episodes, mean reward 0.017, speed 71.95 f/s\n",
      "68468: done 3928 episodes, mean reward 0.017, speed 69.45 f/s\n",
      "68548: done 3931 episodes, mean reward 0.017, speed 67.92 f/s\n",
      "68620: done 3935 episodes, mean reward 0.016, speed 70.75 f/s\n",
      "68710: done 3939 episodes, mean reward 0.016, speed 67.91 f/s\n",
      "68801: done 3943 episodes, mean reward 0.017, speed 70.86 f/s\n",
      "68890: done 3947 episodes, mean reward 0.017, speed 69.29 f/s\n",
      "68982: done 3951 episodes, mean reward 0.018, speed 68.04 f/s\n",
      "69055: done 3955 episodes, mean reward 0.018, speed 70.01 f/s\n",
      "69146: done 3959 episodes, mean reward 0.018, speed 70.69 f/s\n",
      "69219: done 3962 episodes, mean reward 0.018, speed 70.02 f/s\n",
      "69307: done 3967 episodes, mean reward 0.017, speed 67.25 f/s\n",
      "69394: done 3972 episodes, mean reward 0.017, speed 66.72 f/s\n",
      "69496: done 3977 episodes, mean reward 0.018, speed 71.11 f/s\n",
      "69570: done 3981 episodes, mean reward 0.017, speed 68.63 f/s\n",
      "69704: done 3984 episodes, mean reward 0.018, speed 68.74 f/s\n",
      "69777: done 3988 episodes, mean reward 0.019, speed 67.38 f/s\n",
      "69870: done 3991 episodes, mean reward 0.020, speed 68.18 f/s\n",
      "69945: done 3995 episodes, mean reward 0.018, speed 66.50 f/s\n",
      "Test done in 1.65 sec, reward 0.018, steps 23\n",
      "70018: done 3996 episodes, mean reward 0.019, speed 26.47 f/s\n",
      "70132: done 3999 episodes, mean reward 0.021, speed 67.59 f/s\n",
      "70205: done 4003 episodes, mean reward 0.021, speed 65.38 f/s\n",
      "70285: done 4006 episodes, mean reward 0.021, speed 67.35 f/s\n",
      "70374: done 4010 episodes, mean reward 0.021, speed 68.09 f/s\n",
      "70448: done 4013 episodes, mean reward 0.021, speed 70.70 f/s\n",
      "70527: done 4016 episodes, mean reward 0.021, speed 68.57 f/s\n",
      "70610: done 4019 episodes, mean reward 0.021, speed 64.85 f/s\n",
      "70731: done 4021 episodes, mean reward 0.024, speed 67.39 f/s\n",
      "70822: done 4024 episodes, mean reward 0.024, speed 68.77 f/s\n",
      "70913: done 4028 episodes, mean reward 0.023, speed 69.99 f/s\n",
      "71009: done 4031 episodes, mean reward 0.024, speed 68.39 f/s\n",
      "71082: done 4036 episodes, mean reward 0.023, speed 68.82 f/s\n",
      "71170: done 4040 episodes, mean reward 0.023, speed 68.66 f/s\n",
      "71243: done 4044 episodes, mean reward 0.022, speed 66.56 f/s\n",
      "71321: done 4047 episodes, mean reward 0.023, speed 69.29 f/s\n",
      "71424: done 4052 episodes, mean reward 0.023, speed 69.95 f/s\n",
      "71501: done 4056 episodes, mean reward 0.023, speed 71.25 f/s\n",
      "71590: done 4060 episodes, mean reward 0.022, speed 67.00 f/s\n",
      "71663: done 4064 episodes, mean reward 0.022, speed 68.26 f/s\n",
      "71745: done 4066 episodes, mean reward 0.024, speed 68.77 f/s\n",
      "71896: done 4069 episodes, mean reward 0.026, speed 69.19 f/s\n",
      "71971: done 4072 episodes, mean reward 0.027, speed 69.52 f/s\n",
      "72043: done 4076 episodes, mean reward 0.027, speed 71.26 f/s\n",
      "72112: done 4080 episodes, mean reward 0.026, speed 66.12 f/s\n",
      "72183: done 4085 episodes, mean reward 0.024, speed 70.93 f/s\n",
      "72271: done 4089 episodes, mean reward 0.024, speed 67.00 f/s\n",
      "72346: done 4092 episodes, mean reward 0.024, speed 69.17 f/s\n",
      "72426: done 4095 episodes, mean reward 0.024, speed 67.40 f/s\n",
      "72518: done 4098 episodes, mean reward 0.023, speed 72.60 f/s\n",
      "72609: done 4102 episodes, mean reward 0.022, speed 68.69 f/s\n",
      "72676: done 4104 episodes, mean reward 0.023, speed 65.83 f/s\n",
      "72761: done 4106 episodes, mean reward 0.024, speed 71.76 f/s\n",
      "72839: done 4109 episodes, mean reward 0.025, speed 65.25 f/s\n",
      "72949: done 4113 episodes, mean reward 0.025, speed 69.00 f/s\n",
      "73042: done 4116 episodes, mean reward 0.026, speed 72.67 f/s\n",
      "73141: done 4118 episodes, mean reward 0.027, speed 68.05 f/s\n",
      "73214: done 4122 episodes, mean reward 0.024, speed 69.02 f/s\n",
      "73305: done 4125 episodes, mean reward 0.025, speed 71.15 f/s\n",
      "73394: done 4129 episodes, mean reward 0.024, speed 69.12 f/s\n",
      "73491: done 4132 episodes, mean reward 0.025, speed 72.39 f/s\n",
      "73601: done 4136 episodes, mean reward 0.027, speed 70.15 f/s\n",
      "73676: done 4139 episodes, mean reward 0.027, speed 69.62 f/s\n",
      "73748: done 4142 episodes, mean reward 0.027, speed 68.09 f/s\n",
      "73822: done 4146 episodes, mean reward 0.027, speed 65.52 f/s\n",
      "73904: done 4148 episodes, mean reward 0.027, speed 69.79 f/s\n",
      "73993: done 4152 episodes, mean reward 0.027, speed 66.66 f/s\n",
      "74090: done 4155 episodes, mean reward 0.028, speed 65.62 f/s\n",
      "74162: done 4159 episodes, mean reward 0.028, speed 68.06 f/s\n",
      "74234: done 4163 episodes, mean reward 0.027, speed 68.30 f/s\n",
      "74308: done 4167 episodes, mean reward 0.026, speed 66.88 f/s\n",
      "74384: done 4170 episodes, mean reward 0.024, speed 70.65 f/s\n",
      "74454: done 4175 episodes, mean reward 0.023, speed 68.91 f/s\n",
      "74583: done 4178 episodes, mean reward 0.025, speed 70.81 f/s\n",
      "74659: done 4182 episodes, mean reward 0.025, speed 68.33 f/s\n",
      "74734: done 4185 episodes, mean reward 0.026, speed 65.89 f/s\n",
      "74827: done 4188 episodes, mean reward 0.027, speed 67.01 f/s\n",
      "74916: done 4192 episodes, mean reward 0.027, speed 69.30 f/s\n",
      "74993: done 4195 episodes, mean reward 0.027, speed 70.09 f/s\n",
      "75064: done 4197 episodes, mean reward 0.027, speed 70.03 f/s\n",
      "75192: done 4201 episodes, mean reward 0.028, speed 67.74 f/s\n",
      "75273: done 4203 episodes, mean reward 0.029, speed 67.82 f/s\n",
      "75348: done 4206 episodes, mean reward 0.027, speed 69.19 f/s\n",
      "75444: done 4209 episodes, mean reward 0.028, speed 65.79 f/s\n",
      "75534: done 4213 episodes, mean reward 0.027, speed 68.30 f/s\n",
      "75635: done 4216 episodes, mean reward 0.027, speed 70.80 f/s\n",
      "75727: done 4219 episodes, mean reward 0.026, speed 67.88 f/s\n",
      "75802: done 4222 episodes, mean reward 0.027, speed 70.04 f/s\n",
      "75876: done 4225 episodes, mean reward 0.027, speed 70.24 f/s\n",
      "75964: done 4229 episodes, mean reward 0.027, speed 69.69 f/s\n",
      "76043: done 4232 episodes, mean reward 0.026, speed 69.48 f/s\n",
      "76127: done 4234 episodes, mean reward 0.028, speed 67.76 f/s\n",
      "76198: done 4239 episodes, mean reward 0.025, speed 69.90 f/s\n",
      "76288: done 4243 episodes, mean reward 0.025, speed 70.09 f/s\n",
      "76383: done 4245 episodes, mean reward 0.027, speed 71.16 f/s\n",
      "76456: done 4249 episodes, mean reward 0.027, speed 68.70 f/s\n",
      "76525: done 4250 episodes, mean reward 0.028, speed 68.33 f/s\n",
      "76613: done 4252 episodes, mean reward 0.029, speed 71.02 f/s\n",
      "76695: done 4254 episodes, mean reward 0.030, speed 66.31 f/s\n",
      "76771: done 4258 episodes, mean reward 0.029, speed 72.31 f/s\n",
      "76847: done 4261 episodes, mean reward 0.030, speed 66.68 f/s\n",
      "76938: done 4265 episodes, mean reward 0.030, speed 71.74 f/s\n",
      "77012: done 4269 episodes, mean reward 0.030, speed 69.72 f/s\n",
      "77096: done 4273 episodes, mean reward 0.030, speed 66.66 f/s\n",
      "77169: done 4277 episodes, mean reward 0.030, speed 67.31 f/s\n",
      "77244: done 4280 episodes, mean reward 0.029, speed 68.10 f/s\n",
      "77326: done 4282 episodes, mean reward 0.030, speed 64.76 f/s\n",
      "77408: done 4284 episodes, mean reward 0.031, speed 70.50 f/s\n",
      "77496: done 4288 episodes, mean reward 0.030, speed 68.54 f/s\n",
      "77574: done 4291 episodes, mean reward 0.030, speed 69.07 f/s\n",
      "77646: done 4295 episodes, mean reward 0.029, speed 67.83 f/s\n",
      "77726: done 4298 episodes, mean reward 0.029, speed 69.14 f/s\n",
      "77817: done 4301 episodes, mean reward 0.028, speed 66.28 f/s\n",
      "77889: done 4305 episodes, mean reward 0.027, speed 71.12 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77972: done 4307 episodes, mean reward 0.028, speed 68.34 f/s\n",
      "78046: done 4310 episodes, mean reward 0.027, speed 66.45 f/s\n",
      "78160: done 4315 episodes, mean reward 0.027, speed 70.03 f/s\n",
      "78250: done 4319 episodes, mean reward 0.026, speed 71.42 f/s\n",
      "78324: done 4323 episodes, mean reward 0.025, speed 68.69 f/s\n",
      "78400: done 4326 episodes, mean reward 0.025, speed 70.67 f/s\n",
      "78467: done 4328 episodes, mean reward 0.026, speed 63.14 f/s\n",
      "78555: done 4332 episodes, mean reward 0.025, speed 68.86 f/s\n",
      "78631: done 4334 episodes, mean reward 0.025, speed 70.36 f/s\n",
      "78723: done 4338 episodes, mean reward 0.026, speed 70.13 f/s\n",
      "78798: done 4341 episodes, mean reward 0.027, speed 71.92 f/s\n",
      "78892: done 4345 episodes, mean reward 0.025, speed 68.41 f/s\n",
      "78984: done 4347 episodes, mean reward 0.026, speed 69.70 f/s\n",
      "79168: done 4351 episodes, mean reward 0.027, speed 67.84 f/s\n",
      "79257: done 4355 episodes, mean reward 0.026, speed 70.48 f/s\n",
      "79346: done 4359 episodes, mean reward 0.026, speed 68.12 f/s\n",
      "79416: done 4363 episodes, mean reward 0.026, speed 63.71 f/s\n",
      "79490: done 4366 episodes, mean reward 0.026, speed 69.42 f/s\n",
      "79572: done 4368 episodes, mean reward 0.027, speed 65.35 f/s\n",
      "79647: done 4372 episodes, mean reward 0.028, speed 67.06 f/s\n",
      "79723: done 4375 episodes, mean reward 0.028, speed 69.71 f/s\n",
      "79790: done 4378 episodes, mean reward 0.028, speed 66.96 f/s\n",
      "79859: done 4382 episodes, mean reward 0.027, speed 67.51 f/s\n",
      "79942: done 4384 episodes, mean reward 0.027, speed 70.30 f/s\n",
      "Test done in 1.66 sec, reward 0.021, steps 23\n",
      "80001: done 4387 episodes, mean reward 0.027, speed 23.86 f/s\n",
      "80092: done 4391 episodes, mean reward 0.027, speed 67.10 f/s\n",
      "80167: done 4394 episodes, mean reward 0.027, speed 66.41 f/s\n",
      "80263: done 4397 episodes, mean reward 0.029, speed 68.55 f/s\n",
      "80355: done 4401 episodes, mean reward 0.027, speed 70.38 f/s\n",
      "80446: done 4404 episodes, mean reward 0.028, speed 66.03 f/s\n",
      "80540: done 4408 episodes, mean reward 0.028, speed 67.18 f/s\n",
      "80616: done 4411 episodes, mean reward 0.028, speed 66.14 f/s\n",
      "80690: done 4414 episodes, mean reward 0.029, speed 64.22 f/s\n",
      "80767: done 4417 episodes, mean reward 0.028, speed 68.72 f/s\n",
      "80843: done 4420 episodes, mean reward 0.029, speed 68.68 f/s\n",
      "80932: done 4424 episodes, mean reward 0.029, speed 67.13 f/s\n",
      "81005: done 4428 episodes, mean reward 0.028, speed 68.64 f/s\n",
      "81080: done 4431 episodes, mean reward 0.028, speed 66.50 f/s\n",
      "81192: done 4435 episodes, mean reward 0.028, speed 67.95 f/s\n",
      "81275: done 4437 episodes, mean reward 0.029, speed 70.23 f/s\n",
      "81347: done 4441 episodes, mean reward 0.028, speed 67.64 f/s\n",
      "81423: done 4445 episodes, mean reward 0.028, speed 67.99 f/s\n",
      "81511: done 4449 episodes, mean reward 0.026, speed 66.95 f/s\n",
      "81594: done 4451 episodes, mean reward 0.025, speed 71.47 f/s\n",
      "81669: done 4454 episodes, mean reward 0.025, speed 72.14 f/s\n",
      "81782: done 4458 episodes, mean reward 0.026, speed 64.96 f/s\n",
      "81856: done 4461 episodes, mean reward 0.026, speed 66.50 f/s\n",
      "81935: done 4464 episodes, mean reward 0.026, speed 69.68 f/s\n",
      "82031: done 4467 episodes, mean reward 0.026, speed 68.32 f/s\n",
      "82104: done 4471 episodes, mean reward 0.026, speed 68.85 f/s\n",
      "82196: done 4475 episodes, mean reward 0.026, speed 68.88 f/s\n",
      "82277: done 4479 episodes, mean reward 0.026, speed 70.82 f/s\n",
      "82352: done 4482 episodes, mean reward 0.027, speed 69.43 f/s\n",
      "82438: done 4487 episodes, mean reward 0.025, speed 69.91 f/s\n",
      "82535: done 4489 episodes, mean reward 0.027, speed 70.54 f/s\n",
      "82630: done 4491 episodes, mean reward 0.027, speed 67.78 f/s\n",
      "82723: done 4494 episodes, mean reward 0.028, speed 68.29 f/s\n",
      "82798: done 4497 episodes, mean reward 0.027, speed 67.16 f/s\n",
      "82886: done 4502 episodes, mean reward 0.026, speed 68.11 f/s\n",
      "82969: done 4505 episodes, mean reward 0.026, speed 70.33 f/s\n",
      "83064: done 4508 episodes, mean reward 0.026, speed 70.56 f/s\n",
      "83146: done 4510 episodes, mean reward 0.027, speed 68.63 f/s\n",
      "83235: done 4514 episodes, mean reward 0.027, speed 69.16 f/s\n",
      "83305: done 4519 episodes, mean reward 0.025, speed 67.62 f/s\n",
      "83396: done 4523 episodes, mean reward 0.025, speed 65.71 f/s\n",
      "83471: done 4526 episodes, mean reward 0.025, speed 67.38 f/s\n",
      "83568: done 4528 episodes, mean reward 0.027, speed 65.52 f/s\n",
      "83661: done 4531 episodes, mean reward 0.027, speed 69.57 f/s\n",
      "83781: done 4533 episodes, mean reward 0.030, speed 68.50 f/s\n",
      "83865: done 4535 episodes, mean reward 0.030, speed 70.74 f/s\n",
      "83939: done 4538 episodes, mean reward 0.029, speed 68.50 f/s\n",
      "84029: done 4542 episodes, mean reward 0.030, speed 68.78 f/s\n",
      "84104: done 4545 episodes, mean reward 0.030, speed 66.82 f/s\n",
      "84185: done 4547 episodes, mean reward 0.031, speed 67.58 f/s\n",
      "84277: done 4550 episodes, mean reward 0.031, speed 67.81 f/s\n",
      "84359: done 4553 episodes, mean reward 0.031, speed 69.73 f/s\n",
      "84434: done 4556 episodes, mean reward 0.032, speed 68.64 f/s\n",
      "84548: done 4559 episodes, mean reward 0.032, speed 70.44 f/s\n",
      "84623: done 4562 episodes, mean reward 0.032, speed 67.74 f/s\n",
      "84724: done 4568 episodes, mean reward 0.030, speed 72.08 f/s\n",
      "84815: done 4571 episodes, mean reward 0.031, speed 68.81 f/s\n",
      "84885: done 4572 episodes, mean reward 0.033, speed 66.23 f/s\n",
      "84969: done 4574 episodes, mean reward 0.034, speed 69.12 f/s\n",
      "85060: done 4577 episodes, mean reward 0.035, speed 69.42 f/s\n",
      "85149: done 4578 episodes, mean reward 0.036, speed 68.99 f/s\n",
      "85263: done 4581 episodes, mean reward 0.037, speed 66.97 f/s\n",
      "85374: done 4584 episodes, mean reward 0.038, speed 71.16 f/s\n",
      "85471: done 4587 episodes, mean reward 0.040, speed 67.61 f/s\n",
      "85548: done 4590 episodes, mean reward 0.038, speed 68.51 f/s\n",
      "85630: done 4592 episodes, mean reward 0.038, speed 65.90 f/s\n",
      "85773: done 4594 episodes, mean reward 0.040, speed 69.33 f/s\n",
      "85855: done 4596 episodes, mean reward 0.041, speed 70.23 f/s\n",
      "85981: done 4600 episodes, mean reward 0.043, speed 68.10 f/s\n",
      "86093: done 4604 episodes, mean reward 0.043, speed 71.50 f/s\n",
      "86182: done 4608 episodes, mean reward 0.043, speed 69.33 f/s\n",
      "86260: done 4611 episodes, mean reward 0.042, speed 69.43 f/s\n",
      "86349: done 4615 episodes, mean reward 0.042, speed 70.79 f/s\n",
      "86433: done 4617 episodes, mean reward 0.044, speed 68.23 f/s\n",
      "86515: done 4619 episodes, mean reward 0.045, speed 70.63 f/s\n",
      "86607: done 4623 episodes, mean reward 0.046, speed 66.33 f/s\n",
      "86800: done 4626 episodes, mean reward 0.049, speed 69.18 f/s\n",
      "86899: done 4629 episodes, mean reward 0.047, speed 67.52 f/s\n",
      "86988: done 4632 episodes, mean reward 0.047, speed 67.08 f/s\n",
      "87066: done 4634 episodes, mean reward 0.046, speed 70.33 f/s\n",
      "87167: done 4636 episodes, mean reward 0.046, speed 69.52 f/s\n",
      "87240: done 4640 episodes, mean reward 0.046, speed 69.29 f/s\n",
      "87322: done 4643 episodes, mean reward 0.046, speed 70.42 f/s\n",
      "87406: done 4646 episodes, mean reward 0.046, speed 69.52 f/s\n",
      "87495: done 4650 episodes, mean reward 0.045, speed 70.13 f/s\n",
      "87567: done 4654 episodes, mean reward 0.044, speed 67.69 f/s\n",
      "87665: done 4656 episodes, mean reward 0.045, speed 68.27 f/s\n",
      "87740: done 4660 episodes, mean reward 0.043, speed 68.70 f/s\n",
      "87846: done 4661 episodes, mean reward 0.045, speed 68.81 f/s\n",
      "87951: done 4664 episodes, mean reward 0.047, speed 67.07 f/s\n",
      "88216: done 4667 episodes, mean reward 0.053, speed 68.60 f/s\n",
      "88350: done 4670 episodes, mean reward 0.054, speed 68.29 f/s\n",
      "88444: done 4672 episodes, mean reward 0.053, speed 72.00 f/s\n",
      "88521: done 4675 episodes, mean reward 0.052, speed 66.03 f/s\n",
      "88612: done 4679 episodes, mean reward 0.050, speed 72.40 f/s\n",
      "88754: done 4682 episodes, mean reward 0.050, speed 68.64 f/s\n",
      "88838: done 4684 episodes, mean reward 0.050, speed 71.50 f/s\n",
      "88923: done 4687 episodes, mean reward 0.050, speed 66.73 f/s\n",
      "88999: done 4690 episodes, mean reward 0.050, speed 70.13 f/s\n",
      "89073: done 4694 episodes, mean reward 0.046, speed 65.46 f/s\n",
      "89148: done 4697 episodes, mean reward 0.045, speed 70.62 f/s\n",
      "89223: done 4700 episodes, mean reward 0.045, speed 66.76 f/s\n",
      "89322: done 4702 episodes, mean reward 0.046, speed 69.06 f/s\n",
      "89426: done 4704 episodes, mean reward 0.047, speed 67.23 f/s\n",
      "89501: done 4707 episodes, mean reward 0.048, speed 67.57 f/s\n",
      "89668: done 4711 episodes, mean reward 0.049, speed 68.85 f/s\n",
      "89757: done 4712 episodes, mean reward 0.051, speed 68.17 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89857: done 4714 episodes, mean reward 0.053, speed 70.17 f/s\n",
      "89960: done 4717 episodes, mean reward 0.052, speed 68.79 f/s\n",
      "Test done in 3.46 sec, reward 0.084, steps 47\n",
      "Best reward updated: 0.032 -> 0.084\n",
      "90017: done 4718 episodes, mean reward 0.052, speed 13.16 f/s\n",
      "90109: done 4721 episodes, mean reward 0.053, speed 70.82 f/s\n",
      "90232: done 4723 episodes, mean reward 0.055, speed 69.77 f/s\n",
      "90307: done 4727 episodes, mean reward 0.051, speed 67.31 f/s\n",
      "90387: done 4730 episodes, mean reward 0.051, speed 68.58 f/s\n",
      "90468: done 4732 episodes, mean reward 0.052, speed 65.18 f/s\n",
      "90544: done 4735 episodes, mean reward 0.050, speed 70.60 f/s\n",
      "90644: done 4737 episodes, mean reward 0.051, speed 71.85 f/s\n",
      "90794: done 4740 episodes, mean reward 0.054, speed 69.36 f/s\n",
      "90869: done 4744 episodes, mean reward 0.053, speed 67.35 f/s\n",
      "91042: done 4747 episodes, mean reward 0.055, speed 69.74 f/s\n",
      "91145: done 4749 episodes, mean reward 0.057, speed 71.78 f/s\n",
      "91296: done 4752 episodes, mean reward 0.059, speed 69.06 f/s\n",
      "91414: done 4755 episodes, mean reward 0.060, speed 70.05 f/s\n",
      "91507: done 4758 episodes, mean reward 0.060, speed 70.33 f/s\n",
      "91598: done 4762 episodes, mean reward 0.058, speed 70.78 f/s\n",
      "91674: done 4765 episodes, mean reward 0.057, speed 67.47 f/s\n",
      "91804: done 4768 episodes, mean reward 0.054, speed 70.99 f/s\n",
      "91899: done 4770 episodes, mean reward 0.053, speed 66.37 f/s\n",
      "91973: done 4773 episodes, mean reward 0.052, speed 65.38 f/s\n",
      "92051: done 4777 episodes, mean reward 0.051, speed 67.46 f/s\n",
      "92151: done 4779 episodes, mean reward 0.052, speed 67.18 f/s\n",
      "92252: done 4780 episodes, mean reward 0.054, speed 69.95 f/s\n",
      "92343: done 4783 episodes, mean reward 0.052, speed 67.75 f/s\n",
      "92435: done 4786 episodes, mean reward 0.052, speed 71.87 f/s\n",
      "92636: done 4787 episodes, mean reward 0.057, speed 70.78 f/s\n",
      "92712: done 4790 episodes, mean reward 0.057, speed 68.28 f/s\n",
      "92810: done 4793 episodes, mean reward 0.058, speed 72.37 f/s\n",
      "92893: done 4795 episodes, mean reward 0.060, speed 70.94 f/s\n",
      "92962: done 4796 episodes, mean reward 0.061, speed 66.09 f/s\n",
      "93044: done 4798 episodes, mean reward 0.062, speed 72.32 f/s\n",
      "93135: done 4801 episodes, mean reward 0.062, speed 68.66 f/s\n",
      "93243: done 4802 episodes, mean reward 0.063, speed 68.84 f/s\n",
      "93344: done 4805 episodes, mean reward 0.062, speed 70.41 f/s\n",
      "93418: done 4808 episodes, mean reward 0.062, speed 68.28 f/s\n",
      "93503: done 4810 episodes, mean reward 0.063, speed 67.47 f/s\n",
      "93611: done 4812 episodes, mean reward 0.061, speed 67.49 f/s\n",
      "93692: done 4815 episodes, mean reward 0.059, speed 70.82 f/s\n",
      "93774: done 4817 episodes, mean reward 0.060, speed 71.26 f/s\n",
      "93854: done 4820 episodes, mean reward 0.058, speed 70.68 f/s\n",
      "93945: done 4823 episodes, mean reward 0.057, speed 68.18 f/s\n",
      "94021: done 4827 episodes, mean reward 0.057, speed 71.47 f/s\n",
      "94132: done 4830 episodes, mean reward 0.058, speed 69.98 f/s\n",
      "94215: done 4832 episodes, mean reward 0.058, speed 67.08 f/s\n",
      "94312: done 4835 episodes, mean reward 0.058, speed 67.81 f/s\n",
      "94606: done 4838 episodes, mean reward 0.062, speed 68.36 f/s\n",
      "94740: done 4840 episodes, mean reward 0.063, speed 69.83 f/s\n",
      "94830: done 4841 episodes, mean reward 0.064, speed 68.98 f/s\n",
      "94938: done 4842 episodes, mean reward 0.067, speed 68.48 f/s\n",
      "95060: done 4844 episodes, mean reward 0.069, speed 68.60 f/s\n",
      "95142: done 4846 episodes, mean reward 0.070, speed 67.18 f/s\n",
      "95278: done 4849 episodes, mean reward 0.068, speed 67.34 f/s\n",
      "95346: done 4851 episodes, mean reward 0.068, speed 67.66 f/s\n",
      "95491: done 4852 episodes, mean reward 0.069, speed 70.84 f/s\n",
      "95589: done 4854 episodes, mean reward 0.070, speed 69.30 f/s\n",
      "95705: done 4857 episodes, mean reward 0.070, speed 67.58 f/s\n",
      "95779: done 4859 episodes, mean reward 0.071, speed 72.28 f/s\n",
      "95877: done 4862 episodes, mean reward 0.072, speed 68.65 f/s\n",
      "95991: done 4865 episodes, mean reward 0.073, speed 69.25 f/s\n",
      "96104: done 4868 episodes, mean reward 0.073, speed 67.38 f/s\n",
      "96192: done 4872 episodes, mean reward 0.072, speed 68.13 f/s\n",
      "96282: done 4875 episodes, mean reward 0.073, speed 68.12 f/s\n",
      "96377: done 4878 episodes, mean reward 0.074, speed 70.15 f/s\n",
      "96455: done 4881 episodes, mean reward 0.071, speed 73.14 f/s\n",
      "96545: done 4884 episodes, mean reward 0.071, speed 68.45 f/s\n",
      "96635: done 4888 episodes, mean reward 0.066, speed 70.75 f/s\n",
      "96769: done 4891 episodes, mean reward 0.068, speed 66.58 f/s\n",
      "96890: done 4893 episodes, mean reward 0.069, speed 69.63 f/s\n",
      "96965: done 4896 episodes, mean reward 0.067, speed 69.94 f/s\n",
      "97048: done 4898 episodes, mean reward 0.067, speed 70.61 f/s\n",
      "97149: done 4900 episodes, mean reward 0.068, speed 67.09 f/s\n",
      "97233: done 4902 episodes, mean reward 0.066, speed 66.28 f/s\n",
      "97323: done 4903 episodes, mean reward 0.067, speed 68.55 f/s\n",
      "97392: done 4905 episodes, mean reward 0.067, speed 67.61 f/s\n",
      "97509: done 4907 episodes, mean reward 0.069, speed 71.80 f/s\n",
      "97626: done 4911 episodes, mean reward 0.069, speed 68.03 f/s\n",
      "97808: done 4912 episodes, mean reward 0.072, speed 68.06 f/s\n",
      "97884: done 4915 episodes, mean reward 0.072, speed 73.53 f/s\n",
      "97979: done 4918 episodes, mean reward 0.071, speed 69.30 f/s\n",
      "98062: done 4920 episodes, mean reward 0.072, speed 67.48 f/s\n",
      "98136: done 4924 episodes, mean reward 0.071, speed 70.76 f/s\n",
      "98238: done 4926 episodes, mean reward 0.073, speed 68.66 f/s\n",
      "98359: done 4928 episodes, mean reward 0.074, speed 69.01 f/s\n",
      "98480: done 4930 episodes, mean reward 0.075, speed 69.35 f/s\n",
      "98557: done 4932 episodes, mean reward 0.075, speed 66.79 f/s\n",
      "98626: done 4933 episodes, mean reward 0.077, speed 67.24 f/s\n",
      "98743: done 4934 episodes, mean reward 0.078, speed 67.71 f/s\n",
      "98834: done 4938 episodes, mean reward 0.072, speed 71.00 f/s\n",
      "98917: done 4940 episodes, mean reward 0.071, speed 66.67 f/s\n",
      "99000: done 4942 episodes, mean reward 0.068, speed 69.75 f/s\n",
      "99083: done 4944 episodes, mean reward 0.067, speed 68.72 f/s\n",
      "99151: done 4946 episodes, mean reward 0.066, speed 66.52 f/s\n",
      "99222: done 4948 episodes, mean reward 0.067, speed 69.24 f/s\n",
      "99305: done 4950 episodes, mean reward 0.066, speed 66.32 f/s\n",
      "99412: done 4953 episodes, mean reward 0.062, speed 68.52 f/s\n",
      "99480: done 4954 episodes, mean reward 0.063, speed 67.65 f/s\n",
      "99614: done 4957 episodes, mean reward 0.063, speed 66.24 f/s\n",
      "99736: done 4959 episodes, mean reward 0.064, speed 68.82 f/s\n",
      "99825: done 4960 episodes, mean reward 0.066, speed 69.33 f/s\n",
      "99893: done 4961 episodes, mean reward 0.067, speed 67.44 f/s\n",
      "99975: done 4963 episodes, mean reward 0.067, speed 66.18 f/s\n",
      "Test done in 2.62 sec, reward 0.055, steps 35\n",
      "100016: done 4965 episodes, mean reward 0.065, speed 12.78 f/s\n",
      "100094: done 4968 episodes, mean reward 0.064, speed 66.75 f/s\n",
      "100184: done 4971 episodes, mean reward 0.065, speed 69.26 f/s\n",
      "100278: done 4974 episodes, mean reward 0.065, speed 68.34 f/s\n",
      "100475: done 4976 episodes, mean reward 0.068, speed 68.53 f/s\n",
      "100576: done 4978 episodes, mean reward 0.069, speed 70.61 f/s\n",
      "100675: done 4979 episodes, mean reward 0.071, speed 67.86 f/s\n",
      "100753: done 4982 episodes, mean reward 0.071, speed 66.69 f/s\n",
      "100873: done 4984 episodes, mean reward 0.072, speed 70.10 f/s\n",
      "101001: done 4985 episodes, mean reward 0.075, speed 68.46 f/s\n",
      "101176: done 4987 episodes, mean reward 0.079, speed 69.45 f/s\n",
      "101259: done 4989 episodes, mean reward 0.080, speed 69.02 f/s\n",
      "101357: done 4991 episodes, mean reward 0.079, speed 71.75 f/s\n",
      "101438: done 4993 episodes, mean reward 0.078, speed 64.87 f/s\n",
      "101549: done 4997 episodes, mean reward 0.079, speed 71.77 f/s\n",
      "101638: done 5000 episodes, mean reward 0.077, speed 69.86 f/s\n",
      "101704: done 5002 episodes, mean reward 0.076, speed 64.74 f/s\n",
      "101898: done 5005 episodes, mean reward 0.077, speed 70.63 f/s\n",
      "101974: done 5008 episodes, mean reward 0.075, speed 72.75 f/s\n",
      "102055: done 5010 episodes, mean reward 0.076, speed 69.14 f/s\n",
      "102121: done 5012 episodes, mean reward 0.071, speed 64.99 f/s\n",
      "102219: done 5014 episodes, mean reward 0.072, speed 68.32 f/s\n",
      "102303: done 5016 episodes, mean reward 0.072, speed 70.80 f/s\n",
      "102378: done 5019 episodes, mean reward 0.072, speed 67.81 f/s\n",
      "102468: done 5020 episodes, mean reward 0.073, speed 71.19 f/s\n",
      "102538: done 5021 episodes, mean reward 0.075, speed 67.43 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102637: done 5023 episodes, mean reward 0.077, speed 69.98 f/s\n",
      "102725: done 5028 episodes, mean reward 0.073, speed 69.44 f/s\n",
      "102859: done 5031 episodes, mean reward 0.072, speed 68.82 f/s\n",
      "102942: done 5033 episodes, mean reward 0.071, speed 68.73 f/s\n",
      "103041: done 5035 episodes, mean reward 0.071, speed 73.07 f/s\n",
      "103131: done 5036 episodes, mean reward 0.073, speed 68.27 f/s\n",
      "103204: done 5038 episodes, mean reward 0.073, speed 69.18 f/s\n",
      "103300: done 5041 episodes, mean reward 0.072, speed 68.12 f/s\n",
      "103376: done 5044 episodes, mean reward 0.071, speed 67.47 f/s\n",
      "103506: done 5047 episodes, mean reward 0.072, speed 70.51 f/s\n",
      "103624: done 5050 episodes, mean reward 0.072, speed 68.79 f/s\n",
      "103714: done 5054 episodes, mean reward 0.070, speed 68.39 f/s\n",
      "103796: done 5056 episodes, mean reward 0.070, speed 70.21 f/s\n",
      "103955: done 5059 episodes, mean reward 0.070, speed 70.04 f/s\n",
      "104139: done 5060 episodes, mean reward 0.072, speed 69.83 f/s\n",
      "104407: done 5063 episodes, mean reward 0.075, speed 69.85 f/s\n",
      "104514: done 5064 episodes, mean reward 0.077, speed 68.99 f/s\n",
      "104697: done 5065 episodes, mean reward 0.082, speed 71.41 f/s\n",
      "104820: done 5067 episodes, mean reward 0.083, speed 69.48 f/s\n",
      "104977: done 5069 episodes, mean reward 0.086, speed 67.65 f/s\n",
      "105072: done 5072 episodes, mean reward 0.086, speed 67.73 f/s\n",
      "105162: done 5073 episodes, mean reward 0.087, speed 67.46 f/s\n",
      "105257: done 5075 episodes, mean reward 0.088, speed 68.48 f/s\n",
      "105375: done 5077 episodes, mean reward 0.085, speed 67.35 f/s\n",
      "105482: done 5078 episodes, mean reward 0.087, speed 68.88 f/s\n",
      "105609: done 5082 episodes, mean reward 0.085, speed 67.88 f/s\n",
      "105843: done 5084 episodes, mean reward 0.088, speed 68.81 f/s\n",
      "105932: done 5085 episodes, mean reward 0.087, speed 71.23 f/s\n",
      "106028: done 5087 episodes, mean reward 0.085, speed 68.30 f/s\n",
      "106126: done 5089 episodes, mean reward 0.085, speed 71.38 f/s\n",
      "106217: done 5092 episodes, mean reward 0.084, speed 69.42 f/s\n",
      "106325: done 5093 episodes, mean reward 0.086, speed 67.85 f/s\n",
      "106589: done 5096 episodes, mean reward 0.091, speed 67.56 f/s\n",
      "106717: done 5100 episodes, mean reward 0.091, speed 67.88 f/s\n",
      "106792: done 5103 episodes, mean reward 0.091, speed 68.28 f/s\n",
      "106892: done 5106 episodes, mean reward 0.089, speed 66.49 f/s\n",
      "106995: done 5108 episodes, mean reward 0.090, speed 65.69 f/s\n",
      "107148: done 5110 episodes, mean reward 0.092, speed 69.50 f/s\n",
      "107225: done 5113 episodes, mean reward 0.090, speed 67.27 f/s\n",
      "107332: done 5115 episodes, mean reward 0.092, speed 68.79 f/s\n",
      "107445: done 5118 episodes, mean reward 0.092, speed 68.40 f/s\n",
      "107536: done 5121 episodes, mean reward 0.090, speed 71.47 f/s\n",
      "107612: done 5124 episodes, mean reward 0.089, speed 72.17 f/s\n",
      "107709: done 5126 episodes, mean reward 0.091, speed 68.60 f/s\n",
      "107822: done 5129 episodes, mean reward 0.092, speed 67.34 f/s\n",
      "107891: done 5130 episodes, mean reward 0.093, speed 68.66 f/s\n",
      "108010: done 5132 episodes, mean reward 0.093, speed 67.97 f/s\n",
      "108092: done 5134 episodes, mean reward 0.092, speed 66.29 f/s\n",
      "108212: done 5136 episodes, mean reward 0.092, speed 71.58 f/s\n",
      "108349: done 5138 episodes, mean reward 0.094, speed 67.98 f/s\n",
      "108456: done 5139 episodes, mean reward 0.096, speed 69.96 f/s\n",
      "108597: done 5141 episodes, mean reward 0.097, speed 70.03 f/s\n",
      "108776: done 5142 episodes, mean reward 0.101, speed 69.62 f/s\n",
      "108911: done 5145 episodes, mean reward 0.102, speed 69.10 f/s\n",
      "109002: done 5148 episodes, mean reward 0.101, speed 68.18 f/s\n",
      "109084: done 5150 episodes, mean reward 0.101, speed 68.96 f/s\n",
      "109186: done 5152 episodes, mean reward 0.102, speed 67.29 f/s\n",
      "109284: done 5153 episodes, mean reward 0.104, speed 70.86 f/s\n",
      "109374: done 5156 episodes, mean reward 0.103, speed 70.44 f/s\n",
      "109518: done 5158 episodes, mean reward 0.106, speed 69.16 f/s\n",
      "109636: done 5161 episodes, mean reward 0.100, speed 67.57 f/s\n",
      "109734: done 5163 episodes, mean reward 0.097, speed 64.13 f/s\n",
      "109810: done 5166 episodes, mean reward 0.091, speed 70.26 f/s\n",
      "109880: done 5167 episodes, mean reward 0.091, speed 68.21 f/s\n",
      "Test done in 3.43 sec, reward 0.086, steps 47\n",
      "Best reward updated: 0.084 -> 0.086\n",
      "110033: done 5168 episodes, mean reward 0.094, speed 26.92 f/s\n",
      "110150: done 5169 episodes, mean reward 0.093, speed 69.38 f/s\n",
      "110238: done 5170 episodes, mean reward 0.095, speed 68.63 f/s\n",
      "110397: done 5172 episodes, mean reward 0.097, speed 67.74 f/s\n",
      "110570: done 5175 episodes, mean reward 0.096, speed 68.43 f/s\n",
      "110651: done 5177 episodes, mean reward 0.096, speed 71.48 f/s\n",
      "110733: done 5179 episodes, mean reward 0.094, speed 70.77 f/s\n",
      "110813: done 5182 episodes, mean reward 0.094, speed 69.88 f/s\n",
      "110893: done 5185 episodes, mean reward 0.088, speed 68.46 f/s\n",
      "110989: done 5187 episodes, mean reward 0.088, speed 69.11 f/s\n",
      "111105: done 5190 episodes, mean reward 0.087, speed 70.47 f/s\n",
      "111209: done 5192 episodes, mean reward 0.088, speed 68.14 f/s\n",
      "111290: done 5194 episodes, mean reward 0.087, speed 64.02 f/s\n",
      "111381: done 5195 episodes, mean reward 0.088, speed 69.59 f/s\n",
      "111485: done 5197 episodes, mean reward 0.085, speed 70.62 f/s\n",
      "111553: done 5198 episodes, mean reward 0.087, speed 63.32 f/s\n",
      "111634: done 5200 episodes, mean reward 0.086, speed 71.93 f/s\n",
      "111745: done 5203 episodes, mean reward 0.087, speed 69.73 f/s\n",
      "111948: done 5204 episodes, mean reward 0.092, speed 70.12 f/s\n",
      "112030: done 5206 episodes, mean reward 0.092, speed 69.46 f/s\n",
      "112157: done 5207 episodes, mean reward 0.094, speed 69.41 f/s\n",
      "112271: done 5210 episodes, mean reward 0.092, speed 66.44 f/s\n",
      "112475: done 5211 episodes, mean reward 0.096, speed 67.59 f/s\n",
      "112579: done 5213 episodes, mean reward 0.098, speed 69.54 f/s\n",
      "112667: done 5217 episodes, mean reward 0.096, speed 68.85 f/s\n",
      "112768: done 5219 episodes, mean reward 0.096, speed 70.44 f/s\n",
      "112862: done 5222 episodes, mean reward 0.095, speed 68.23 f/s\n",
      "112932: done 5223 episodes, mean reward 0.096, speed 68.58 f/s\n",
      "113050: done 5226 episodes, mean reward 0.096, speed 69.38 f/s\n",
      "113235: done 5227 episodes, mean reward 0.100, speed 69.59 f/s\n",
      "113317: done 5229 episodes, mean reward 0.100, speed 69.64 f/s\n",
      "113407: done 5230 episodes, mean reward 0.101, speed 70.60 f/s\n",
      "113490: done 5232 episodes, mean reward 0.100, speed 68.78 f/s\n",
      "113608: done 5234 episodes, mean reward 0.101, speed 67.41 f/s\n",
      "113723: done 5237 episodes, mean reward 0.100, speed 69.27 f/s\n",
      "113893: done 5240 episodes, mean reward 0.097, speed 67.76 f/s\n",
      "113993: done 5242 episodes, mean reward 0.094, speed 68.82 f/s\n",
      "114186: done 5244 episodes, mean reward 0.097, speed 69.09 f/s\n",
      "114389: done 5247 episodes, mean reward 0.099, speed 67.95 f/s\n",
      "114611: done 5248 episodes, mean reward 0.104, speed 67.92 f/s\n",
      "114707: done 5250 episodes, mean reward 0.104, speed 67.66 f/s\n",
      "114802: done 5253 episodes, mean reward 0.102, speed 69.40 f/s\n",
      "114899: done 5256 episodes, mean reward 0.102, speed 65.29 f/s\n",
      "115091: done 5259 episodes, mean reward 0.103, speed 70.51 f/s\n",
      "115280: done 5262 episodes, mean reward 0.104, speed 68.20 f/s\n",
      "115389: done 5263 episodes, mean reward 0.105, speed 68.49 f/s\n",
      "115493: done 5265 episodes, mean reward 0.107, speed 70.06 f/s\n",
      "115576: done 5267 episodes, mean reward 0.106, speed 68.08 f/s\n",
      "115644: done 5268 episodes, mean reward 0.104, speed 64.12 f/s\n",
      "115748: done 5270 episodes, mean reward 0.101, speed 69.51 f/s\n",
      "115837: done 5273 episodes, mean reward 0.099, speed 69.82 f/s\n",
      "115962: done 5275 episodes, mean reward 0.099, speed 69.91 f/s\n",
      "116075: done 5278 episodes, mean reward 0.098, speed 71.22 f/s\n",
      "116222: done 5279 episodes, mean reward 0.101, speed 70.98 f/s\n",
      "116320: done 5281 episodes, mean reward 0.102, speed 69.01 f/s\n",
      "116440: done 5283 episodes, mean reward 0.104, speed 69.26 f/s\n",
      "116639: done 5285 episodes, mean reward 0.108, speed 67.89 f/s\n",
      "116885: done 5288 episodes, mean reward 0.111, speed 70.13 f/s\n",
      "117213: done 5289 episodes, mean reward 0.119, speed 71.05 f/s\n",
      "117313: done 5291 episodes, mean reward 0.119, speed 71.76 f/s\n",
      "117420: done 5292 episodes, mean reward 0.120, speed 68.60 f/s\n",
      "117604: done 5293 episodes, mean reward 0.123, speed 65.94 f/s\n",
      "117685: done 5295 episodes, mean reward 0.122, speed 70.45 f/s\n",
      "117794: done 5296 episodes, mean reward 0.123, speed 68.81 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117947: done 5298 episodes, mean reward 0.124, speed 67.25 f/s\n",
      "118023: done 5301 episodes, mean reward 0.123, speed 63.69 f/s\n",
      "118207: done 5302 episodes, mean reward 0.127, speed 68.39 f/s\n",
      "118310: done 5304 episodes, mean reward 0.123, speed 69.97 f/s\n",
      "118442: done 5307 episodes, mean reward 0.121, speed 70.19 f/s\n",
      "118598: done 5308 episodes, mean reward 0.124, speed 69.40 f/s\n",
      "118687: done 5310 episodes, mean reward 0.124, speed 68.91 f/s\n",
      "118769: done 5312 episodes, mean reward 0.121, speed 69.99 f/s\n",
      "118991: done 5313 episodes, mean reward 0.124, speed 67.72 f/s\n",
      "119074: done 5315 episodes, mean reward 0.125, speed 69.87 f/s\n",
      "119184: done 5318 episodes, mean reward 0.125, speed 71.59 f/s\n",
      "119344: done 5320 episodes, mean reward 0.127, speed 70.33 f/s\n",
      "119471: done 5321 episodes, mean reward 0.130, speed 68.12 f/s\n",
      "119637: done 5322 episodes, mean reward 0.134, speed 71.15 f/s\n",
      "119910: done 5324 episodes, mean reward 0.137, speed 68.24 f/s\n",
      "Test done in 7.63 sec, reward 0.233, steps 102\n",
      "Best reward updated: 0.086 -> 0.233\n",
      "120055: done 5326 episodes, mean reward 0.139, speed 14.80 f/s\n",
      "120143: done 5327 episodes, mean reward 0.136, speed 67.08 f/s\n",
      "120225: done 5329 episodes, mean reward 0.136, speed 69.84 f/s\n",
      "120377: done 5332 episodes, mean reward 0.136, speed 68.11 f/s\n",
      "120489: done 5334 episodes, mean reward 0.136, speed 67.36 f/s\n",
      "120721: done 5337 episodes, mean reward 0.139, speed 70.15 f/s\n",
      "120820: done 5339 episodes, mean reward 0.140, speed 69.57 f/s\n",
      "120904: done 5341 episodes, mean reward 0.139, speed 69.95 f/s\n",
      "120990: done 5342 episodes, mean reward 0.139, speed 70.65 f/s\n",
      "121103: done 5345 episodes, mean reward 0.136, speed 67.05 f/s\n",
      "121648: done 5346 episodes, mean reward 0.149, speed 69.57 f/s\n",
      "121866: done 5347 episodes, mean reward 0.152, speed 66.49 f/s\n",
      "122028: done 5348 episodes, mean reward 0.150, speed 68.21 f/s\n",
      "122132: done 5350 episodes, mean reward 0.151, speed 70.17 f/s\n",
      "122254: done 5351 episodes, mean reward 0.154, speed 69.58 f/s\n",
      "122475: done 5352 episodes, mean reward 0.158, speed 68.93 f/s\n",
      "122655: done 5353 episodes, mean reward 0.162, speed 69.27 f/s\n",
      "123029: done 5354 episodes, mean reward 0.171, speed 70.08 f/s\n",
      "123132: done 5355 episodes, mean reward 0.173, speed 68.31 f/s\n",
      "123236: done 5357 episodes, mean reward 0.174, speed 70.51 f/s\n",
      "123340: done 5360 episodes, mean reward 0.172, speed 67.67 f/s\n",
      "123410: done 5361 episodes, mean reward 0.173, speed 68.76 f/s\n",
      "123918: done 5362 episodes, mean reward 0.183, speed 69.05 f/s\n",
      "124023: done 5364 episodes, mean reward 0.182, speed 68.40 f/s\n",
      "124183: done 5365 episodes, mean reward 0.185, speed 68.75 f/s\n",
      "124272: done 5366 episodes, mean reward 0.186, speed 68.91 f/s\n",
      "124502: done 5368 episodes, mean reward 0.189, speed 70.23 f/s\n",
      "124623: done 5370 episodes, mean reward 0.190, speed 70.86 f/s\n",
      "124692: done 5371 episodes, mean reward 0.191, speed 66.85 f/s\n",
      "124813: done 5373 episodes, mean reward 0.192, speed 69.38 f/s\n",
      "124988: done 5375 episodes, mean reward 0.194, speed 69.84 f/s\n",
      "125072: done 5377 episodes, mean reward 0.194, speed 69.08 f/s\n",
      "125152: done 5379 episodes, mean reward 0.191, speed 68.49 f/s\n",
      "125321: done 5382 episodes, mean reward 0.192, speed 69.44 f/s\n",
      "125391: done 5383 episodes, mean reward 0.192, speed 67.36 f/s\n",
      "125481: done 5386 episodes, mean reward 0.189, speed 70.33 f/s\n",
      "125571: done 5389 episodes, mean reward 0.177, speed 70.36 f/s\n",
      "125639: done 5390 episodes, mean reward 0.178, speed 67.63 f/s\n",
      "125823: done 5391 episodes, mean reward 0.181, speed 67.80 f/s\n",
      "125911: done 5393 episodes, mean reward 0.176, speed 68.59 f/s\n",
      "126022: done 5395 episodes, mean reward 0.177, speed 68.07 f/s\n",
      "126201: done 5397 episodes, mean reward 0.178, speed 69.80 f/s\n",
      "126284: done 5399 episodes, mean reward 0.176, speed 67.25 f/s\n",
      "126394: done 5400 episodes, mean reward 0.179, speed 70.45 f/s\n",
      "126465: done 5401 episodes, mean reward 0.180, speed 70.88 f/s\n",
      "126548: done 5403 episodes, mean reward 0.176, speed 68.91 f/s\n",
      "126652: done 5405 episodes, mean reward 0.176, speed 68.74 f/s\n",
      "126782: done 5408 episodes, mean reward 0.173, speed 69.76 f/s\n",
      "126874: done 5411 episodes, mean reward 0.173, speed 71.89 f/s\n",
      "127027: done 5413 episodes, mean reward 0.171, speed 67.28 f/s\n",
      "127100: done 5414 episodes, mean reward 0.171, speed 67.92 f/s\n",
      "127340: done 5415 episodes, mean reward 0.176, speed 67.90 f/s\n",
      "127413: done 5417 episodes, mean reward 0.177, speed 68.30 f/s\n",
      "127535: done 5420 episodes, mean reward 0.175, speed 71.00 f/s\n",
      "127734: done 5422 episodes, mean reward 0.172, speed 67.02 f/s\n",
      "127850: done 5425 episodes, mean reward 0.167, speed 67.65 f/s\n",
      "127970: done 5427 episodes, mean reward 0.166, speed 70.46 f/s\n",
      "128049: done 5430 episodes, mean reward 0.165, speed 67.85 f/s\n",
      "128171: done 5432 episodes, mean reward 0.164, speed 71.54 f/s\n",
      "128261: done 5434 episodes, mean reward 0.163, speed 66.21 f/s\n",
      "128346: done 5437 episodes, mean reward 0.159, speed 67.55 f/s\n",
      "128429: done 5439 episodes, mean reward 0.159, speed 67.79 f/s\n",
      "128610: done 5440 episodes, mean reward 0.162, speed 68.07 f/s\n",
      "128725: done 5442 episodes, mean reward 0.162, speed 68.72 f/s\n",
      "128816: done 5445 episodes, mean reward 0.162, speed 65.18 f/s\n",
      "128900: done 5447 episodes, mean reward 0.144, speed 69.24 f/s\n",
      "129065: done 5448 episodes, mean reward 0.144, speed 66.38 f/s\n",
      "129213: done 5449 episodes, mean reward 0.146, speed 70.70 f/s\n",
      "129339: done 5450 episodes, mean reward 0.148, speed 70.67 f/s\n",
      "129438: done 5452 episodes, mean reward 0.142, speed 67.88 f/s\n",
      "129549: done 5455 episodes, mean reward 0.128, speed 71.00 f/s\n",
      "129652: done 5457 episodes, mean reward 0.128, speed 67.18 f/s\n",
      "129745: done 5460 episodes, mean reward 0.127, speed 70.57 f/s\n",
      "129868: done 5461 episodes, mean reward 0.129, speed 67.85 f/s\n",
      "Test done in 9.41 sec, reward 0.284, steps 124\n",
      "Best reward updated: 0.233 -> 0.284\n",
      "130180: done 5464 episodes, mean reward 0.120, speed 22.47 f/s\n",
      "130298: done 5466 episodes, mean reward 0.116, speed 68.94 f/s\n",
      "130402: done 5468 episodes, mean reward 0.113, speed 67.94 f/s\n",
      "130550: done 5469 episodes, mean reward 0.115, speed 68.78 f/s\n",
      "130622: done 5470 episodes, mean reward 0.115, speed 67.58 f/s\n",
      "130709: done 5471 episodes, mean reward 0.116, speed 67.80 f/s\n",
      "130803: done 5474 episodes, mean reward 0.114, speed 67.09 f/s\n",
      "131106: done 5476 episodes, mean reward 0.116, speed 69.00 f/s\n",
      "131261: done 5478 episodes, mean reward 0.118, speed 69.20 f/s\n",
      "131343: done 5480 episodes, mean reward 0.118, speed 66.29 f/s\n",
      "131431: done 5482 episodes, mean reward 0.117, speed 67.40 f/s\n",
      "131630: done 5484 episodes, mean reward 0.119, speed 70.06 f/s\n",
      "131712: done 5486 episodes, mean reward 0.120, speed 68.29 f/s\n",
      "131799: done 5487 episodes, mean reward 0.121, speed 67.74 f/s\n",
      "131927: done 5488 episodes, mean reward 0.124, speed 66.98 f/s\n",
      "132163: done 5490 episodes, mean reward 0.127, speed 69.21 f/s\n",
      "132264: done 5491 episodes, mean reward 0.125, speed 72.87 f/s\n",
      "132373: done 5492 episodes, mean reward 0.126, speed 70.83 f/s\n",
      "132480: done 5494 episodes, mean reward 0.126, speed 73.24 f/s\n",
      "132844: done 5497 episodes, mean reward 0.129, speed 68.23 f/s\n",
      "132927: done 5499 episodes, mean reward 0.130, speed 70.80 f/s\n",
      "133010: done 5501 episodes, mean reward 0.127, speed 70.27 f/s\n",
      "133208: done 5502 episodes, mean reward 0.132, speed 68.75 f/s\n",
      "133305: done 5504 episodes, mean reward 0.133, speed 68.47 f/s\n",
      "133406: done 5506 episodes, mean reward 0.132, speed 69.51 f/s\n",
      "133525: done 5508 episodes, mean reward 0.132, speed 67.13 f/s\n",
      "133654: done 5511 episodes, mean reward 0.133, speed 70.57 f/s\n",
      "133752: done 5513 episodes, mean reward 0.131, speed 72.97 f/s\n",
      "133912: done 5515 episodes, mean reward 0.128, speed 70.85 f/s\n",
      "134101: done 5516 episodes, mean reward 0.131, speed 69.39 f/s\n",
      "134180: done 5518 episodes, mean reward 0.132, speed 67.63 f/s\n",
      "134434: done 5520 episodes, mean reward 0.136, speed 68.95 f/s\n",
      "134552: done 5521 episodes, mean reward 0.137, speed 69.35 f/s\n",
      "134638: done 5522 episodes, mean reward 0.136, speed 70.16 f/s\n",
      "134724: done 5523 episodes, mean reward 0.137, speed 70.34 f/s\n",
      "134823: done 5525 episodes, mean reward 0.138, speed 70.83 f/s\n",
      "134978: done 5527 episodes, mean reward 0.139, speed 69.49 f/s\n",
      "135127: done 5528 episodes, mean reward 0.142, speed 67.99 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135257: done 5531 episodes, mean reward 0.143, speed 69.64 f/s\n",
      "135359: done 5533 episodes, mean reward 0.144, speed 67.27 f/s\n",
      "135573: done 5535 episodes, mean reward 0.147, speed 70.01 f/s\n",
      "135645: done 5536 episodes, mean reward 0.148, speed 67.21 f/s\n",
      "135747: done 5538 episodes, mean reward 0.149, speed 68.19 f/s\n",
      "135854: done 5539 episodes, mean reward 0.151, speed 69.25 f/s\n",
      "135938: done 5541 episodes, mean reward 0.147, speed 69.95 f/s\n",
      "136156: done 5543 episodes, mean reward 0.150, speed 67.44 f/s\n",
      "136316: done 5545 episodes, mean reward 0.153, speed 68.63 f/s\n",
      "136505: done 5548 episodes, mean reward 0.151, speed 68.81 f/s\n",
      "136625: done 5549 episodes, mean reward 0.150, speed 69.13 f/s\n",
      "136751: done 5551 episodes, mean reward 0.149, speed 66.99 f/s\n",
      "136855: done 5553 episodes, mean reward 0.149, speed 68.11 f/s\n",
      "136961: done 5554 episodes, mean reward 0.151, speed 70.74 f/s\n",
      "137062: done 5556 episodes, mean reward 0.151, speed 69.55 f/s\n",
      "137148: done 5557 episodes, mean reward 0.152, speed 67.78 f/s\n",
      "137311: done 5558 episodes, mean reward 0.155, speed 70.88 f/s\n",
      "137394: done 5560 episodes, mean reward 0.156, speed 66.14 f/s\n",
      "137633: done 5561 episodes, mean reward 0.158, speed 70.46 f/s\n",
      "137749: done 5563 episodes, mean reward 0.160, speed 70.79 f/s\n",
      "137836: done 5564 episodes, mean reward 0.157, speed 68.31 f/s\n",
      "137907: done 5565 episodes, mean reward 0.157, speed 70.18 f/s\n",
      "138039: done 5568 episodes, mean reward 0.156, speed 67.71 f/s\n",
      "138121: done 5570 episodes, mean reward 0.153, speed 67.42 f/s\n",
      "138433: done 5571 episodes, mean reward 0.158, speed 68.10 f/s\n",
      "138517: done 5573 episodes, mean reward 0.159, speed 66.74 f/s\n",
      "138599: done 5575 episodes, mean reward 0.159, speed 68.31 f/s\n",
      "138728: done 5576 episodes, mean reward 0.156, speed 70.10 f/s\n",
      "138932: done 5579 episodes, mean reward 0.156, speed 66.72 f/s\n",
      "139029: done 5581 episodes, mean reward 0.157, speed 69.67 f/s\n",
      "139176: done 5582 episodes, mean reward 0.159, speed 68.74 f/s\n",
      "139258: done 5584 episodes, mean reward 0.156, speed 69.71 f/s\n",
      "139328: done 5585 episodes, mean reward 0.157, speed 65.02 f/s\n",
      "139468: done 5587 episodes, mean reward 0.157, speed 68.71 f/s\n",
      "139565: done 5589 episodes, mean reward 0.155, speed 70.99 f/s\n",
      "139654: done 5592 episodes, mean reward 0.148, speed 68.88 f/s\n",
      "139781: done 5593 episodes, mean reward 0.150, speed 69.02 f/s\n",
      "139962: done 5594 episodes, mean reward 0.154, speed 70.19 f/s\n",
      "Test done in 7.65 sec, reward 0.228, steps 101\n",
      "140111: done 5596 episodes, mean reward 0.156, speed 15.19 f/s\n",
      "140299: done 5599 episodes, mean reward 0.151, speed 70.27 f/s\n",
      "140385: done 5600 episodes, mean reward 0.152, speed 73.68 f/s\n",
      "140503: done 5601 episodes, mean reward 0.153, speed 68.66 f/s\n",
      "140675: done 5603 episodes, mean reward 0.152, speed 69.61 f/s\n",
      "140758: done 5605 episodes, mean reward 0.151, speed 69.66 f/s\n",
      "140899: done 5607 episodes, mean reward 0.152, speed 69.29 f/s\n",
      "141024: done 5608 episodes, mean reward 0.154, speed 67.30 f/s\n",
      "141168: done 5609 episodes, mean reward 0.157, speed 71.97 f/s\n",
      "141332: done 5610 episodes, mean reward 0.160, speed 68.96 f/s\n",
      "141443: done 5613 episodes, mean reward 0.159, speed 69.16 f/s\n",
      "141568: done 5615 episodes, mean reward 0.158, speed 71.86 f/s\n",
      "141778: done 5618 episodes, mean reward 0.157, speed 67.97 f/s\n",
      "142032: done 5620 episodes, mean reward 0.157, speed 70.32 f/s\n",
      "142174: done 5621 episodes, mean reward 0.158, speed 68.35 f/s\n",
      "142258: done 5623 episodes, mean reward 0.155, speed 66.98 f/s\n",
      "142332: done 5625 episodes, mean reward 0.155, speed 68.85 f/s\n",
      "142414: done 5627 episodes, mean reward 0.153, speed 67.18 f/s\n",
      "142517: done 5629 episodes, mean reward 0.151, speed 70.37 f/s\n",
      "142605: done 5630 episodes, mean reward 0.152, speed 72.03 f/s\n",
      "142712: done 5631 episodes, mean reward 0.153, speed 68.35 f/s\n",
      "143023: done 5633 episodes, mean reward 0.159, speed 68.57 f/s\n",
      "143111: done 5634 episodes, mean reward 0.160, speed 67.61 f/s\n",
      "143185: done 5635 episodes, mean reward 0.158, speed 70.04 f/s\n",
      "143665: done 5637 episodes, mean reward 0.167, speed 69.49 f/s\n",
      "143739: done 5638 episodes, mean reward 0.167, speed 64.11 f/s\n",
      "143814: done 5641 episodes, mean reward 0.164, speed 69.62 f/s\n",
      "143917: done 5642 episodes, mean reward 0.167, speed 66.69 f/s\n",
      "144057: done 5644 episodes, mean reward 0.165, speed 68.61 f/s\n",
      "144172: done 5647 episodes, mean reward 0.162, speed 71.45 f/s\n",
      "144421: done 5648 episodes, mean reward 0.165, speed 68.25 f/s\n",
      "144496: done 5651 episodes, mean reward 0.161, speed 64.43 f/s\n",
      "144677: done 5652 episodes, mean reward 0.164, speed 70.57 f/s\n",
      "145071: done 5653 episodes, mean reward 0.172, speed 69.48 f/s\n",
      "145332: done 5654 episodes, mean reward 0.176, speed 67.36 f/s\n",
      "145402: done 5656 episodes, mean reward 0.176, speed 69.39 f/s\n",
      "145472: done 5657 episodes, mean reward 0.175, speed 69.22 f/s\n",
      "145817: done 5659 episodes, mean reward 0.178, speed 71.71 f/s\n",
      "145921: done 5661 episodes, mean reward 0.174, speed 65.34 f/s\n",
      "145992: done 5662 episodes, mean reward 0.175, speed 67.91 f/s\n",
      "146101: done 5663 episodes, mean reward 0.176, speed 69.73 f/s\n",
      "146267: done 5664 episodes, mean reward 0.178, speed 69.45 f/s\n",
      "146343: done 5667 episodes, mean reward 0.176, speed 72.85 f/s\n",
      "146415: done 5668 episodes, mean reward 0.177, speed 67.03 f/s\n",
      "146542: done 5671 episodes, mean reward 0.169, speed 71.46 f/s\n",
      "146649: done 5672 episodes, mean reward 0.171, speed 69.30 f/s\n",
      "146727: done 5673 episodes, mean reward 0.171, speed 69.49 f/s\n",
      "146930: done 5674 episodes, mean reward 0.175, speed 67.45 f/s\n",
      "147012: done 5676 episodes, mean reward 0.174, speed 68.09 f/s\n",
      "147144: done 5678 episodes, mean reward 0.175, speed 69.91 f/s\n",
      "147440: done 5679 episodes, mean reward 0.180, speed 70.61 f/s\n",
      "147756: done 5681 episodes, mean reward 0.185, speed 68.18 f/s\n",
      "147840: done 5683 episodes, mean reward 0.182, speed 73.96 f/s\n",
      "148029: done 5686 episodes, mean reward 0.183, speed 67.79 f/s\n",
      "148102: done 5687 episodes, mean reward 0.183, speed 69.55 f/s\n",
      "148189: done 5689 episodes, mean reward 0.183, speed 67.99 f/s\n",
      "148386: done 5691 episodes, mean reward 0.187, speed 70.55 f/s\n",
      "148546: done 5694 episodes, mean reward 0.181, speed 71.97 f/s\n",
      "148639: done 5696 episodes, mean reward 0.180, speed 69.47 f/s\n",
      "148722: done 5698 episodes, mean reward 0.181, speed 70.94 f/s\n",
      "148813: done 5700 episodes, mean reward 0.177, speed 70.08 f/s\n",
      "149010: done 5702 episodes, mean reward 0.179, speed 62.49 f/s\n",
      "149218: done 5703 episodes, mean reward 0.180, speed 58.35 f/s\n",
      "149344: done 5704 episodes, mean reward 0.182, speed 69.44 f/s\n",
      "149469: done 5705 episodes, mean reward 0.184, speed 70.32 f/s\n",
      "149551: done 5707 episodes, mean reward 0.183, speed 70.04 f/s\n",
      "149635: done 5709 episodes, mean reward 0.178, speed 70.51 f/s\n",
      "149745: done 5710 episodes, mean reward 0.177, speed 72.09 f/s\n",
      "149854: done 5711 episodes, mean reward 0.179, speed 70.14 f/s\n",
      "149977: done 5713 episodes, mean reward 0.180, speed 68.49 f/s\n",
      "Test done in 10.88 sec, reward 0.382, steps 162\n",
      "Best reward updated: 0.284 -> 0.382\n",
      "150054: done 5715 episodes, mean reward 0.178, speed 6.39 f/s\n",
      "150196: done 5717 episodes, mean reward 0.181, speed 72.96 f/s\n",
      "150384: done 5718 episodes, mean reward 0.181, speed 71.57 f/s\n",
      "150636: done 5720 episodes, mean reward 0.180, speed 74.12 f/s\n",
      "150800: done 5721 episodes, mean reward 0.181, speed 72.30 f/s\n",
      "151037: done 5723 episodes, mean reward 0.185, speed 73.27 f/s\n",
      "151139: done 5725 episodes, mean reward 0.185, speed 73.69 f/s\n",
      "151285: done 5726 episodes, mean reward 0.187, speed 75.68 f/s\n",
      "151382: done 5729 episodes, mean reward 0.186, speed 72.14 f/s\n",
      "151483: done 5731 episodes, mean reward 0.184, speed 72.70 f/s\n",
      "151835: done 5733 episodes, mean reward 0.185, speed 73.39 f/s\n",
      "151946: done 5734 episodes, mean reward 0.185, speed 71.68 f/s\n",
      "152102: done 5736 episodes, mean reward 0.186, speed 72.15 f/s\n",
      "152382: done 5740 episodes, mean reward 0.179, speed 73.61 f/s\n",
      "152599: done 5742 episodes, mean reward 0.181, speed 72.95 f/s\n",
      "152878: done 5744 episodes, mean reward 0.184, speed 72.25 f/s\n",
      "153101: done 5746 episodes, mean reward 0.188, speed 69.61 f/s\n",
      "153321: done 5747 episodes, mean reward 0.192, speed 72.95 f/s\n",
      "153574: done 5749 episodes, mean reward 0.192, speed 72.89 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153651: done 5750 episodes, mean reward 0.194, speed 71.79 f/s\n",
      "153740: done 5751 episodes, mean reward 0.195, speed 73.32 f/s\n",
      "153853: done 5754 episodes, mean reward 0.178, speed 71.37 f/s\n",
      "153963: done 5755 episodes, mean reward 0.180, speed 73.12 f/s\n",
      "154056: done 5758 episodes, mean reward 0.178, speed 75.09 f/s\n",
      "154160: done 5759 episodes, mean reward 0.173, speed 72.65 f/s\n",
      "154359: done 5761 episodes, mean reward 0.176, speed 72.30 f/s\n",
      "154513: done 5762 episodes, mean reward 0.178, speed 71.51 f/s\n",
      "154649: done 5765 episodes, mean reward 0.173, speed 71.55 f/s\n",
      "154728: done 5767 episodes, mean reward 0.173, speed 72.83 f/s\n",
      "154963: done 5768 episodes, mean reward 0.177, speed 71.21 f/s\n",
      "155052: done 5769 episodes, mean reward 0.178, speed 74.09 f/s\n",
      "155302: done 5771 episodes, mean reward 0.183, speed 70.83 f/s\n",
      "155519: done 5773 episodes, mean reward 0.184, speed 72.16 f/s\n",
      "155625: done 5774 episodes, mean reward 0.181, speed 74.57 f/s\n",
      "155724: done 5776 episodes, mean reward 0.182, speed 73.17 f/s\n",
      "155810: done 5778 episodes, mean reward 0.181, speed 71.58 f/s\n",
      "156003: done 5780 episodes, mean reward 0.177, speed 73.48 f/s\n",
      "156131: done 5781 episodes, mean reward 0.172, speed 71.66 f/s\n",
      "156345: done 5783 episodes, mean reward 0.176, speed 74.95 f/s\n",
      "156431: done 5785 episodes, mean reward 0.176, speed 73.15 f/s\n",
      "156665: done 5787 episodes, mean reward 0.177, speed 72.55 f/s\n",
      "157076: done 5788 episodes, mean reward 0.186, speed 73.64 f/s\n",
      "157206: done 5789 episodes, mean reward 0.188, speed 71.86 f/s\n",
      "157393: done 5793 episodes, mean reward 0.186, speed 73.00 f/s\n",
      "157730: done 5794 episodes, mean reward 0.192, speed 73.00 f/s\n",
      "157825: done 5795 episodes, mean reward 0.194, speed 72.47 f/s\n",
      "158059: done 5797 episodes, mean reward 0.197, speed 71.20 f/s\n",
      "158160: done 5799 episodes, mean reward 0.197, speed 75.57 f/s\n",
      "158339: done 5801 episodes, mean reward 0.199, speed 74.16 f/s\n",
      "158603: done 5802 episodes, mean reward 0.203, speed 73.81 f/s\n",
      "158724: done 5804 episodes, mean reward 0.197, speed 71.96 f/s\n",
      "158813: done 5805 episodes, mean reward 0.196, speed 71.95 f/s\n",
      "158922: done 5806 episodes, mean reward 0.198, speed 75.03 f/s\n",
      "159134: done 5807 episodes, mean reward 0.201, speed 73.15 f/s\n",
      "159340: done 5808 episodes, mean reward 0.205, speed 73.84 f/s\n",
      "159444: done 5810 episodes, mean reward 0.204, speed 72.08 f/s\n",
      "159548: done 5812 episodes, mean reward 0.203, speed 71.40 f/s\n",
      "159653: done 5813 episodes, mean reward 0.204, speed 68.29 f/s\n",
      "159809: done 5814 episodes, mean reward 0.207, speed 71.76 f/s\n",
      "Test done in 21.32 sec, reward 0.766, steps 311\n",
      "Best reward updated: 0.382 -> 0.766\n",
      "160054: done 5815 episodes, mean reward 0.211, speed 9.90 f/s\n",
      "160366: done 5817 episodes, mean reward 0.216, speed 72.67 f/s\n",
      "160445: done 5819 episodes, mean reward 0.212, speed 72.05 f/s\n",
      "160586: done 5821 episodes, mean reward 0.206, speed 71.63 f/s\n",
      "160784: done 5823 episodes, mean reward 0.205, speed 71.36 f/s\n",
      "160997: done 5826 episodes, mean reward 0.204, speed 71.88 f/s\n",
      "161078: done 5828 episodes, mean reward 0.205, speed 72.17 f/s\n",
      "161245: done 5829 episodes, mean reward 0.208, speed 72.20 f/s\n",
      "161410: done 5831 episodes, mean reward 0.209, speed 69.76 f/s\n",
      "161532: done 5833 episodes, mean reward 0.203, speed 74.08 f/s\n",
      "161679: done 5834 episodes, mean reward 0.204, speed 74.85 f/s\n",
      "161875: done 5835 episodes, mean reward 0.208, speed 73.59 f/s\n",
      "161991: done 5837 episodes, mean reward 0.207, speed 69.52 f/s\n",
      "162081: done 5838 episodes, mean reward 0.209, speed 71.66 f/s\n",
      "162274: done 5839 episodes, mean reward 0.213, speed 73.21 f/s\n",
      "162364: done 5840 episodes, mean reward 0.210, speed 71.86 f/s\n",
      "162730: done 5842 episodes, mean reward 0.212, speed 72.35 f/s\n",
      "162852: done 5844 episodes, mean reward 0.209, speed 75.68 f/s\n",
      "162942: done 5845 episodes, mean reward 0.210, speed 72.47 f/s\n",
      "163052: done 5846 episodes, mean reward 0.208, speed 72.53 f/s\n",
      "163172: done 5847 episodes, mean reward 0.206, speed 73.57 f/s\n",
      "163312: done 5848 episodes, mean reward 0.207, speed 70.25 f/s\n",
      "163401: done 5849 episodes, mean reward 0.204, speed 72.29 f/s\n",
      "163542: done 5850 episodes, mean reward 0.206, speed 69.48 f/s\n",
      "163660: done 5852 episodes, mean reward 0.207, speed 71.79 f/s\n",
      "163750: done 5853 episodes, mean reward 0.208, speed 74.48 f/s\n",
      "163962: done 5855 episodes, mean reward 0.209, speed 74.33 f/s\n",
      "164402: done 5857 episodes, mean reward 0.219, speed 74.07 f/s\n",
      "164485: done 5859 episodes, mean reward 0.217, speed 72.74 f/s\n",
      "164666: done 5860 episodes, mean reward 0.222, speed 74.02 f/s\n",
      "164751: done 5862 episodes, mean reward 0.215, speed 70.67 f/s\n",
      "164969: done 5863 episodes, mean reward 0.220, speed 72.56 f/s\n",
      "165415: done 5865 episodes, mean reward 0.230, speed 73.66 f/s\n",
      "166114: done 5866 episodes, mean reward 0.248, speed 72.59 f/s\n",
      "166440: done 5869 episodes, mean reward 0.247, speed 73.29 f/s\n",
      "166531: done 5870 episodes, mean reward 0.248, speed 74.82 f/s\n",
      "166787: done 5872 episodes, mean reward 0.249, speed 72.94 f/s\n",
      "166993: done 5875 episodes, mean reward 0.246, speed 72.47 f/s\n",
      "167119: done 5876 episodes, mean reward 0.247, speed 73.65 f/s\n",
      "167493: done 5879 episodes, mean reward 0.254, speed 73.00 f/s\n",
      "167659: done 5880 episodes, mean reward 0.254, speed 72.49 f/s\n",
      "167846: done 5881 episodes, mean reward 0.256, speed 70.43 f/s\n",
      "167971: done 5882 episodes, mean reward 0.258, speed 72.30 f/s\n",
      "168045: done 5883 episodes, mean reward 0.255, speed 72.28 f/s\n",
      "168135: done 5884 episodes, mean reward 0.256, speed 74.34 f/s\n",
      "168451: done 5887 episodes, mean reward 0.258, speed 72.35 f/s\n",
      "168610: done 5889 episodes, mean reward 0.249, speed 71.76 f/s\n",
      "168789: done 5891 episodes, mean reward 0.253, speed 73.16 f/s\n",
      "168989: done 5893 episodes, mean reward 0.254, speed 73.46 f/s\n",
      "169343: done 5894 episodes, mean reward 0.254, speed 74.18 f/s\n",
      "169660: done 5895 episodes, mean reward 0.260, speed 71.09 f/s\n",
      "169882: done 5896 episodes, mean reward 0.264, speed 71.95 f/s\n",
      "Test done in 43.38 sec, reward 1.648, steps 634\n",
      "Best reward updated: 0.766 -> 1.648\n",
      "170033: done 5897 episodes, mean reward 0.264, speed 3.32 f/s\n",
      "170172: done 5899 episodes, mean reward 0.264, speed 72.56 f/s\n",
      "170367: done 5901 episodes, mean reward 0.264, speed 72.65 f/s\n",
      "170532: done 5902 episodes, mean reward 0.262, speed 73.19 f/s\n",
      "170660: done 5904 episodes, mean reward 0.262, speed 74.06 f/s\n",
      "170750: done 5905 episodes, mean reward 0.262, speed 73.15 f/s\n",
      "170931: done 5907 episodes, mean reward 0.260, speed 73.65 f/s\n",
      "171264: done 5909 episodes, mean reward 0.263, speed 72.76 f/s\n",
      "171468: done 5910 episodes, mean reward 0.266, speed 71.58 f/s\n",
      "171569: done 5912 episodes, mean reward 0.266, speed 72.62 f/s\n",
      "171823: done 5914 episodes, mean reward 0.266, speed 73.49 f/s\n",
      "171963: done 5915 episodes, mean reward 0.264, speed 70.54 f/s\n",
      "172392: done 5916 episodes, mean reward 0.273, speed 75.01 f/s\n",
      "172630: done 5917 episodes, mean reward 0.272, speed 70.68 f/s\n",
      "172834: done 5918 episodes, mean reward 0.276, speed 71.08 f/s\n",
      "173166: done 5919 episodes, mean reward 0.283, speed 71.25 f/s\n",
      "173247: done 5921 episodes, mean reward 0.281, speed 71.51 f/s\n",
      "173337: done 5922 episodes, mean reward 0.282, speed 72.64 f/s\n",
      "173484: done 5923 episodes, mean reward 0.282, speed 71.41 f/s\n",
      "173653: done 5924 episodes, mean reward 0.285, speed 71.02 f/s\n",
      "173838: done 5925 episodes, mean reward 0.290, speed 71.11 f/s\n",
      "174003: done 5926 episodes, mean reward 0.290, speed 75.55 f/s\n",
      "174326: done 5929 episodes, mean reward 0.292, speed 72.05 f/s\n",
      "174532: done 5930 episodes, mean reward 0.296, speed 75.52 f/s\n",
      "174737: done 5931 episodes, mean reward 0.299, speed 73.60 f/s\n",
      "174996: done 5932 episodes, mean reward 0.304, speed 74.12 f/s\n",
      "175175: done 5933 episodes, mean reward 0.307, speed 71.80 f/s\n",
      "175341: done 5934 episodes, mean reward 0.307, speed 71.60 f/s\n",
      "175563: done 5935 episodes, mean reward 0.308, speed 69.95 f/s\n",
      "175832: done 5936 episodes, mean reward 0.315, speed 74.07 f/s\n",
      "176064: done 5937 episodes, mean reward 0.318, speed 72.39 f/s\n",
      "176213: done 5938 episodes, mean reward 0.320, speed 73.96 f/s\n",
      "176438: done 5939 episodes, mean reward 0.321, speed 72.32 f/s\n",
      "176580: done 5940 episodes, mean reward 0.322, speed 72.00 f/s\n",
      "176683: done 5941 episodes, mean reward 0.324, speed 73.46 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177211: done 5942 episodes, mean reward 0.330, speed 71.58 f/s\n",
      "177472: done 5943 episodes, mean reward 0.336, speed 69.79 f/s\n",
      "177624: done 5944 episodes, mean reward 0.337, speed 72.86 f/s\n",
      "177790: done 5945 episodes, mean reward 0.339, speed 72.58 f/s\n",
      "177918: done 5946 episodes, mean reward 0.340, speed 75.61 f/s\n",
      "178136: done 5948 episodes, mean reward 0.339, speed 71.37 f/s\n",
      "178288: done 5950 episodes, mean reward 0.337, speed 73.93 f/s\n",
      "178411: done 5952 episodes, mean reward 0.337, speed 73.67 f/s\n",
      "178517: done 5953 episodes, mean reward 0.337, speed 71.91 f/s\n",
      "178607: done 5954 episodes, mean reward 0.338, speed 71.05 f/s\n",
      "178752: done 5955 episodes, mean reward 0.338, speed 73.30 f/s\n",
      "178855: done 5957 episodes, mean reward 0.329, speed 74.52 f/s\n",
      "178961: done 5958 episodes, mean reward 0.330, speed 71.08 f/s\n",
      "179063: done 5960 episodes, mean reward 0.327, speed 75.22 f/s\n",
      "179178: done 5962 episodes, mean reward 0.328, speed 72.73 f/s\n",
      "179324: done 5963 episodes, mean reward 0.326, speed 71.44 f/s\n",
      "179413: done 5964 episodes, mean reward 0.326, speed 74.73 f/s\n",
      "179751: done 5965 episodes, mean reward 0.325, speed 71.48 f/s\n",
      "179893: done 5967 episodes, mean reward 0.310, speed 70.60 f/s\n",
      "179997: done 5968 episodes, mean reward 0.312, speed 72.61 f/s\n",
      "Test done in 31.20 sec, reward 1.165, steps 455\n",
      "180109: done 5969 episodes, mean reward 0.308, speed 3.40 f/s\n",
      "180530: done 5971 episodes, mean reward 0.316, speed 73.67 f/s\n",
      "180624: done 5973 episodes, mean reward 0.312, speed 70.43 f/s\n",
      "180804: done 5975 episodes, mean reward 0.313, speed 70.33 f/s\n",
      "180892: done 5976 episodes, mean reward 0.311, speed 71.95 f/s\n",
      "181213: done 5977 episodes, mean reward 0.318, speed 73.50 f/s\n",
      "181335: done 5979 episodes, mean reward 0.313, speed 72.79 f/s\n",
      "181524: done 5981 episodes, mean reward 0.308, speed 73.94 f/s\n",
      "181627: done 5983 episodes, mean reward 0.305, speed 71.95 f/s\n",
      "181714: done 5984 episodes, mean reward 0.305, speed 72.06 f/s\n",
      "181795: done 5986 episodes, mean reward 0.305, speed 72.68 f/s\n",
      "181956: done 5987 episodes, mean reward 0.302, speed 69.86 f/s\n",
      "182046: done 5988 episodes, mean reward 0.304, speed 75.32 f/s\n",
      "182163: done 5990 episodes, mean reward 0.302, speed 73.67 f/s\n",
      "182269: done 5991 episodes, mean reward 0.302, speed 71.93 f/s\n",
      "182490: done 5993 episodes, mean reward 0.302, speed 72.75 f/s\n",
      "182682: done 5994 episodes, mean reward 0.298, speed 72.99 f/s\n",
      "182771: done 5995 episodes, mean reward 0.292, speed 74.27 f/s\n",
      "182899: done 5996 episodes, mean reward 0.290, speed 70.80 f/s\n",
      "183517: done 5999 episodes, mean reward 0.299, speed 72.25 f/s\n",
      "183716: done 6000 episodes, mean reward 0.303, speed 71.36 f/s\n",
      "183803: done 6001 episodes, mean reward 0.302, speed 72.12 f/s\n",
      "183892: done 6002 episodes, mean reward 0.300, speed 70.99 f/s\n",
      "184042: done 6006 episodes, mean reward 0.296, speed 73.19 f/s\n",
      "184184: done 6008 episodes, mean reward 0.295, speed 74.94 f/s\n",
      "184274: done 6009 episodes, mean reward 0.290, speed 72.19 f/s\n",
      "184430: done 6010 episodes, mean reward 0.289, speed 70.17 f/s\n",
      "184593: done 6011 episodes, mean reward 0.291, speed 70.84 f/s\n",
      "184676: done 6013 episodes, mean reward 0.291, speed 71.11 f/s\n",
      "184835: done 6015 episodes, mean reward 0.286, speed 73.69 f/s\n",
      "185065: done 6016 episodes, mean reward 0.282, speed 72.60 f/s\n",
      "185155: done 6017 episodes, mean reward 0.278, speed 72.33 f/s\n",
      "185263: done 6018 episodes, mean reward 0.276, speed 75.89 f/s\n",
      "185578: done 6021 episodes, mean reward 0.275, speed 70.81 f/s\n",
      "185689: done 6022 episodes, mean reward 0.276, speed 72.04 f/s\n",
      "185770: done 6023 episodes, mean reward 0.274, speed 71.06 f/s\n",
      "185973: done 6024 episodes, mean reward 0.275, speed 72.11 f/s\n",
      "186315: done 6025 episodes, mean reward 0.279, speed 71.14 f/s\n",
      "186419: done 6026 episodes, mean reward 0.278, speed 73.41 f/s\n",
      "186564: done 6027 episodes, mean reward 0.281, speed 72.05 f/s\n",
      "186653: done 6028 episodes, mean reward 0.282, speed 75.24 f/s\n",
      "186762: done 6029 episodes, mean reward 0.279, speed 74.07 f/s\n",
      "186939: done 6031 episodes, mean reward 0.273, speed 73.87 f/s\n",
      "187085: done 6032 episodes, mean reward 0.270, speed 70.89 f/s\n",
      "187186: done 6034 episodes, mean reward 0.264, speed 71.65 f/s\n",
      "187614: done 6036 episodes, mean reward 0.262, speed 71.68 f/s\n",
      "187746: done 6038 episodes, mean reward 0.256, speed 72.54 f/s\n",
      "188045: done 6039 episodes, mean reward 0.258, speed 72.68 f/s\n",
      "188155: done 6040 episodes, mean reward 0.257, speed 70.03 f/s\n",
      "188397: done 6041 episodes, mean reward 0.261, speed 71.28 f/s\n",
      "188525: done 6042 episodes, mean reward 0.251, speed 71.41 f/s\n",
      "188654: done 6043 episodes, mean reward 0.247, speed 69.43 f/s\n",
      "188782: done 6044 episodes, mean reward 0.247, speed 71.52 f/s\n",
      "189024: done 6045 episodes, mean reward 0.249, speed 72.40 f/s\n",
      "189124: done 6046 episodes, mean reward 0.248, speed 72.25 f/s\n",
      "189329: done 6047 episodes, mean reward 0.252, speed 71.04 f/s\n",
      "189557: done 6048 episodes, mean reward 0.254, speed 73.33 f/s\n",
      "189813: done 6050 episodes, mean reward 0.257, speed 71.93 f/s\n",
      "189896: done 6052 episodes, mean reward 0.256, speed 71.76 f/s\n",
      "Test done in 36.95 sec, reward 1.360, steps 535\n",
      "190004: done 6053 episodes, mean reward 0.256, speed 2.81 f/s\n",
      "190126: done 6055 episodes, mean reward 0.253, speed 69.05 f/s\n",
      "190222: done 6057 episodes, mean reward 0.253, speed 72.35 f/s\n",
      "190521: done 6058 episodes, mean reward 0.258, speed 71.43 f/s\n",
      "190793: done 6060 episodes, mean reward 0.262, speed 72.24 f/s\n",
      "190941: done 6062 episodes, mean reward 0.263, speed 71.03 f/s\n",
      "191158: done 6064 episodes, mean reward 0.262, speed 72.03 f/s\n",
      "191305: done 6065 episodes, mean reward 0.257, speed 72.96 f/s\n",
      "191811: done 6067 episodes, mean reward 0.267, speed 73.22 f/s\n",
      "191949: done 6069 episodes, mean reward 0.265, speed 74.27 f/s\n",
      "192179: done 6071 episodes, mean reward 0.260, speed 71.39 f/s\n",
      "192264: done 6073 episodes, mean reward 0.259, speed 73.33 f/s\n",
      "192447: done 6075 episodes, mean reward 0.259, speed 73.23 f/s\n",
      "192620: done 6077 episodes, mean reward 0.254, speed 73.38 f/s\n",
      "192838: done 6079 episodes, mean reward 0.256, speed 73.50 f/s\n",
      "192972: done 6082 episodes, mean reward 0.255, speed 74.67 f/s\n",
      "193397: done 6083 episodes, mean reward 0.264, speed 71.49 f/s\n",
      "194000: done 6084 episodes, mean reward 0.278, speed 73.08 f/s\n",
      "194103: done 6086 episodes, mean reward 0.279, speed 76.40 f/s\n",
      "194249: done 6087 episodes, mean reward 0.279, speed 71.79 f/s\n",
      "194428: done 6088 episodes, mean reward 0.282, speed 72.55 f/s\n",
      "195127: done 6089 episodes, mean reward 0.300, speed 72.96 f/s\n",
      "195255: done 6090 episodes, mean reward 0.300, speed 72.79 f/s\n",
      "195812: done 6091 episodes, mean reward 0.312, speed 71.67 f/s\n",
      "195921: done 6092 episodes, mean reward 0.313, speed 72.65 f/s\n",
      "196087: done 6093 episodes, mean reward 0.313, speed 73.72 f/s\n",
      "196305: done 6095 episodes, mean reward 0.311, speed 74.22 f/s\n",
      "196415: done 6096 episodes, mean reward 0.311, speed 70.71 f/s\n",
      "196784: done 6099 episodes, mean reward 0.305, speed 72.81 f/s\n",
      "197002: done 6100 episodes, mean reward 0.305, speed 74.30 f/s\n",
      "197200: done 6101 episodes, mean reward 0.307, speed 73.27 f/s\n",
      "197481: done 6102 episodes, mean reward 0.312, speed 75.82 f/s\n",
      "197565: done 6103 episodes, mean reward 0.314, speed 69.53 f/s\n",
      "198238: done 6105 episodes, mean reward 0.331, speed 72.84 f/s\n",
      "198499: done 6106 episodes, mean reward 0.335, speed 73.50 f/s\n",
      "198627: done 6107 episodes, mean reward 0.338, speed 72.86 f/s\n",
      "198793: done 6108 episodes, mean reward 0.339, speed 72.36 f/s\n",
      "199316: done 6110 episodes, mean reward 0.347, speed 72.45 f/s\n",
      "199672: done 6111 episodes, mean reward 0.352, speed 71.99 f/s\n",
      "199922: done 6112 episodes, mean reward 0.358, speed 74.63 f/s\n",
      "199995: done 6114 episodes, mean reward 0.357, speed 72.32 f/s\n",
      "Test done in 51.73 sec, reward 1.991, steps 762\n",
      "Best reward updated: 1.648 -> 1.991\n",
      "200205: done 6115 episodes, mean reward 0.359, speed 3.84 f/s\n",
      "200294: done 6116 episodes, mean reward 0.355, speed 73.03 f/s\n",
      "200440: done 6117 episodes, mean reward 0.356, speed 71.57 f/s\n",
      "200628: done 6119 episodes, mean reward 0.357, speed 73.14 f/s\n",
      "200737: done 6120 episodes, mean reward 0.359, speed 73.67 f/s\n",
      "201130: done 6121 episodes, mean reward 0.363, speed 71.84 f/s\n",
      "201299: done 6124 episodes, mean reward 0.356, speed 73.60 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201478: done 6125 episodes, mean reward 0.352, speed 72.31 f/s\n",
      "201892: done 6126 episodes, mean reward 0.360, speed 72.70 f/s\n",
      "201981: done 6127 episodes, mean reward 0.358, speed 72.44 f/s\n",
      "202869: done 6128 episodes, mean reward 0.379, speed 72.31 f/s\n",
      "203073: done 6129 episodes, mean reward 0.382, speed 74.35 f/s\n",
      "203272: done 6131 episodes, mean reward 0.382, speed 73.21 f/s\n",
      "203529: done 6133 episodes, mean reward 0.384, speed 72.49 f/s\n",
      "203658: done 6134 episodes, mean reward 0.386, speed 72.17 f/s\n",
      "203896: done 6135 episodes, mean reward 0.391, speed 72.17 f/s\n",
      "204156: done 6136 episodes, mean reward 0.387, speed 72.03 f/s\n",
      "204412: done 6137 episodes, mean reward 0.392, speed 71.55 f/s\n",
      "204506: done 6138 episodes, mean reward 0.393, speed 74.17 f/s\n",
      "204608: done 6139 episodes, mean reward 0.388, speed 74.05 f/s\n",
      "204806: done 6140 episodes, mean reward 0.390, speed 73.38 f/s\n",
      "205067: done 6141 episodes, mean reward 0.390, speed 73.05 f/s\n",
      "205621: done 6142 episodes, mean reward 0.401, speed 72.75 f/s\n",
      "206025: done 6144 episodes, mean reward 0.405, speed 74.26 f/s\n",
      "206167: done 6146 episodes, mean reward 0.399, speed 72.31 f/s\n",
      "206270: done 6147 episodes, mean reward 0.396, speed 71.04 f/s\n",
      "206443: done 6149 episodes, mean reward 0.395, speed 71.20 f/s\n",
      "206639: done 6151 episodes, mean reward 0.391, speed 73.92 f/s\n",
      "206875: done 6153 episodes, mean reward 0.394, speed 74.03 f/s\n",
      "207281: done 6154 episodes, mean reward 0.404, speed 73.87 f/s\n",
      "207500: done 6155 episodes, mean reward 0.408, speed 73.77 f/s\n",
      "207830: done 6157 episodes, mean reward 0.414, speed 73.03 f/s\n",
      "208246: done 6160 episodes, mean reward 0.410, speed 71.56 f/s\n",
      "208544: done 6161 episodes, mean reward 0.416, speed 70.98 f/s\n",
      "208710: done 6162 episodes, mean reward 0.418, speed 71.21 f/s\n",
      "208817: done 6163 episodes, mean reward 0.421, speed 72.15 f/s\n",
      "209468: done 6165 episodes, mean reward 0.429, speed 72.91 f/s\n",
      "209848: done 6167 episodes, mean reward 0.426, speed 72.18 f/s\n",
      "209990: done 6168 episodes, mean reward 0.428, speed 74.81 f/s\n",
      "Test done in 56.92 sec, reward 2.184, steps 835\n",
      "Best reward updated: 1.991 -> 2.184\n",
      "210080: done 6169 episodes, mean reward 0.428, speed 1.55 f/s\n",
      "210455: done 6170 episodes, mean reward 0.438, speed 72.64 f/s\n",
      "210578: done 6171 episodes, mean reward 0.435, speed 71.82 f/s\n",
      "210871: done 6173 episodes, mean reward 0.440, speed 73.70 f/s\n",
      "211021: done 6174 episodes, mean reward 0.443, speed 73.07 f/s\n",
      "211485: done 6176 episodes, mean reward 0.450, speed 73.16 f/s\n",
      "211766: done 6177 episodes, mean reward 0.455, speed 71.53 f/s\n",
      "212122: done 6178 episodes, mean reward 0.464, speed 72.53 f/s\n",
      "212290: done 6179 episodes, mean reward 0.463, speed 74.82 f/s\n",
      "212373: done 6181 episodes, mean reward 0.463, speed 73.33 f/s\n",
      "212463: done 6182 episodes, mean reward 0.464, speed 72.41 f/s\n",
      "212685: done 6183 episodes, mean reward 0.459, speed 72.69 f/s\n",
      "212960: done 6185 episodes, mean reward 0.449, speed 72.85 f/s\n",
      "213483: done 6187 episodes, mean reward 0.457, speed 71.83 f/s\n",
      "213630: done 6188 episodes, mean reward 0.456, speed 72.79 f/s\n",
      "213949: done 6190 episodes, mean reward 0.442, speed 72.09 f/s\n",
      "214181: done 6192 episodes, mean reward 0.431, speed 72.93 f/s\n",
      "214361: done 6194 episodes, mean reward 0.430, speed 71.71 f/s\n",
      "214528: done 6195 episodes, mean reward 0.430, speed 72.27 f/s\n",
      "214751: done 6196 episodes, mean reward 0.433, speed 70.52 f/s\n",
      "214954: done 6197 episodes, mean reward 0.438, speed 73.16 f/s\n",
      "215058: done 6198 episodes, mean reward 0.439, speed 69.59 f/s\n",
      "215174: done 6200 episodes, mean reward 0.429, speed 73.02 f/s\n",
      "215391: done 6202 episodes, mean reward 0.423, speed 71.46 f/s\n",
      "215530: done 6203 episodes, mean reward 0.424, speed 70.06 f/s\n",
      "215727: done 6204 episodes, mean reward 0.429, speed 71.81 f/s\n",
      "215850: done 6205 episodes, mean reward 0.415, speed 72.34 f/s\n",
      "215971: done 6207 episodes, mean reward 0.408, speed 71.04 f/s\n",
      "216150: done 6209 episodes, mean reward 0.407, speed 73.11 f/s\n",
      "216275: done 6210 episodes, mean reward 0.397, speed 73.82 f/s\n",
      "216431: done 6211 episodes, mean reward 0.391, speed 71.74 f/s\n",
      "216767: done 6212 episodes, mean reward 0.394, speed 72.30 f/s\n",
      "216990: done 6213 episodes, mean reward 0.399, speed 72.48 f/s\n",
      "217209: done 6214 episodes, mean reward 0.404, speed 72.29 f/s\n",
      "217464: done 6215 episodes, mean reward 0.406, speed 72.30 f/s\n",
      "217704: done 6216 episodes, mean reward 0.410, speed 72.02 f/s\n",
      "217915: done 6217 episodes, mean reward 0.411, speed 72.83 f/s\n",
      "218024: done 6218 episodes, mean reward 0.412, speed 71.48 f/s\n",
      "218204: done 6220 episodes, mean reward 0.411, speed 70.56 f/s\n",
      "218616: done 6221 episodes, mean reward 0.412, speed 70.71 f/s\n",
      "218697: done 6223 episodes, mean reward 0.413, speed 72.07 f/s\n",
      "219045: done 6225 episodes, mean reward 0.415, speed 72.52 f/s\n",
      "219458: done 6226 episodes, mean reward 0.415, speed 72.37 f/s\n",
      "219887: done 6227 episodes, mean reward 0.424, speed 72.49 f/s\n",
      "Test done in 35.45 sec, reward 1.307, steps 517\n",
      "220002: done 6229 episodes, mean reward 0.398, speed 3.10 f/s\n",
      "220075: done 6230 episodes, mean reward 0.400, speed 67.49 f/s\n",
      "220298: done 6231 episodes, mean reward 0.401, speed 71.10 f/s\n",
      "220482: done 6232 episodes, mean reward 0.404, speed 74.59 f/s\n",
      "220571: done 6233 episodes, mean reward 0.401, speed 72.41 f/s\n",
      "221003: done 6234 episodes, mean reward 0.409, speed 72.68 f/s\n",
      "221180: done 6236 episodes, mean reward 0.401, speed 70.48 f/s\n",
      "221419: done 6237 episodes, mean reward 0.400, speed 72.55 f/s\n",
      "221650: done 6239 episodes, mean reward 0.401, speed 72.04 f/s\n",
      "221831: done 6241 episodes, mean reward 0.394, speed 72.63 f/s\n",
      "222007: done 6243 episodes, mean reward 0.383, speed 72.54 f/s\n",
      "222914: done 6244 episodes, mean reward 0.398, speed 72.52 f/s\n",
      "223061: done 6245 episodes, mean reward 0.401, speed 72.68 f/s\n",
      "223300: done 6246 episodes, mean reward 0.405, speed 72.01 f/s\n",
      "223544: done 6248 episodes, mean reward 0.408, speed 72.45 f/s\n",
      "223648: done 6249 episodes, mean reward 0.407, speed 72.28 f/s\n",
      "223846: done 6250 episodes, mean reward 0.411, speed 73.09 f/s\n",
      "223936: done 6251 episodes, mean reward 0.410, speed 73.65 f/s\n",
      "224043: done 6252 episodes, mean reward 0.411, speed 75.51 f/s\n",
      "224239: done 6254 episodes, mean reward 0.401, speed 71.29 f/s\n",
      "224403: done 6255 episodes, mean reward 0.399, speed 70.51 f/s\n",
      "224512: done 6256 episodes, mean reward 0.401, speed 71.72 f/s\n",
      "224677: done 6257 episodes, mean reward 0.397, speed 72.50 f/s\n",
      "225073: done 6258 episodes, mean reward 0.407, speed 72.47 f/s\n",
      "225409: done 6260 episodes, mean reward 0.405, speed 71.00 f/s\n",
      "226015: done 6262 episodes, mean reward 0.409, speed 73.16 f/s\n",
      "226105: done 6263 episodes, mean reward 0.409, speed 69.24 f/s\n",
      "226330: done 6264 episodes, mean reward 0.414, speed 72.67 f/s\n",
      "226534: done 6265 episodes, mean reward 0.403, speed 70.78 f/s\n",
      "226662: done 6266 episodes, mean reward 0.405, speed 73.57 f/s\n",
      "226823: done 6268 episodes, mean reward 0.397, speed 74.56 f/s\n",
      "226970: done 6269 episodes, mean reward 0.398, speed 72.03 f/s\n",
      "227270: done 6270 episodes, mean reward 0.396, speed 73.48 f/s\n",
      "227475: done 6271 episodes, mean reward 0.399, speed 70.91 f/s\n",
      "227918: done 6273 episodes, mean reward 0.403, speed 73.31 f/s\n",
      "228173: done 6275 episodes, mean reward 0.404, speed 72.22 f/s\n",
      "228547: done 6276 episodes, mean reward 0.403, speed 73.43 f/s\n",
      "228734: done 6277 episodes, mean reward 0.401, speed 71.37 f/s\n",
      "228814: done 6278 episodes, mean reward 0.393, speed 71.37 f/s\n",
      "229815: done 6279 episodes, mean reward 0.416, speed 72.28 f/s\n",
      "229893: done 6280 episodes, mean reward 0.417, speed 74.34 f/s\n",
      "Test done in 55.54 sec, reward 2.140, steps 820\n",
      "230233: done 6281 episodes, mean reward 0.425, speed 5.63 f/s\n",
      "230357: done 6282 episodes, mean reward 0.425, speed 74.02 f/s\n",
      "230503: done 6283 episodes, mean reward 0.423, speed 73.36 f/s\n",
      "230731: done 6285 episodes, mean reward 0.422, speed 72.20 f/s\n",
      "231353: done 6286 episodes, mean reward 0.437, speed 73.32 f/s\n",
      "231539: done 6287 episodes, mean reward 0.430, speed 75.47 f/s\n",
      "231757: done 6288 episodes, mean reward 0.432, speed 73.96 f/s\n",
      "231847: done 6289 episodes, mean reward 0.433, speed 72.29 f/s\n",
      "232162: done 6290 episodes, mean reward 0.435, speed 74.13 f/s\n",
      "232306: done 6291 episodes, mean reward 0.437, speed 72.85 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232489: done 6292 episodes, mean reward 0.437, speed 70.25 f/s\n",
      "233375: done 6293 episodes, mean reward 0.459, speed 70.83 f/s\n",
      "233458: done 6294 episodes, mean reward 0.457, speed 72.62 f/s\n",
      "233681: done 6295 episodes, mean reward 0.459, speed 69.72 f/s\n",
      "234107: done 6297 episodes, mean reward 0.458, speed 69.42 f/s\n",
      "234292: done 6298 episodes, mean reward 0.460, speed 71.72 f/s\n",
      "234944: done 6299 episodes, mean reward 0.475, speed 70.01 f/s\n",
      "235289: done 6300 episodes, mean reward 0.483, speed 71.00 f/s\n",
      "235462: done 6302 episodes, mean reward 0.482, speed 72.11 f/s\n",
      "235801: done 6303 episodes, mean reward 0.488, speed 69.64 f/s\n",
      "235949: done 6304 episodes, mean reward 0.487, speed 70.62 f/s\n",
      "236038: done 6305 episodes, mean reward 0.486, speed 71.83 f/s\n",
      "236172: done 6307 episodes, mean reward 0.486, speed 69.94 f/s\n",
      "236315: done 6308 episodes, mean reward 0.489, speed 72.15 f/s\n",
      "236782: done 6309 episodes, mean reward 0.497, speed 70.11 f/s\n",
      "237047: done 6311 episodes, mean reward 0.497, speed 69.85 f/s\n",
      "237192: done 6312 episodes, mean reward 0.492, speed 73.45 f/s\n",
      "237598: done 6315 episodes, mean reward 0.484, speed 70.15 f/s\n",
      "237745: done 6316 episodes, mean reward 0.482, speed 68.67 f/s\n",
      "238063: done 6317 episodes, mean reward 0.485, speed 72.30 f/s\n",
      "238152: done 6318 episodes, mean reward 0.484, speed 70.20 f/s\n",
      "238259: done 6320 episodes, mean reward 0.482, speed 70.47 f/s\n",
      "238387: done 6321 episodes, mean reward 0.474, speed 70.84 f/s\n",
      "238728: done 6323 episodes, mean reward 0.481, speed 70.82 f/s\n",
      "238951: done 6324 episodes, mean reward 0.486, speed 71.67 f/s\n",
      "239059: done 6325 episodes, mean reward 0.480, speed 70.51 f/s\n",
      "239150: done 6326 episodes, mean reward 0.472, speed 68.15 f/s\n",
      "239309: done 6327 episodes, mean reward 0.465, speed 71.10 f/s\n",
      "239413: done 6328 episodes, mean reward 0.466, speed 72.01 f/s\n",
      "239502: done 6329 episodes, mean reward 0.467, speed 73.36 f/s\n",
      "239763: done 6330 episodes, mean reward 0.472, speed 70.71 f/s\n",
      "Test done in 55.76 sec, reward 2.127, steps 814\n",
      "240137: done 6331 episodes, mean reward 0.476, speed 6.12 f/s\n",
      "240436: done 6332 episodes, mean reward 0.479, speed 73.83 f/s\n",
      "240586: done 6333 episodes, mean reward 0.481, speed 73.24 f/s\n",
      "240694: done 6334 episodes, mean reward 0.472, speed 71.51 f/s\n",
      "240803: done 6335 episodes, mean reward 0.474, speed 71.28 f/s\n",
      "241054: done 6337 episodes, mean reward 0.470, speed 74.19 f/s\n",
      "241482: done 6338 episodes, mean reward 0.480, speed 71.19 f/s\n",
      "241630: done 6340 episodes, mean reward 0.478, speed 72.81 f/s\n",
      "241720: done 6341 episodes, mean reward 0.477, speed 74.87 f/s\n",
      "241850: done 6343 episodes, mean reward 0.475, speed 73.33 f/s\n",
      "241939: done 6345 episodes, mean reward 0.449, speed 72.21 f/s\n",
      "242480: done 6347 episodes, mean reward 0.455, speed 72.40 f/s\n",
      "242562: done 6349 episodes, mean reward 0.450, speed 67.23 f/s\n",
      "242766: done 6350 episodes, mean reward 0.451, speed 72.15 f/s\n",
      "243141: done 6351 episodes, mean reward 0.458, speed 75.03 f/s\n",
      "243263: done 6353 episodes, mean reward 0.456, speed 72.88 f/s\n",
      "243461: done 6355 episodes, mean reward 0.454, speed 73.39 f/s\n",
      "243626: done 6356 episodes, mean reward 0.455, speed 71.89 f/s\n",
      "243808: done 6359 episodes, mean reward 0.444, speed 72.80 f/s\n",
      "243973: done 6360 episodes, mean reward 0.441, speed 72.24 f/s\n",
      "244617: done 6362 episodes, mean reward 0.442, speed 72.47 f/s\n",
      "245137: done 6363 episodes, mean reward 0.453, speed 72.14 f/s\n",
      "245378: done 6364 episodes, mean reward 0.454, speed 74.34 f/s\n",
      "245488: done 6365 episodes, mean reward 0.451, speed 72.25 f/s\n",
      "245987: done 6366 episodes, mean reward 0.461, speed 72.36 f/s\n",
      "246077: done 6367 episodes, mean reward 0.462, speed 72.20 f/s\n",
      "246206: done 6368 episodes, mean reward 0.462, speed 70.89 f/s\n",
      "246482: done 6369 episodes, mean reward 0.466, speed 72.86 f/s\n",
      "246661: done 6371 episodes, mean reward 0.457, speed 73.42 f/s\n",
      "246789: done 6372 episodes, mean reward 0.460, speed 72.34 f/s\n",
      "247131: done 6374 episodes, mean reward 0.457, speed 72.40 f/s\n",
      "247449: done 6375 episodes, mean reward 0.459, speed 74.18 f/s\n",
      "247974: done 6376 episodes, mean reward 0.463, speed 71.65 f/s\n",
      "248464: done 6377 episodes, mean reward 0.471, speed 72.54 f/s\n",
      "249106: done 6378 episodes, mean reward 0.486, speed 72.80 f/s\n",
      "249425: done 6379 episodes, mean reward 0.468, speed 71.81 f/s\n",
      "Test done in 54.23 sec, reward 2.078, steps 795\n",
      "250426: done 6380 episodes, mean reward 0.492, speed 14.66 f/s\n",
      "250649: done 6381 episodes, mean reward 0.489, speed 73.31 f/s\n",
      "251025: done 6382 episodes, mean reward 0.496, speed 72.61 f/s\n",
      "251849: done 6383 episodes, mean reward 0.513, speed 72.41 f/s\n",
      "252395: done 6384 episodes, mean reward 0.526, speed 73.35 f/s\n",
      "252880: done 6385 episodes, mean reward 0.534, speed 73.36 f/s\n",
      "253084: done 6386 episodes, mean reward 0.523, speed 73.62 f/s\n",
      "253250: done 6387 episodes, mean reward 0.522, speed 71.91 f/s\n",
      "253627: done 6388 episodes, mean reward 0.526, speed 73.33 f/s\n",
      "253815: done 6389 episodes, mean reward 0.528, speed 70.62 f/s\n",
      "254179: done 6390 episodes, mean reward 0.529, speed 71.22 f/s\n",
      "254266: done 6391 episodes, mean reward 0.529, speed 74.89 f/s\n",
      "255263: done 6392 episodes, mean reward 0.550, speed 72.40 f/s\n",
      "255341: done 6394 episodes, mean reward 0.527, speed 74.11 f/s\n",
      "255621: done 6395 episodes, mean reward 0.528, speed 73.36 f/s\n",
      "255709: done 6396 episodes, mean reward 0.529, speed 70.82 f/s\n",
      "256185: done 6397 episodes, mean reward 0.531, speed 72.30 f/s\n",
      "256328: done 6398 episodes, mean reward 0.530, speed 69.40 f/s\n",
      "256906: done 6399 episodes, mean reward 0.529, speed 72.88 f/s\n",
      "257294: done 6400 episodes, mean reward 0.530, speed 70.75 f/s\n",
      "257557: done 6403 episodes, mean reward 0.523, speed 72.22 f/s\n",
      "257780: done 6404 episodes, mean reward 0.525, speed 73.38 f/s\n",
      "257921: done 6405 episodes, mean reward 0.526, speed 72.26 f/s\n",
      "258505: done 6406 episodes, mean reward 0.541, speed 72.53 f/s\n",
      "258729: done 6407 episodes, mean reward 0.544, speed 74.18 f/s\n",
      "259004: done 6408 episodes, mean reward 0.547, speed 70.62 f/s\n",
      "259107: done 6409 episodes, mean reward 0.538, speed 70.28 f/s\n",
      "259234: done 6411 episodes, mean reward 0.535, speed 73.20 f/s\n",
      "259372: done 6412 episodes, mean reward 0.535, speed 73.52 f/s\n",
      "259649: done 6413 episodes, mean reward 0.542, speed 70.56 f/s\n",
      "259849: done 6415 episodes, mean reward 0.537, speed 72.15 f/s\n",
      "Test done in 65.33 sec, reward 2.511, steps 954\n",
      "Best reward updated: 2.184 -> 2.511\n",
      "260124: done 6416 episodes, mean reward 0.540, speed 3.97 f/s\n",
      "260274: done 6417 episodes, mean reward 0.536, speed 71.15 f/s\n",
      "260558: done 6418 episodes, mean reward 0.541, speed 73.87 f/s\n",
      "260883: done 6420 episodes, mean reward 0.547, speed 73.20 f/s\n",
      "260968: done 6421 episodes, mean reward 0.546, speed 67.71 f/s\n",
      "261217: done 6422 episodes, mean reward 0.551, speed 71.93 f/s\n",
      "261322: done 6424 episodes, mean reward 0.539, speed 71.36 f/s\n",
      "261412: done 6425 episodes, mean reward 0.539, speed 71.53 f/s\n",
      "261685: done 6426 episodes, mean reward 0.544, speed 70.86 f/s\n",
      "261842: done 6427 episodes, mean reward 0.544, speed 71.74 f/s\n",
      "261970: done 6428 episodes, mean reward 0.545, speed 73.05 f/s\n",
      "262325: done 6429 episodes, mean reward 0.552, speed 73.40 f/s\n",
      "262463: done 6430 episodes, mean reward 0.548, speed 71.91 f/s\n",
      "262648: done 6431 episodes, mean reward 0.543, speed 72.91 f/s\n",
      "262827: done 6432 episodes, mean reward 0.540, speed 73.87 f/s\n",
      "263185: done 6434 episodes, mean reward 0.543, speed 72.71 f/s\n",
      "263294: done 6435 episodes, mean reward 0.543, speed 73.13 f/s\n",
      "264184: done 6436 episodes, mean reward 0.565, speed 71.84 f/s\n",
      "264328: done 6437 episodes, mean reward 0.564, speed 72.33 f/s\n",
      "264461: done 6439 episodes, mean reward 0.555, speed 70.40 f/s\n",
      "264663: done 6440 episodes, mean reward 0.557, speed 70.72 f/s\n",
      "264818: done 6442 episodes, mean reward 0.559, speed 73.16 f/s\n",
      "264965: done 6443 episodes, mean reward 0.560, speed 73.38 f/s\n",
      "265087: done 6445 episodes, mean reward 0.562, speed 73.08 f/s\n",
      "266128: done 6447 episodes, mean reward 0.574, speed 72.62 f/s\n",
      "266521: done 6448 episodes, mean reward 0.583, speed 73.07 f/s\n",
      "266853: done 6449 episodes, mean reward 0.591, speed 71.76 f/s\n",
      "267285: done 6450 episodes, mean reward 0.597, speed 72.12 f/s\n",
      "267391: done 6451 episodes, mean reward 0.590, speed 74.10 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267475: done 6452 episodes, mean reward 0.590, speed 72.59 f/s\n",
      "267596: done 6454 episodes, mean reward 0.592, speed 73.39 f/s\n",
      "267687: done 6455 episodes, mean reward 0.589, speed 68.97 f/s\n",
      "268117: done 6456 episodes, mean reward 0.596, speed 72.12 f/s\n",
      "268278: done 6458 episodes, mean reward 0.600, speed 73.72 f/s\n",
      "268782: done 6459 episodes, mean reward 0.609, speed 73.17 f/s\n",
      "269025: done 6460 episodes, mean reward 0.612, speed 71.39 f/s\n",
      "269204: done 6461 episodes, mean reward 0.616, speed 75.13 f/s\n",
      "269408: done 6462 episodes, mean reward 0.605, speed 73.09 f/s\n",
      "269495: done 6463 episodes, mean reward 0.593, speed 70.64 f/s\n",
      "269797: done 6464 episodes, mean reward 0.594, speed 72.30 f/s\n",
      "269906: done 6465 episodes, mean reward 0.594, speed 70.14 f/s\n",
      "Test done in 48.73 sec, reward 1.848, steps 715\n",
      "270155: done 6466 episodes, mean reward 0.588, speed 4.76 f/s\n",
      "270562: done 6468 episodes, mean reward 0.592, speed 72.84 f/s\n",
      "270666: done 6470 episodes, mean reward 0.586, speed 69.64 f/s\n",
      "270756: done 6471 episodes, mean reward 0.585, speed 71.24 f/s\n",
      "270911: done 6473 episodes, mean reward 0.585, speed 73.19 f/s\n",
      "271285: done 6474 episodes, mean reward 0.587, speed 72.14 f/s\n",
      "271412: done 6475 episodes, mean reward 0.582, speed 72.40 f/s\n",
      "271503: done 6476 episodes, mean reward 0.571, speed 72.21 f/s\n",
      "272020: done 6477 episodes, mean reward 0.571, speed 72.10 f/s\n",
      "272095: done 6478 episodes, mean reward 0.556, speed 73.40 f/s\n",
      "272505: done 6479 episodes, mean reward 0.559, speed 72.19 f/s\n",
      "272594: done 6480 episodes, mean reward 0.535, speed 74.45 f/s\n",
      "272817: done 6481 episodes, mean reward 0.535, speed 72.94 f/s\n",
      "273339: done 6483 episodes, mean reward 0.517, speed 73.04 f/s\n",
      "273467: done 6484 episodes, mean reward 0.507, speed 74.16 f/s\n",
      "273670: done 6485 episodes, mean reward 0.499, speed 74.18 f/s\n",
      "273863: done 6486 episodes, mean reward 0.499, speed 71.94 f/s\n",
      "274290: done 6487 episodes, mean reward 0.506, speed 72.42 f/s\n",
      "274566: done 6489 episodes, mean reward 0.499, speed 73.47 f/s\n",
      "274915: done 6491 episodes, mean reward 0.496, speed 70.55 f/s\n",
      "275324: done 6492 episodes, mean reward 0.480, speed 72.87 f/s\n",
      "275791: done 6493 episodes, mean reward 0.491, speed 73.05 f/s\n",
      "276122: done 6495 episodes, mean reward 0.492, speed 73.25 f/s\n",
      "276496: done 6496 episodes, mean reward 0.500, speed 70.72 f/s\n",
      "276815: done 6499 episodes, mean reward 0.476, speed 72.41 f/s\n",
      "277043: done 6501 episodes, mean reward 0.471, speed 73.73 f/s\n",
      "277160: done 6503 episodes, mean reward 0.468, speed 70.94 f/s\n",
      "277305: done 6504 episodes, mean reward 0.466, speed 73.00 f/s\n",
      "277661: done 6505 episodes, mean reward 0.472, speed 76.52 f/s\n",
      "278662: done 6506 episodes, mean reward 0.484, speed 72.58 f/s\n",
      "279360: done 6508 episodes, mean reward 0.490, speed 72.82 f/s\n",
      "279654: done 6509 episodes, mean reward 0.495, speed 73.26 f/s\n",
      "279731: done 6511 episodes, mean reward 0.494, speed 74.60 f/s\n",
      "279973: done 6514 episodes, mean reward 0.488, speed 74.23 f/s\n",
      "Test done in 18.66 sec, reward 0.679, steps 275\n",
      "280054: done 6515 episodes, mean reward 0.486, speed 4.09 f/s\n",
      "280206: done 6516 episodes, mean reward 0.484, speed 73.12 f/s\n",
      "280402: done 6518 episodes, mean reward 0.478, speed 72.70 f/s\n",
      "280578: done 6519 episodes, mean reward 0.481, speed 73.82 f/s\n",
      "280763: done 6520 episodes, mean reward 0.478, speed 74.05 f/s\n",
      "280966: done 6521 episodes, mean reward 0.481, speed 74.01 f/s\n",
      "281183: done 6522 episodes, mean reward 0.481, speed 74.68 f/s\n",
      "281273: done 6523 episodes, mean reward 0.483, speed 73.67 f/s\n",
      "281818: done 6524 episodes, mean reward 0.496, speed 73.38 f/s\n",
      "282174: done 6525 episodes, mean reward 0.503, speed 73.35 f/s\n",
      "282356: done 6526 episodes, mean reward 0.501, speed 72.02 f/s\n",
      "282443: done 6527 episodes, mean reward 0.499, speed 70.35 f/s\n",
      "282599: done 6529 episodes, mean reward 0.490, speed 73.75 f/s\n",
      "282767: done 6530 episodes, mean reward 0.491, speed 71.17 f/s\n",
      "283043: done 6531 episodes, mean reward 0.493, speed 74.19 f/s\n",
      "283133: done 6532 episodes, mean reward 0.491, speed 72.91 f/s\n",
      "283223: done 6533 episodes, mean reward 0.492, speed 74.83 f/s\n",
      "283521: done 6534 episodes, mean reward 0.492, speed 72.71 f/s\n",
      "283773: done 6536 episodes, mean reward 0.472, speed 71.87 f/s\n",
      "283876: done 6537 episodes, mean reward 0.471, speed 75.21 f/s\n",
      "283996: done 6539 episodes, mean reward 0.471, speed 74.50 f/s\n",
      "284161: done 6540 episodes, mean reward 0.470, speed 75.67 f/s\n",
      "284547: done 6541 episodes, mean reward 0.478, speed 72.79 f/s\n",
      "284718: done 6543 episodes, mean reward 0.477, speed 72.73 f/s\n",
      "284959: done 6544 episodes, mean reward 0.481, speed 73.51 f/s\n",
      "285205: done 6546 episodes, mean reward 0.485, speed 71.36 f/s\n",
      "285345: done 6548 episodes, mean reward 0.454, speed 73.40 f/s\n",
      "285492: done 6549 episodes, mean reward 0.449, speed 72.53 f/s\n",
      "285657: done 6550 episodes, mean reward 0.442, speed 71.48 f/s\n",
      "285936: done 6551 episodes, mean reward 0.446, speed 71.69 f/s\n",
      "286146: done 6553 episodes, mean reward 0.448, speed 69.80 f/s\n",
      "286553: done 6554 episodes, mean reward 0.457, speed 72.83 f/s\n",
      "286889: done 6555 episodes, mean reward 0.463, speed 71.49 f/s\n",
      "287265: done 6556 episodes, mean reward 0.462, speed 69.53 f/s\n",
      "287864: done 6558 episodes, mean reward 0.473, speed 72.77 f/s\n",
      "288258: done 6559 episodes, mean reward 0.471, speed 73.04 f/s\n",
      "288379: done 6561 episodes, mean reward 0.463, speed 71.23 f/s\n",
      "288523: done 6562 episodes, mean reward 0.461, speed 72.69 f/s\n",
      "289004: done 6564 episodes, mean reward 0.464, speed 71.23 f/s\n",
      "289147: done 6565 episodes, mean reward 0.465, speed 72.53 f/s\n",
      "289255: done 6566 episodes, mean reward 0.462, speed 71.70 f/s\n",
      "289694: done 6568 episodes, mean reward 0.463, speed 71.87 f/s\n",
      "Test done in 29.26 sec, reward 1.091, steps 428\n",
      "290071: done 6569 episodes, mean reward 0.473, speed 10.92 f/s\n",
      "290214: done 6570 episodes, mean reward 0.474, speed 70.33 f/s\n",
      "291158: done 6571 episodes, mean reward 0.497, speed 72.29 f/s\n",
      "291284: done 6572 episodes, mean reward 0.499, speed 74.61 f/s\n",
      "291409: done 6573 episodes, mean reward 0.499, speed 72.19 f/s\n",
      "291534: done 6574 episodes, mean reward 0.493, speed 70.28 f/s\n",
      "291643: done 6575 episodes, mean reward 0.492, speed 73.59 f/s\n",
      "291759: done 6577 episodes, mean reward 0.479, speed 73.23 f/s\n",
      "291943: done 6578 episodes, mean reward 0.482, speed 72.98 f/s\n",
      "292052: done 6579 episodes, mean reward 0.474, speed 70.91 f/s\n",
      "292314: done 6580 episodes, mean reward 0.479, speed 72.34 f/s\n",
      "292777: done 6581 episodes, mean reward 0.484, speed 71.76 f/s\n",
      "292873: done 6584 episodes, mean reward 0.470, speed 73.40 f/s\n",
      "293020: done 6585 episodes, mean reward 0.468, speed 75.89 f/s\n",
      "293413: done 6586 episodes, mean reward 0.474, speed 71.44 f/s\n",
      "293496: done 6589 episodes, mean reward 0.457, speed 72.71 f/s\n",
      "293633: done 6590 episodes, mean reward 0.459, speed 73.07 f/s\n",
      "293775: done 6591 episodes, mean reward 0.455, speed 71.75 f/s\n",
      "294252: done 6593 episodes, mean reward 0.445, speed 72.54 f/s\n",
      "294359: done 6594 episodes, mean reward 0.447, speed 72.66 f/s\n",
      "294593: done 6596 episodes, mean reward 0.436, speed 73.41 f/s\n",
      "294715: done 6597 episodes, mean reward 0.439, speed 70.55 f/s\n",
      "294942: done 6599 episodes, mean reward 0.438, speed 72.12 f/s\n",
      "295129: done 6600 episodes, mean reward 0.441, speed 75.51 f/s\n",
      "295276: done 6601 episodes, mean reward 0.440, speed 73.20 f/s\n",
      "295356: done 6603 episodes, mean reward 0.439, speed 71.30 f/s\n",
      "295460: done 6604 episodes, mean reward 0.438, speed 72.40 f/s\n",
      "295742: done 6606 episodes, mean reward 0.409, speed 71.80 f/s\n",
      "295846: done 6607 episodes, mean reward 0.410, speed 72.67 f/s\n",
      "295941: done 6608 episodes, mean reward 0.395, speed 70.12 f/s\n",
      "296068: done 6609 episodes, mean reward 0.390, speed 71.07 f/s\n",
      "296251: done 6610 episodes, mean reward 0.394, speed 72.24 f/s\n",
      "296525: done 6611 episodes, mean reward 0.400, speed 71.28 f/s\n",
      "296669: done 6612 episodes, mean reward 0.402, speed 72.93 f/s\n",
      "296955: done 6614 episodes, mean reward 0.404, speed 70.94 f/s\n",
      "297292: done 6615 episodes, mean reward 0.411, speed 74.13 f/s\n",
      "297472: done 6616 episodes, mean reward 0.412, speed 70.85 f/s\n",
      "297595: done 6617 episodes, mean reward 0.413, speed 72.13 f/s\n",
      "298304: done 6618 episodes, mean reward 0.428, speed 71.86 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299012: done 6619 episodes, mean reward 0.442, speed 71.44 f/s\n",
      "299114: done 6620 episodes, mean reward 0.441, speed 71.83 f/s\n",
      "299232: done 6622 episodes, mean reward 0.433, speed 73.09 f/s\n",
      "299601: done 6623 episodes, mean reward 0.440, speed 69.39 f/s\n",
      "299852: done 6624 episodes, mean reward 0.432, speed 69.73 f/s\n",
      "Test done in 24.53 sec, reward 0.907, steps 357\n",
      "300056: done 6625 episodes, mean reward 0.428, speed 7.44 f/s\n",
      "300314: done 6626 episodes, mean reward 0.430, speed 71.67 f/s\n",
      "300490: done 6627 episodes, mean reward 0.433, speed 72.49 f/s\n",
      "300689: done 6628 episodes, mean reward 0.438, speed 70.40 f/s\n",
      "301446: done 6630 episodes, mean reward 0.450, speed 70.57 f/s\n",
      "301561: done 6631 episodes, mean reward 0.445, speed 72.61 f/s\n",
      "301643: done 6632 episodes, mean reward 0.445, speed 69.73 f/s\n",
      "301715: done 6633 episodes, mean reward 0.445, speed 71.26 f/s\n",
      "301957: done 6634 episodes, mean reward 0.443, speed 71.85 f/s\n",
      "302175: done 6635 episodes, mean reward 0.448, speed 72.01 f/s\n",
      "302410: done 6636 episodes, mean reward 0.448, speed 72.49 f/s\n",
      "302974: done 6637 episodes, mean reward 0.460, speed 72.44 f/s\n",
      "303061: done 6638 episodes, mean reward 0.462, speed 70.61 f/s\n",
      "303148: done 6639 episodes, mean reward 0.461, speed 71.22 f/s\n",
      "303311: done 6640 episodes, mean reward 0.461, speed 70.95 f/s\n",
      "303452: done 6641 episodes, mean reward 0.455, speed 73.00 f/s\n",
      "303673: done 6642 episodes, mean reward 0.460, speed 71.17 f/s\n",
      "303969: done 6643 episodes, mean reward 0.464, speed 71.92 f/s\n",
      "304050: done 6645 episodes, mean reward 0.459, speed 74.43 f/s\n",
      "304591: done 6646 episodes, mean reward 0.468, speed 73.14 f/s\n",
      "304863: done 6648 episodes, mean reward 0.471, speed 73.82 f/s\n",
      "305083: done 6649 episodes, mean reward 0.473, speed 72.27 f/s\n",
      "305170: done 6650 episodes, mean reward 0.471, speed 72.57 f/s\n",
      "305293: done 6651 episodes, mean reward 0.467, speed 76.57 f/s\n",
      "305699: done 6652 episodes, mean reward 0.476, speed 70.87 f/s\n",
      "305900: done 6653 episodes, mean reward 0.477, speed 73.33 f/s\n",
      "306097: done 6654 episodes, mean reward 0.472, speed 73.47 f/s\n",
      "306187: done 6655 episodes, mean reward 0.466, speed 71.44 f/s\n",
      "306402: done 6657 episodes, mean reward 0.460, speed 72.60 f/s\n",
      "306551: done 6658 episodes, mean reward 0.449, speed 71.05 f/s\n",
      "306793: done 6659 episodes, mean reward 0.445, speed 72.98 f/s\n",
      "307100: done 6660 episodes, mean reward 0.451, speed 73.15 f/s\n",
      "307360: done 6661 episodes, mean reward 0.456, speed 72.63 f/s\n",
      "307560: done 6662 episodes, mean reward 0.457, speed 73.70 f/s\n",
      "307864: done 6665 episodes, mean reward 0.448, speed 72.83 f/s\n",
      "307978: done 6667 episodes, mean reward 0.448, speed 74.19 f/s\n",
      "308060: done 6669 episodes, mean reward 0.429, speed 72.89 f/s\n",
      "308180: done 6670 episodes, mean reward 0.428, speed 73.81 f/s\n",
      "309225: done 6672 episodes, mean reward 0.427, speed 71.96 f/s\n",
      "309533: done 6673 episodes, mean reward 0.432, speed 71.13 f/s\n",
      "309641: done 6675 episodes, mean reward 0.428, speed 76.00 f/s\n",
      "Test done in 32.96 sec, reward 1.241, steps 482\n",
      "310315: done 6677 episodes, mean reward 0.443, speed 15.91 f/s\n",
      "310492: done 6678 episodes, mean reward 0.443, speed 72.95 f/s\n",
      "310616: done 6680 episodes, mean reward 0.436, speed 73.82 f/s\n",
      "310817: done 6681 episodes, mean reward 0.430, speed 72.44 f/s\n",
      "310961: done 6682 episodes, mean reward 0.434, speed 73.63 f/s\n",
      "311141: done 6684 episodes, mean reward 0.436, speed 72.30 f/s\n",
      "311357: done 6685 episodes, mean reward 0.438, speed 72.01 f/s\n",
      "311668: done 6686 episodes, mean reward 0.435, speed 73.51 f/s\n",
      "311845: done 6687 episodes, mean reward 0.440, speed 73.32 f/s\n",
      "312296: done 6688 episodes, mean reward 0.451, speed 72.04 f/s\n",
      "312393: done 6691 episodes, mean reward 0.445, speed 73.91 f/s\n",
      "312495: done 6692 episodes, mean reward 0.447, speed 75.07 f/s\n",
      "312615: done 6693 episodes, mean reward 0.439, speed 72.80 f/s\n",
      "312912: done 6694 episodes, mean reward 0.444, speed 73.55 f/s\n",
      "313913: done 6695 episodes, mean reward 0.470, speed 47.69 f/s\n",
      "314187: done 6697 episodes, mean reward 0.468, speed 66.74 f/s\n",
      "314448: done 6698 episodes, mean reward 0.474, speed 69.13 f/s\n",
      "315224: done 6700 episodes, mean reward 0.484, speed 67.22 f/s\n",
      "315316: done 6703 episodes, mean reward 0.481, speed 65.20 f/s\n",
      "315395: done 6704 episodes, mean reward 0.480, speed 66.92 f/s\n",
      "315486: done 6705 episodes, mean reward 0.481, speed 72.03 f/s\n",
      "315691: done 6707 episodes, mean reward 0.478, speed 68.09 f/s\n",
      "315914: done 6708 episodes, mean reward 0.482, speed 68.07 f/s\n",
      "316455: done 6710 episodes, mean reward 0.488, speed 68.80 f/s\n",
      "316743: done 6711 episodes, mean reward 0.489, speed 68.32 f/s\n",
      "316904: done 6712 episodes, mean reward 0.490, speed 70.12 f/s\n",
      "317373: done 6713 episodes, mean reward 0.501, speed 69.35 f/s\n",
      "318434: done 6715 episodes, mean reward 0.513, speed 68.01 f/s\n",
      "318549: done 6716 episodes, mean reward 0.511, speed 70.53 f/s\n",
      "318675: done 6717 episodes, mean reward 0.511, speed 70.75 f/s\n",
      "318864: done 6718 episodes, mean reward 0.497, speed 68.23 f/s\n",
      "319103: done 6719 episodes, mean reward 0.484, speed 68.03 f/s\n",
      "319207: done 6720 episodes, mean reward 0.484, speed 67.05 f/s\n",
      "319496: done 6722 episodes, mean reward 0.489, speed 71.34 f/s\n",
      "319619: done 6723 episodes, mean reward 0.483, speed 69.60 f/s\n",
      "319737: done 6725 episodes, mean reward 0.474, speed 70.36 f/s\n",
      "319952: done 6726 episodes, mean reward 0.473, speed 68.39 f/s\n",
      "Test done in 70.38 sec, reward 2.256, steps 859\n",
      "320039: done 6727 episodes, mean reward 0.470, speed 1.21 f/s\n",
      "320262: done 6728 episodes, mean reward 0.471, speed 67.11 f/s\n",
      "320437: done 6730 episodes, mean reward 0.456, speed 68.51 f/s\n",
      "320546: done 6731 episodes, mean reward 0.456, speed 70.37 f/s\n",
      "320938: done 6732 episodes, mean reward 0.464, speed 68.79 f/s\n",
      "321198: done 6733 episodes, mean reward 0.469, speed 68.44 f/s\n",
      "321321: done 6734 episodes, mean reward 0.466, speed 69.73 f/s\n",
      "321857: done 6736 episodes, mean reward 0.469, speed 69.48 f/s\n",
      "322267: done 6737 episodes, mean reward 0.465, speed 66.00 f/s\n",
      "322388: done 6738 episodes, mean reward 0.466, speed 64.79 f/s\n",
      "322603: done 6740 episodes, mean reward 0.465, speed 64.52 f/s\n",
      "322706: done 6741 episodes, mean reward 0.464, speed 67.38 f/s\n",
      "322881: done 6742 episodes, mean reward 0.463, speed 69.84 f/s\n",
      "323002: done 6745 episodes, mean reward 0.455, speed 67.88 f/s\n",
      "323068: done 6747 episodes, mean reward 0.441, speed 65.26 f/s\n",
      "323137: done 6749 episodes, mean reward 0.432, speed 67.24 f/s\n",
      "323259: done 6750 episodes, mean reward 0.433, speed 66.91 f/s\n",
      "323610: done 6751 episodes, mean reward 0.439, speed 69.55 f/s\n",
      "323749: done 6752 episodes, mean reward 0.432, speed 70.31 f/s\n",
      "324196: done 6753 episodes, mean reward 0.438, speed 69.25 f/s\n",
      "324968: done 6754 episodes, mean reward 0.453, speed 67.87 f/s\n",
      "325054: done 6755 episodes, mean reward 0.453, speed 69.60 f/s\n",
      "325652: done 6756 episodes, mean reward 0.468, speed 69.75 f/s\n",
      "326558: done 6757 episodes, mean reward 0.488, speed 68.76 f/s\n",
      "326927: done 6758 episodes, mean reward 0.494, speed 67.14 f/s\n",
      "327189: done 6759 episodes, mean reward 0.495, speed 68.11 f/s\n",
      "327289: done 6761 episodes, mean reward 0.482, speed 68.88 f/s\n",
      "327384: done 6762 episodes, mean reward 0.480, speed 66.69 f/s\n",
      "327587: done 6763 episodes, mean reward 0.485, speed 70.20 f/s\n",
      "327819: done 6764 episodes, mean reward 0.490, speed 69.59 f/s\n",
      "327909: done 6765 episodes, mean reward 0.486, speed 71.26 f/s\n",
      "328174: done 6767 episodes, mean reward 0.490, speed 68.82 f/s\n",
      "328834: done 6768 episodes, mean reward 0.506, speed 69.15 f/s\n",
      "328943: done 6769 episodes, mean reward 0.508, speed 68.96 f/s\n",
      "329268: done 6770 episodes, mean reward 0.513, speed 69.70 f/s\n",
      "329531: done 6771 episodes, mean reward 0.519, speed 70.33 f/s\n",
      "329934: done 6773 episodes, mean reward 0.494, speed 68.07 f/s\n",
      "Test done in 69.75 sec, reward 2.162, steps 825\n",
      "330133: done 6775 episodes, mean reward 0.497, speed 2.74 f/s\n",
      "330299: done 6776 episodes, mean reward 0.500, speed 64.27 f/s\n",
      "330449: done 6777 episodes, mean reward 0.487, speed 70.01 f/s\n",
      "330553: done 6778 episodes, mean reward 0.485, speed 67.87 f/s\n",
      "330834: done 6779 episodes, mean reward 0.492, speed 69.85 f/s\n",
      "331018: done 6780 episodes, mean reward 0.494, speed 67.42 f/s\n",
      "331411: done 6781 episodes, mean reward 0.499, speed 65.34 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331696: done 6783 episodes, mean reward 0.501, speed 66.94 f/s\n",
      "332094: done 6785 episodes, mean reward 0.503, speed 66.52 f/s\n",
      "332483: done 6788 episodes, mean reward 0.489, speed 66.81 f/s\n",
      "333303: done 6789 episodes, mean reward 0.509, speed 67.80 f/s\n",
      "333619: done 6790 episodes, mean reward 0.517, speed 67.81 f/s\n",
      "334009: done 6791 episodes, mean reward 0.526, speed 69.74 f/s\n",
      "334308: done 6792 episodes, mean reward 0.531, speed 69.33 f/s\n",
      "334486: done 6794 episodes, mean reward 0.525, speed 70.37 f/s\n",
      "334552: done 6796 episodes, mean reward 0.498, speed 65.78 f/s\n",
      "335026: done 6798 episodes, mean reward 0.498, speed 67.29 f/s\n",
      "335672: done 6799 episodes, mean reward 0.514, speed 68.58 f/s\n",
      "336066: done 6800 episodes, mean reward 0.505, speed 67.82 f/s\n",
      "336298: done 6802 episodes, mean reward 0.509, speed 70.21 f/s\n",
      "337018: done 6803 episodes, mean reward 0.527, speed 69.36 f/s\n",
      "337591: done 6804 episodes, mean reward 0.540, speed 69.56 f/s\n",
      "337850: done 6805 episodes, mean reward 0.545, speed 70.18 f/s\n",
      "338001: done 6808 episodes, mean reward 0.537, speed 68.40 f/s\n",
      "338507: done 6809 episodes, mean reward 0.549, speed 69.89 f/s\n",
      "338607: done 6810 episodes, mean reward 0.539, speed 67.27 f/s\n",
      "338716: done 6811 episodes, mean reward 0.534, speed 69.91 f/s\n",
      "339186: done 6812 episodes, mean reward 0.542, speed 69.83 f/s\n",
      "339306: done 6814 episodes, mean reward 0.532, speed 66.83 f/s\n",
      "339499: done 6816 episodes, mean reward 0.507, speed 70.36 f/s\n",
      "339951: done 6817 episodes, mean reward 0.515, speed 69.21 f/s\n",
      "Test done in 68.89 sec, reward 2.200, steps 838\n",
      "340009: done 6818 episodes, mean reward 0.512, speed 0.83 f/s\n",
      "340209: done 6820 episodes, mean reward 0.509, speed 69.09 f/s\n",
      "340760: done 6821 episodes, mean reward 0.522, speed 68.92 f/s\n",
      "341097: done 6822 episodes, mean reward 0.524, speed 68.85 f/s\n",
      "341372: done 6823 episodes, mean reward 0.527, speed 69.99 f/s\n",
      "341444: done 6824 episodes, mean reward 0.527, speed 69.26 f/s\n",
      "341508: done 6825 episodes, mean reward 0.528, speed 63.72 f/s\n",
      "341670: done 6826 episodes, mean reward 0.526, speed 66.23 f/s\n",
      "342582: done 6827 episodes, mean reward 0.548, speed 67.88 f/s\n",
      "343186: done 6828 episodes, mean reward 0.558, speed 68.81 f/s\n",
      "343655: done 6829 episodes, mean reward 0.568, speed 69.47 f/s\n",
      "343721: done 6831 episodes, mean reward 0.564, speed 65.71 f/s\n",
      "343993: done 6833 episodes, mean reward 0.554, speed 69.83 f/s\n",
      "344222: done 6835 episodes, mean reward 0.556, speed 70.79 f/s\n",
      "344779: done 6836 episodes, mean reward 0.557, speed 70.28 f/s\n",
      "344849: done 6837 episodes, mean reward 0.548, speed 62.34 f/s\n",
      "345850: done 6838 episodes, mean reward 0.571, speed 66.99 f/s\n",
      "346182: done 6840 episodes, mean reward 0.575, speed 67.48 f/s\n",
      "346624: done 6841 episodes, mean reward 0.583, speed 67.68 f/s\n",
      "347211: done 6842 episodes, mean reward 0.594, speed 65.88 f/s\n",
      "347395: done 6843 episodes, mean reward 0.598, speed 68.19 f/s\n",
      "347485: done 6844 episodes, mean reward 0.600, speed 69.92 f/s\n",
      "347603: done 6846 episodes, mean reward 0.601, speed 70.97 f/s\n",
      "347703: done 6847 episodes, mean reward 0.602, speed 69.74 f/s\n",
      "347967: done 6849 episodes, mean reward 0.608, speed 67.54 f/s\n",
      "348095: done 6850 episodes, mean reward 0.608, speed 68.87 f/s\n",
      "348289: done 6851 episodes, mean reward 0.603, speed 68.19 f/s\n",
      "348417: done 6852 episodes, mean reward 0.603, speed 66.96 f/s\n",
      "348592: done 6853 episodes, mean reward 0.596, speed 66.90 f/s\n",
      "349156: done 6854 episodes, mean reward 0.590, speed 69.91 f/s\n",
      "349930: done 6855 episodes, mean reward 0.608, speed 70.90 f/s\n",
      "Test done in 75.25 sec, reward 2.402, steps 914\n",
      "350072: done 6856 episodes, mean reward 0.595, speed 1.84 f/s\n",
      "350236: done 6857 episodes, mean reward 0.576, speed 67.69 f/s\n",
      "350338: done 6859 episodes, mean reward 0.562, speed 68.13 f/s\n",
      "350477: done 6860 episodes, mean reward 0.565, speed 69.36 f/s\n",
      "350601: done 6861 episodes, mean reward 0.566, speed 67.15 f/s\n",
      "351116: done 6863 episodes, mean reward 0.572, speed 69.56 f/s\n",
      "351361: done 6864 episodes, mean reward 0.572, speed 68.59 f/s\n",
      "351526: done 6865 episodes, mean reward 0.574, speed 68.62 f/s\n",
      "351731: done 6866 episodes, mean reward 0.578, speed 68.53 f/s\n",
      "351847: done 6868 episodes, mean reward 0.559, speed 71.22 f/s\n",
      "351918: done 6870 episodes, mean reward 0.549, speed 68.56 f/s\n",
      "352319: done 6871 episodes, mean reward 0.553, speed 68.45 f/s\n",
      "352461: done 6872 episodes, mean reward 0.555, speed 66.56 f/s\n",
      "352671: done 6873 episodes, mean reward 0.552, speed 69.96 f/s\n",
      "353443: done 6874 episodes, mean reward 0.571, speed 68.83 f/s\n",
      "353949: done 6875 episodes, mean reward 0.580, speed 68.52 f/s\n",
      "354194: done 6876 episodes, mean reward 0.583, speed 68.12 f/s\n",
      "354435: done 6877 episodes, mean reward 0.585, speed 69.13 f/s\n",
      "354949: done 6878 episodes, mean reward 0.596, speed 69.03 f/s\n",
      "355172: done 6879 episodes, mean reward 0.595, speed 68.11 f/s\n",
      "355474: done 6881 episodes, mean reward 0.588, speed 69.33 f/s\n",
      "355580: done 6883 episodes, mean reward 0.584, speed 68.90 f/s\n",
      "355708: done 6884 episodes, mean reward 0.585, speed 67.92 f/s\n",
      "356356: done 6886 episodes, mean reward 0.592, speed 67.34 f/s\n",
      "356497: done 6888 episodes, mean reward 0.587, speed 68.62 f/s\n",
      "356606: done 6889 episodes, mean reward 0.568, speed 68.17 f/s\n",
      "356882: done 6890 episodes, mean reward 0.567, speed 69.05 f/s\n",
      "357883: done 6891 episodes, mean reward 0.583, speed 69.30 f/s\n",
      "358884: done 6892 episodes, mean reward 0.602, speed 68.47 f/s\n",
      "359162: done 6895 episodes, mean reward 0.605, speed 69.67 f/s\n",
      "359345: done 6896 episodes, mean reward 0.608, speed 70.18 f/s\n",
      "359473: done 6897 episodes, mean reward 0.610, speed 67.29 f/s\n",
      "359676: done 6898 episodes, mean reward 0.604, speed 68.40 f/s\n",
      "359856: done 6899 episodes, mean reward 0.592, speed 70.07 f/s\n",
      "359999: done 6900 episodes, mean reward 0.585, speed 69.49 f/s\n",
      "Test done in 69.77 sec, reward 2.204, steps 841\n",
      "360885: done 6901 episodes, mean reward 0.607, speed 10.71 f/s\n",
      "361590: done 6903 episodes, mean reward 0.601, speed 68.74 f/s\n",
      "361684: done 6905 episodes, mean reward 0.582, speed 66.32 f/s\n",
      "361972: done 6907 episodes, mean reward 0.589, speed 69.92 f/s\n",
      "362118: done 6909 episodes, mean reward 0.576, speed 67.36 f/s\n",
      "362208: done 6910 episodes, mean reward 0.576, speed 69.21 f/s\n",
      "362501: done 6911 episodes, mean reward 0.581, speed 68.81 f/s\n",
      "362969: done 6913 episodes, mean reward 0.580, speed 68.48 f/s\n",
      "363172: done 6914 episodes, mean reward 0.583, speed 70.81 f/s\n",
      "363679: done 6915 episodes, mean reward 0.596, speed 68.23 f/s\n",
      "363938: done 6916 episodes, mean reward 0.598, speed 70.62 f/s\n",
      "364247: done 6918 episodes, mean reward 0.593, speed 67.77 f/s\n",
      "364655: done 6919 episodes, mean reward 0.603, speed 69.66 f/s\n",
      "364964: done 6920 episodes, mean reward 0.607, speed 69.22 f/s\n",
      "365731: done 6922 episodes, mean reward 0.604, speed 69.30 f/s\n",
      "365860: done 6924 episodes, mean reward 0.599, speed 68.39 f/s\n",
      "366083: done 6925 episodes, mean reward 0.603, speed 67.76 f/s\n",
      "366354: done 6927 episodes, mean reward 0.581, speed 69.30 f/s\n",
      "366578: done 6928 episodes, mean reward 0.571, speed 69.46 f/s\n",
      "366898: done 6929 episodes, mean reward 0.567, speed 67.68 f/s\n",
      "367143: done 6931 episodes, mean reward 0.572, speed 67.20 f/s\n",
      "367245: done 6932 episodes, mean reward 0.573, speed 67.46 f/s\n",
      "367885: done 6933 episodes, mean reward 0.584, speed 68.53 f/s\n",
      "368102: done 6935 episodes, mean reward 0.583, speed 68.05 f/s\n",
      "368291: done 6936 episodes, mean reward 0.573, speed 70.88 f/s\n",
      "368927: done 6938 episodes, mean reward 0.561, speed 68.50 f/s\n",
      "369252: done 6940 episodes, mean reward 0.561, speed 68.38 f/s\n",
      "369886: done 6941 episodes, mean reward 0.566, speed 68.77 f/s\n",
      "Test done in 82.76 sec, reward 2.634, steps 1001\n",
      "Best reward updated: 2.511 -> 2.634\n",
      "370481: done 6942 episodes, mean reward 0.566, speed 6.50 f/s\n",
      "371124: done 6943 episodes, mean reward 0.578, speed 68.65 f/s\n",
      "371991: done 6944 episodes, mean reward 0.599, speed 69.82 f/s\n",
      "372403: done 6945 episodes, mean reward 0.608, speed 69.01 f/s\n",
      "372911: done 6946 episodes, mean reward 0.620, speed 70.41 f/s\n",
      "373361: done 6947 episodes, mean reward 0.629, speed 69.21 f/s\n",
      "373450: done 6948 episodes, mean reward 0.630, speed 67.97 f/s\n",
      "373852: done 6949 episodes, mean reward 0.634, speed 68.79 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373955: done 6950 episodes, mean reward 0.633, speed 68.02 f/s\n",
      "374121: done 6951 episodes, mean reward 0.633, speed 70.43 f/s\n",
      "374456: done 6952 episodes, mean reward 0.638, speed 70.15 f/s\n",
      "374557: done 6954 episodes, mean reward 0.622, speed 67.17 f/s\n",
      "374801: done 6956 episodes, mean reward 0.605, speed 68.62 f/s\n",
      "375460: done 6957 episodes, mean reward 0.618, speed 68.39 f/s\n",
      "375561: done 6958 episodes, mean reward 0.619, speed 70.69 f/s\n",
      "376562: done 6959 episodes, mean reward 0.644, speed 68.64 f/s\n",
      "377018: done 6960 episodes, mean reward 0.653, speed 68.72 f/s\n",
      "377314: done 6961 episodes, mean reward 0.657, speed 69.21 f/s\n",
      "377474: done 6962 episodes, mean reward 0.661, speed 68.47 f/s\n",
      "378180: done 6964 episodes, mean reward 0.661, speed 69.19 f/s\n",
      "378620: done 6965 episodes, mean reward 0.668, speed 68.54 f/s\n",
      "378834: done 6966 episodes, mean reward 0.668, speed 71.36 f/s\n",
      "379563: done 6968 episodes, mean reward 0.684, speed 69.42 f/s\n",
      "379704: done 6969 episodes, mean reward 0.686, speed 68.02 f/s\n",
      "379850: done 6972 episodes, mean reward 0.676, speed 68.74 f/s\n",
      "379969: done 6973 episodes, mean reward 0.674, speed 68.73 f/s\n",
      "Test done in 82.78 sec, reward 2.633, steps 1001\n",
      "380723: done 6974 episodes, mean reward 0.673, speed 8.03 f/s\n",
      "380845: done 6975 episodes, mean reward 0.663, speed 66.98 f/s\n",
      "380982: done 6976 episodes, mean reward 0.660, speed 70.55 f/s\n",
      "381166: done 6977 episodes, mean reward 0.658, speed 68.71 f/s\n",
      "381420: done 6978 episodes, mean reward 0.651, speed 68.81 f/s\n",
      "381533: done 6980 episodes, mean reward 0.647, speed 69.56 f/s\n",
      "381767: done 6981 episodes, mean reward 0.647, speed 69.50 f/s\n",
      "382047: done 6982 episodes, mean reward 0.653, speed 68.12 f/s\n",
      "382707: done 6984 episodes, mean reward 0.666, speed 67.92 f/s\n",
      "382796: done 6985 episodes, mean reward 0.667, speed 67.44 f/s\n",
      "383038: done 6986 episodes, mean reward 0.658, speed 70.69 f/s\n",
      "383242: done 6987 episodes, mean reward 0.661, speed 68.90 f/s\n",
      "383541: done 6988 episodes, mean reward 0.667, speed 69.06 f/s\n",
      "383659: done 6989 episodes, mean reward 0.668, speed 67.86 f/s\n",
      "383900: done 6991 episodes, mean reward 0.640, speed 68.82 f/s\n",
      "384253: done 6994 episodes, mean reward 0.620, speed 67.32 f/s\n",
      "384381: done 6995 episodes, mean reward 0.618, speed 70.43 f/s\n",
      "384945: done 6996 episodes, mean reward 0.628, speed 70.56 f/s\n",
      "385913: done 6997 episodes, mean reward 0.650, speed 70.32 f/s\n",
      "386354: done 6999 episodes, mean reward 0.652, speed 68.56 f/s\n",
      "386444: done 7000 episodes, mean reward 0.651, speed 71.22 f/s\n",
      "386661: done 7001 episodes, mean reward 0.634, speed 68.69 f/s\n",
      "386759: done 7003 episodes, mean reward 0.618, speed 70.34 f/s\n",
      "386848: done 7004 episodes, mean reward 0.619, speed 70.16 f/s\n",
      "387444: done 7005 episodes, mean reward 0.634, speed 69.37 f/s\n",
      "387682: done 7006 episodes, mean reward 0.639, speed 69.50 f/s\n",
      "387872: done 7007 episodes, mean reward 0.638, speed 70.57 f/s\n",
      "388114: done 7008 episodes, mean reward 0.643, speed 68.69 f/s\n",
      "388659: done 7009 episodes, mean reward 0.655, speed 69.14 f/s\n",
      "388844: done 7010 episodes, mean reward 0.658, speed 67.82 f/s\n",
      "388953: done 7011 episodes, mean reward 0.653, speed 69.81 f/s\n",
      "389360: done 7013 episodes, mean reward 0.651, speed 68.63 f/s\n",
      "389583: done 7014 episodes, mean reward 0.651, speed 70.71 f/s\n",
      "389678: done 7016 episodes, mean reward 0.633, speed 69.18 f/s\n",
      "389752: done 7018 episodes, mean reward 0.627, speed 67.42 f/s\n",
      "Test done in 75.09 sec, reward 2.387, steps 910\n",
      "390921: done 7020 episodes, mean reward 0.638, speed 12.72 f/s\n",
      "391056: done 7022 episodes, mean reward 0.621, speed 70.75 f/s\n",
      "391181: done 7023 episodes, mean reward 0.624, speed 68.17 f/s\n",
      "391659: done 7024 episodes, mean reward 0.634, speed 68.40 f/s\n",
      "392126: done 7025 episodes, mean reward 0.641, speed 70.07 f/s\n",
      "392401: done 7026 episodes, mean reward 0.647, speed 68.87 f/s\n",
      "392915: done 7028 episodes, mean reward 0.648, speed 67.65 f/s\n",
      "393003: done 7029 episodes, mean reward 0.642, speed 72.15 f/s\n",
      "393118: done 7031 episodes, mean reward 0.638, speed 70.91 f/s\n",
      "393417: done 7032 episodes, mean reward 0.644, speed 70.07 f/s\n",
      "394483: done 7034 episodes, mean reward 0.655, speed 69.23 f/s\n",
      "395191: done 7035 episodes, mean reward 0.668, speed 68.28 f/s\n",
      "395467: done 7037 episodes, mean reward 0.669, speed 68.14 f/s\n",
      "396340: done 7039 episodes, mean reward 0.677, speed 69.76 f/s\n",
      "396442: done 7040 episodes, mean reward 0.672, speed 70.26 f/s\n",
      "396584: done 7041 episodes, mean reward 0.659, speed 70.50 f/s\n",
      "396779: done 7044 episodes, mean reward 0.608, speed 66.73 f/s\n",
      "396924: done 7045 episodes, mean reward 0.601, speed 68.37 f/s\n",
      "397459: done 7046 episodes, mean reward 0.601, speed 69.01 f/s\n",
      "397680: done 7047 episodes, mean reward 0.595, speed 71.35 f/s\n",
      "397942: done 7048 episodes, mean reward 0.600, speed 68.80 f/s\n",
      "398122: done 7049 episodes, mean reward 0.595, speed 67.91 f/s\n",
      "398485: done 7051 episodes, mean reward 0.598, speed 70.85 f/s\n",
      "398632: done 7052 episodes, mean reward 0.593, speed 67.70 f/s\n",
      "398760: done 7053 episodes, mean reward 0.595, speed 69.79 f/s\n",
      "399095: done 7055 episodes, mean reward 0.601, speed 69.74 f/s\n",
      "399375: done 7056 episodes, mean reward 0.603, speed 67.83 f/s\n",
      "399537: done 7057 episodes, mean reward 0.590, speed 70.00 f/s\n",
      "399869: done 7058 episodes, mean reward 0.596, speed 68.63 f/s\n",
      "399991: done 7059 episodes, mean reward 0.573, speed 69.09 f/s\n",
      "Test done in 69.10 sec, reward 2.185, steps 835\n",
      "400556: done 7060 episodes, mean reward 0.575, speed 7.31 f/s\n",
      "401272: done 7061 episodes, mean reward 0.586, speed 68.38 f/s\n",
      "401389: done 7063 episodes, mean reward 0.584, speed 68.69 f/s\n",
      "401858: done 7064 episodes, mean reward 0.579, speed 69.34 f/s\n",
      "401943: done 7065 episodes, mean reward 0.570, speed 67.04 f/s\n",
      "402261: done 7066 episodes, mean reward 0.572, speed 68.16 f/s\n",
      "402555: done 7067 episodes, mean reward 0.579, speed 70.41 f/s\n",
      "402680: done 7068 episodes, mean reward 0.565, speed 70.40 f/s\n",
      "402903: done 7069 episodes, mean reward 0.567, speed 67.21 f/s\n",
      "403068: done 7070 episodes, mean reward 0.571, speed 68.32 f/s\n",
      "403396: done 7071 episodes, mean reward 0.579, speed 68.14 f/s\n",
      "403676: done 7072 episodes, mean reward 0.583, speed 70.18 f/s\n",
      "403933: done 7074 episodes, mean reward 0.567, speed 68.69 f/s\n",
      "404031: done 7076 episodes, mean reward 0.563, speed 66.97 f/s\n",
      "404197: done 7077 episodes, mean reward 0.562, speed 72.99 f/s\n",
      "404710: done 7079 episodes, mean reward 0.569, speed 68.69 f/s\n",
      "404784: done 7080 episodes, mean reward 0.568, speed 69.42 f/s\n",
      "404987: done 7081 episodes, mean reward 0.567, speed 67.56 f/s\n",
      "405747: done 7082 episodes, mean reward 0.580, speed 69.04 f/s\n",
      "406292: done 7083 episodes, mean reward 0.593, speed 67.79 f/s\n",
      "406554: done 7084 episodes, mean reward 0.583, speed 70.50 f/s\n",
      "406872: done 7085 episodes, mean reward 0.589, speed 68.61 f/s\n",
      "407121: done 7087 episodes, mean reward 0.584, speed 71.40 f/s\n",
      "407722: done 7088 episodes, mean reward 0.592, speed 68.86 f/s\n",
      "408626: done 7089 episodes, mean reward 0.612, speed 68.07 f/s\n",
      "408996: done 7091 episodes, mean reward 0.616, speed 68.59 f/s\n",
      "409235: done 7092 episodes, mean reward 0.622, speed 68.81 f/s\n",
      "409336: done 7093 episodes, mean reward 0.623, speed 68.31 f/s\n",
      "409525: done 7095 episodes, mean reward 0.618, speed 69.66 f/s\n",
      "Test done in 76.57 sec, reward 2.435, steps 927\n",
      "410129: done 7097 episodes, mean reward 0.593, speed 7.09 f/s\n",
      "410578: done 7098 episodes, mean reward 0.603, speed 68.94 f/s\n",
      "410720: done 7099 episodes, mean reward 0.597, speed 68.87 f/s\n",
      "410918: done 7100 episodes, mean reward 0.600, speed 69.72 f/s\n",
      "411242: done 7101 episodes, mean reward 0.603, speed 68.58 f/s\n",
      "411847: done 7103 episodes, mean reward 0.616, speed 69.46 f/s\n",
      "412600: done 7104 episodes, mean reward 0.634, speed 68.97 f/s\n",
      "412674: done 7105 episodes, mean reward 0.620, speed 68.06 f/s\n",
      "412840: done 7106 episodes, mean reward 0.618, speed 69.03 f/s\n",
      "413248: done 7107 episodes, mean reward 0.623, speed 70.76 f/s\n",
      "413405: done 7109 episodes, mean reward 0.607, speed 69.87 f/s\n",
      "413626: done 7112 episodes, mean reward 0.603, speed 70.41 f/s\n",
      "414376: done 7114 episodes, mean reward 0.608, speed 68.49 f/s\n",
      "414493: done 7116 episodes, mean reward 0.609, speed 68.74 f/s\n",
      "414581: done 7117 episodes, mean reward 0.610, speed 71.95 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414812: done 7118 episodes, mean reward 0.615, speed 69.51 f/s\n",
      "415173: done 7120 episodes, mean reward 0.594, speed 67.66 f/s\n",
      "415343: done 7122 episodes, mean reward 0.595, speed 68.18 f/s\n",
      "415508: done 7123 episodes, mean reward 0.596, speed 68.58 f/s\n",
      "416036: done 7124 episodes, mean reward 0.597, speed 71.20 f/s\n",
      "416202: done 7125 episodes, mean reward 0.589, speed 69.50 f/s\n",
      "416368: done 7126 episodes, mean reward 0.585, speed 67.39 f/s\n",
      "416568: done 7127 episodes, mean reward 0.589, speed 69.43 f/s\n",
      "416956: done 7128 episodes, mean reward 0.588, speed 69.12 f/s\n",
      "417163: done 7130 episodes, mean reward 0.590, speed 69.34 f/s\n",
      "417650: done 7131 episodes, mean reward 0.601, speed 68.44 f/s\n",
      "417892: done 7132 episodes, mean reward 0.600, speed 70.48 f/s\n",
      "418922: done 7134 episodes, mean reward 0.599, speed 69.10 f/s\n",
      "419209: done 7135 episodes, mean reward 0.589, speed 67.50 f/s\n",
      "419543: done 7138 episodes, mean reward 0.590, speed 69.40 f/s\n",
      "419728: done 7139 episodes, mean reward 0.572, speed 68.14 f/s\n",
      "419970: done 7140 episodes, mean reward 0.575, speed 68.09 f/s\n",
      "Test done in 78.73 sec, reward 2.509, steps 954\n",
      "420103: done 7141 episodes, mean reward 0.575, speed 1.65 f/s\n",
      "420316: done 7144 episodes, mean reward 0.576, speed 68.92 f/s\n",
      "420877: done 7145 episodes, mean reward 0.587, speed 68.08 f/s\n",
      "421135: done 7146 episodes, mean reward 0.580, speed 67.62 f/s\n",
      "421832: done 7147 episodes, mean reward 0.593, speed 67.60 f/s\n",
      "422555: done 7148 episodes, mean reward 0.604, speed 68.84 f/s\n",
      "423100: done 7149 episodes, mean reward 0.614, speed 69.35 f/s\n",
      "423336: done 7151 episodes, mean reward 0.610, speed 69.70 f/s\n",
      "423710: done 7152 episodes, mean reward 0.616, speed 69.84 f/s\n",
      "424062: done 7153 episodes, mean reward 0.622, speed 68.37 f/s\n",
      "424162: done 7154 episodes, mean reward 0.623, speed 71.12 f/s\n",
      "424422: done 7155 episodes, mean reward 0.623, speed 69.38 f/s\n",
      "424586: done 7156 episodes, mean reward 0.620, speed 70.45 f/s\n",
      "424792: done 7158 episodes, mean reward 0.612, speed 69.47 f/s\n",
      "425048: done 7160 episodes, mean reward 0.601, speed 69.44 f/s\n",
      "425161: done 7162 episodes, mean reward 0.583, speed 71.62 f/s\n",
      "425251: done 7163 episodes, mean reward 0.583, speed 69.74 f/s\n",
      "425340: done 7164 episodes, mean reward 0.573, speed 68.07 f/s\n",
      "425463: done 7166 episodes, mean reward 0.566, speed 68.27 f/s\n",
      "425763: done 7167 episodes, mean reward 0.566, speed 68.51 f/s\n",
      "425862: done 7168 episodes, mean reward 0.565, speed 68.22 f/s\n",
      "426083: done 7169 episodes, mean reward 0.564, speed 70.35 f/s\n",
      "426299: done 7170 episodes, mean reward 0.566, speed 68.13 f/s\n",
      "426421: done 7171 episodes, mean reward 0.560, speed 68.57 f/s\n",
      "426522: done 7173 episodes, mean reward 0.555, speed 68.92 f/s\n",
      "426655: done 7175 episodes, mean reward 0.551, speed 67.68 f/s\n",
      "426860: done 7177 episodes, mean reward 0.550, speed 69.99 f/s\n",
      "426991: done 7178 episodes, mean reward 0.552, speed 68.14 f/s\n",
      "427188: done 7179 episodes, mean reward 0.545, speed 72.56 f/s\n",
      "427409: done 7180 episodes, mean reward 0.549, speed 69.26 f/s\n",
      "427479: done 7181 episodes, mean reward 0.546, speed 69.49 f/s\n",
      "427665: done 7182 episodes, mean reward 0.531, speed 67.95 f/s\n",
      "427768: done 7183 episodes, mean reward 0.520, speed 71.70 f/s\n",
      "428169: done 7186 episodes, mean reward 0.515, speed 69.94 f/s\n",
      "428360: done 7189 episodes, mean reward 0.474, speed 69.29 f/s\n",
      "428478: done 7191 episodes, mean reward 0.468, speed 71.45 f/s\n",
      "428568: done 7192 episodes, mean reward 0.464, speed 68.15 f/s\n",
      "428685: done 7194 episodes, mean reward 0.462, speed 70.65 f/s\n",
      "428805: done 7196 episodes, mean reward 0.461, speed 66.70 f/s\n",
      "428982: done 7198 episodes, mean reward 0.439, speed 71.23 f/s\n",
      "429140: done 7199 episodes, mean reward 0.440, speed 70.77 f/s\n",
      "429268: done 7200 episodes, mean reward 0.438, speed 71.39 f/s\n",
      "429389: done 7201 episodes, mean reward 0.432, speed 68.95 f/s\n",
      "429626: done 7203 episodes, mean reward 0.422, speed 66.92 f/s\n",
      "429800: done 7205 episodes, mean reward 0.405, speed 69.65 f/s\n",
      "429884: done 7207 episodes, mean reward 0.392, speed 69.69 f/s\n",
      "Test done in 19.92 sec, reward 0.609, steps 247\n",
      "430032: done 7208 episodes, mean reward 0.394, speed 6.69 f/s\n",
      "430152: done 7210 episodes, mean reward 0.393, speed 64.64 f/s\n",
      "431153: done 7211 episodes, mean reward 0.419, speed 70.40 f/s\n",
      "431350: done 7212 episodes, mean reward 0.419, speed 71.13 f/s\n",
      "431584: done 7214 episodes, mean reward 0.406, speed 68.03 f/s\n",
      "431825: done 7215 episodes, mean reward 0.411, speed 69.97 f/s\n",
      "431924: done 7216 episodes, mean reward 0.412, speed 68.12 f/s\n",
      "432121: done 7217 episodes, mean reward 0.416, speed 67.77 f/s\n",
      "432296: done 7218 episodes, mean reward 0.414, speed 66.63 f/s\n",
      "433004: done 7219 episodes, mean reward 0.431, speed 69.46 f/s\n",
      "433222: done 7220 episodes, mean reward 0.429, speed 68.01 f/s\n",
      "433732: done 7221 episodes, mean reward 0.443, speed 69.97 f/s\n",
      "434539: done 7222 episodes, mean reward 0.460, speed 70.19 f/s\n",
      "434625: done 7223 episodes, mean reward 0.458, speed 64.83 f/s\n",
      "435135: done 7226 episodes, mean reward 0.450, speed 67.71 f/s\n",
      "435296: done 7227 episodes, mean reward 0.449, speed 71.31 f/s\n",
      "435385: done 7228 episodes, mean reward 0.441, speed 66.95 f/s\n",
      "435536: done 7230 episodes, mean reward 0.439, speed 69.43 f/s\n",
      "435693: done 7231 episodes, mean reward 0.430, speed 68.57 f/s\n",
      "436172: done 7232 episodes, mean reward 0.436, speed 70.24 f/s\n",
      "436389: done 7233 episodes, mean reward 0.441, speed 68.72 f/s\n",
      "436534: done 7234 episodes, mean reward 0.418, speed 68.61 f/s\n",
      "436770: done 7235 episodes, mean reward 0.416, speed 68.27 f/s\n",
      "437270: done 7236 episodes, mean reward 0.429, speed 69.62 f/s\n",
      "437348: done 7238 episodes, mean reward 0.423, speed 70.12 f/s\n",
      "437484: done 7239 episodes, mean reward 0.421, speed 65.83 f/s\n",
      "438151: done 7240 episodes, mean reward 0.433, speed 70.05 f/s\n",
      "438563: done 7241 episodes, mean reward 0.441, speed 69.69 f/s\n",
      "438951: done 7243 episodes, mean reward 0.450, speed 69.57 f/s\n",
      "439117: done 7244 episodes, mean reward 0.450, speed 67.62 f/s\n",
      "439312: done 7247 episodes, mean reward 0.415, speed 68.85 f/s\n",
      "439629: done 7248 episodes, mean reward 0.405, speed 69.10 f/s\n",
      "439712: done 7250 episodes, mean reward 0.392, speed 69.22 f/s\n",
      "Test done in 64.67 sec, reward 2.056, steps 786\n",
      "440156: done 7251 episodes, mean reward 0.399, speed 6.26 f/s\n",
      "440396: done 7252 episodes, mean reward 0.395, speed 67.89 f/s\n",
      "440992: done 7253 episodes, mean reward 0.401, speed 68.90 f/s\n",
      "441135: done 7255 episodes, mean reward 0.395, speed 69.50 f/s\n",
      "441418: done 7256 episodes, mean reward 0.398, speed 68.50 f/s\n",
      "441517: done 7257 episodes, mean reward 0.400, speed 71.02 f/s\n",
      "441694: done 7258 episodes, mean reward 0.400, speed 69.09 f/s\n",
      "442231: done 7260 episodes, mean reward 0.407, speed 67.44 f/s\n",
      "442772: done 7262 episodes, mean reward 0.418, speed 68.06 f/s\n",
      "443260: done 7263 episodes, mean reward 0.429, speed 68.59 f/s\n",
      "443499: done 7264 episodes, mean reward 0.433, speed 68.99 f/s\n",
      "443678: done 7266 episodes, mean reward 0.434, speed 68.08 f/s\n",
      "443988: done 7268 episodes, mean reward 0.432, speed 70.38 f/s\n",
      "444229: done 7269 episodes, mean reward 0.433, speed 68.13 f/s\n",
      "444490: done 7270 episodes, mean reward 0.434, speed 68.58 f/s\n",
      "444618: done 7271 episodes, mean reward 0.434, speed 70.24 f/s\n",
      "445070: done 7273 episodes, mean reward 0.443, speed 69.20 f/s\n",
      "445758: done 7274 episodes, mean reward 0.460, speed 68.89 f/s\n",
      "445827: done 7275 episodes, mean reward 0.460, speed 68.49 f/s\n",
      "445987: done 7276 episodes, mean reward 0.463, speed 69.45 f/s\n",
      "446495: done 7278 episodes, mean reward 0.468, speed 69.56 f/s\n",
      "446632: done 7279 episodes, mean reward 0.466, speed 68.02 f/s\n",
      "446722: done 7280 episodes, mean reward 0.463, speed 67.61 f/s\n",
      "446885: done 7281 episodes, mean reward 0.465, speed 68.91 f/s\n",
      "447060: done 7282 episodes, mean reward 0.464, speed 69.33 f/s\n",
      "447201: done 7284 episodes, mean reward 0.464, speed 70.80 f/s\n",
      "447290: done 7285 episodes, mean reward 0.465, speed 70.09 f/s\n",
      "447418: done 7286 episodes, mean reward 0.460, speed 69.19 f/s\n",
      "447579: done 7287 episodes, mean reward 0.463, speed 70.81 f/s\n",
      "447896: done 7288 episodes, mean reward 0.471, speed 68.71 f/s\n",
      "448361: done 7290 episodes, mean reward 0.477, speed 68.79 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448784: done 7291 episodes, mean reward 0.487, speed 69.18 f/s\n",
      "448872: done 7292 episodes, mean reward 0.487, speed 69.98 f/s\n",
      "448980: done 7293 episodes, mean reward 0.488, speed 69.00 f/s\n",
      "449137: done 7294 episodes, mean reward 0.491, speed 68.72 f/s\n",
      "449218: done 7296 episodes, mean reward 0.490, speed 66.26 f/s\n",
      "Test done in 78.55 sec, reward 2.506, steps 953\n",
      "450272: done 7297 episodes, mean reward 0.516, speed 11.26 f/s\n",
      "450399: done 7298 episodes, mean reward 0.517, speed 70.88 f/s\n",
      "450520: done 7299 episodes, mean reward 0.516, speed 66.02 f/s\n",
      "450985: done 7301 episodes, mean reward 0.522, speed 68.48 f/s\n",
      "451321: done 7302 episodes, mean reward 0.531, speed 68.63 f/s\n",
      "451415: done 7304 episodes, mean reward 0.526, speed 69.57 f/s\n",
      "451773: done 7307 episodes, mean reward 0.530, speed 69.33 f/s\n",
      "451886: done 7309 episodes, mean reward 0.528, speed 67.46 f/s\n",
      "452370: done 7310 episodes, mean reward 0.539, speed 69.44 f/s\n",
      "452755: done 7311 episodes, mean reward 0.522, speed 67.57 f/s\n",
      "452892: done 7312 episodes, mean reward 0.521, speed 71.02 f/s\n",
      "453065: done 7314 episodes, mean reward 0.519, speed 70.69 f/s\n",
      "453155: done 7315 episodes, mean reward 0.515, speed 69.18 f/s\n",
      "453358: done 7316 episodes, mean reward 0.517, speed 69.37 f/s\n",
      "453758: done 7317 episodes, mean reward 0.522, speed 68.99 f/s\n",
      "453896: done 7318 episodes, mean reward 0.522, speed 69.36 f/s\n",
      "454019: done 7319 episodes, mean reward 0.506, speed 69.04 f/s\n",
      "454122: done 7321 episodes, mean reward 0.489, speed 71.12 f/s\n",
      "454417: done 7322 episodes, mean reward 0.476, speed 71.52 f/s\n",
      "454606: done 7325 episodes, mean reward 0.478, speed 69.87 f/s\n",
      "454736: done 7327 episodes, mean reward 0.464, speed 68.94 f/s\n",
      "454873: done 7328 episodes, mean reward 0.465, speed 68.84 f/s\n",
      "455017: done 7329 episodes, mean reward 0.468, speed 70.29 f/s\n",
      "455153: done 7331 episodes, mean reward 0.465, speed 70.51 f/s\n",
      "455240: done 7332 episodes, mean reward 0.455, speed 69.44 f/s\n",
      "455439: done 7333 episodes, mean reward 0.455, speed 69.25 f/s\n",
      "455526: done 7334 episodes, mean reward 0.454, speed 70.88 f/s\n",
      "456258: done 7335 episodes, mean reward 0.467, speed 69.12 f/s\n",
      "457178: done 7336 episodes, mean reward 0.478, speed 69.56 f/s\n",
      "457344: done 7337 episodes, mean reward 0.481, speed 67.26 f/s\n",
      "457543: done 7339 episodes, mean reward 0.482, speed 69.54 f/s\n",
      "457747: done 7340 episodes, mean reward 0.470, speed 67.88 f/s\n",
      "457913: done 7341 episodes, mean reward 0.463, speed 69.15 f/s\n",
      "457992: done 7342 episodes, mean reward 0.465, speed 69.06 f/s\n",
      "458461: done 7343 episodes, mean reward 0.467, speed 69.67 f/s\n",
      "458931: done 7344 episodes, mean reward 0.475, speed 68.83 f/s\n",
      "459706: done 7346 episodes, mean reward 0.494, speed 68.46 f/s\n",
      "Test done in 61.13 sec, reward 1.940, steps 742\n",
      "460065: done 7347 episodes, mean reward 0.499, speed 5.41 f/s\n",
      "460148: done 7349 episodes, mean reward 0.492, speed 69.52 f/s\n",
      "460365: done 7350 episodes, mean reward 0.497, speed 69.01 f/s\n",
      "460463: done 7353 episodes, mean reward 0.466, speed 69.42 f/s\n",
      "460591: done 7354 episodes, mean reward 0.469, speed 67.24 f/s\n",
      "461612: done 7356 episodes, mean reward 0.484, speed 68.18 f/s\n",
      "461758: done 7357 episodes, mean reward 0.486, speed 69.74 f/s\n",
      "462152: done 7358 episodes, mean reward 0.492, speed 71.57 f/s\n",
      "462384: done 7360 episodes, mean reward 0.484, speed 69.40 f/s\n",
      "462612: done 7361 episodes, mean reward 0.490, speed 69.04 f/s\n",
      "462902: done 7363 episodes, mean reward 0.471, speed 68.87 f/s\n",
      "463289: done 7365 episodes, mean reward 0.474, speed 68.27 f/s\n",
      "464290: done 7366 episodes, mean reward 0.496, speed 69.72 f/s\n",
      "464579: done 7368 episodes, mean reward 0.496, speed 69.02 f/s\n",
      "464972: done 7369 episodes, mean reward 0.500, speed 67.94 f/s\n",
      "465064: done 7370 episodes, mean reward 0.496, speed 69.75 f/s\n",
      "465229: done 7371 episodes, mean reward 0.497, speed 70.97 f/s\n",
      "465568: done 7373 episodes, mean reward 0.493, speed 68.42 f/s\n",
      "465922: done 7374 episodes, mean reward 0.484, speed 68.98 f/s\n",
      "466060: done 7375 episodes, mean reward 0.486, speed 68.22 f/s\n",
      "466245: done 7376 episodes, mean reward 0.487, speed 68.68 f/s\n",
      "466335: done 7377 episodes, mean reward 0.488, speed 68.05 f/s\n",
      "466510: done 7379 episodes, mean reward 0.478, speed 68.93 f/s\n",
      "466688: done 7380 episodes, mean reward 0.481, speed 70.74 f/s\n",
      "466873: done 7382 episodes, mean reward 0.477, speed 68.74 f/s\n",
      "467154: done 7384 episodes, mean reward 0.481, speed 68.47 f/s\n",
      "467682: done 7387 episodes, mean reward 0.486, speed 68.55 f/s\n",
      "467763: done 7389 episodes, mean reward 0.479, speed 67.16 f/s\n",
      "468004: done 7390 episodes, mean reward 0.474, speed 69.34 f/s\n",
      "468217: done 7391 episodes, mean reward 0.468, speed 69.24 f/s\n",
      "468434: done 7392 episodes, mean reward 0.471, speed 67.93 f/s\n",
      "468536: done 7393 episodes, mean reward 0.471, speed 69.91 f/s\n",
      "469138: done 7394 episodes, mean reward 0.483, speed 69.14 f/s\n",
      "469266: done 7395 episodes, mean reward 0.485, speed 67.47 f/s\n",
      "469540: done 7396 episodes, mean reward 0.491, speed 70.15 f/s\n",
      "469718: done 7397 episodes, mean reward 0.468, speed 70.03 f/s\n",
      "469880: done 7399 episodes, mean reward 0.466, speed 73.61 f/s\n",
      "Test done in 71.21 sec, reward 2.261, steps 861\n",
      "470127: done 7401 episodes, mean reward 0.460, speed 3.30 f/s\n",
      "470852: done 7403 episodes, mean reward 0.470, speed 67.87 f/s\n",
      "471098: done 7405 episodes, mean reward 0.474, speed 69.03 f/s\n",
      "471367: done 7406 episodes, mean reward 0.480, speed 72.11 f/s\n",
      "471546: done 7407 episodes, mean reward 0.476, speed 67.39 f/s\n",
      "471649: done 7408 episodes, mean reward 0.479, speed 68.81 f/s\n",
      "471787: done 7409 episodes, mean reward 0.480, speed 68.70 f/s\n",
      "471907: done 7411 episodes, mean reward 0.460, speed 67.86 f/s\n",
      "472313: done 7412 episodes, mean reward 0.468, speed 68.69 f/s\n",
      "472466: done 7414 episodes, mean reward 0.467, speed 68.56 f/s\n",
      "472906: done 7415 episodes, mean reward 0.476, speed 69.60 f/s\n",
      "473907: done 7416 episodes, mean reward 0.497, speed 70.22 f/s\n",
      "473982: done 7417 episodes, mean reward 0.489, speed 72.22 f/s\n",
      "474237: done 7418 episodes, mean reward 0.491, speed 71.08 f/s\n",
      "474588: done 7419 episodes, mean reward 0.498, speed 69.16 f/s\n",
      "475589: done 7420 episodes, mean reward 0.522, speed 69.32 f/s\n",
      "475885: done 7421 episodes, mean reward 0.529, speed 68.74 f/s\n",
      "476183: done 7422 episodes, mean reward 0.529, speed 67.91 f/s\n",
      "476311: done 7423 episodes, mean reward 0.531, speed 69.12 f/s\n",
      "476471: done 7425 episodes, mean reward 0.531, speed 67.46 f/s\n",
      "476577: done 7426 episodes, mean reward 0.533, speed 70.79 f/s\n",
      "476947: done 7427 episodes, mean reward 0.541, speed 68.27 f/s\n",
      "477108: done 7429 episodes, mean reward 0.538, speed 69.85 f/s\n",
      "477363: done 7430 episodes, mean reward 0.544, speed 67.81 f/s\n",
      "477467: done 7431 episodes, mean reward 0.544, speed 67.42 f/s\n",
      "477615: done 7433 episodes, mean reward 0.540, speed 70.12 f/s\n",
      "478306: done 7434 episodes, mean reward 0.556, speed 68.13 f/s\n",
      "478434: done 7435 episodes, mean reward 0.540, speed 67.72 f/s\n",
      "478512: done 7438 episodes, mean reward 0.512, speed 70.33 f/s\n",
      "478602: done 7439 episodes, mean reward 0.511, speed 68.34 f/s\n",
      "478805: done 7440 episodes, mean reward 0.511, speed 70.46 f/s\n",
      "478886: done 7442 episodes, mean reward 0.507, speed 67.99 f/s\n",
      "479022: done 7445 episodes, mean reward 0.484, speed 66.82 f/s\n",
      "479264: done 7446 episodes, mean reward 0.472, speed 69.74 f/s\n",
      "479371: done 7447 episodes, mean reward 0.466, speed 67.08 f/s\n",
      "479685: done 7448 episodes, mean reward 0.472, speed 68.77 f/s\n",
      "479868: done 7449 episodes, mean reward 0.477, speed 68.98 f/s\n",
      "Test done in 59.72 sec, reward 1.891, steps 725\n",
      "480065: done 7450 episodes, mean reward 0.476, speed 3.15 f/s\n",
      "480306: done 7451 episodes, mean reward 0.482, speed 68.61 f/s\n",
      "480424: done 7453 episodes, mean reward 0.484, speed 70.76 f/s\n",
      "480532: done 7454 episodes, mean reward 0.483, speed 69.63 f/s\n",
      "480728: done 7455 episodes, mean reward 0.487, speed 68.78 f/s\n",
      "480845: done 7456 episodes, mean reward 0.465, speed 67.20 f/s\n",
      "480948: done 7457 episodes, mean reward 0.464, speed 66.39 f/s\n",
      "481017: done 7458 episodes, mean reward 0.456, speed 68.13 f/s\n",
      "481177: done 7459 episodes, mean reward 0.460, speed 69.87 f/s\n",
      "481296: done 7460 episodes, mean reward 0.457, speed 67.43 f/s\n",
      "481510: done 7461 episodes, mean reward 0.457, speed 69.33 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482311: done 7462 episodes, mean reward 0.476, speed 71.78 f/s\n",
      "482473: done 7464 episodes, mean reward 0.474, speed 70.08 f/s\n",
      "483119: done 7466 episodes, mean reward 0.455, speed 68.96 f/s\n",
      "483361: done 7467 episodes, mean reward 0.460, speed 69.27 f/s\n",
      "483620: done 7468 episodes, mean reward 0.460, speed 68.78 f/s\n",
      "483975: done 7469 episodes, mean reward 0.459, speed 70.03 f/s\n",
      "484065: done 7470 episodes, mean reward 0.459, speed 68.82 f/s\n",
      "484259: done 7471 episodes, mean reward 0.459, speed 69.32 f/s\n",
      "484340: done 7473 episodes, mean reward 0.453, speed 66.95 f/s\n",
      "484438: done 7474 episodes, mean reward 0.447, speed 66.48 f/s\n",
      "484584: done 7475 episodes, mean reward 0.447, speed 69.69 f/s\n",
      "484864: done 7476 episodes, mean reward 0.448, speed 68.49 f/s\n",
      "485022: done 7477 episodes, mean reward 0.450, speed 70.47 f/s\n",
      "485153: done 7478 episodes, mean reward 0.452, speed 69.27 f/s\n",
      "485472: done 7479 episodes, mean reward 0.457, speed 68.09 f/s\n",
      "485847: done 7480 episodes, mean reward 0.462, speed 69.45 f/s\n",
      "485912: done 7481 episodes, mean reward 0.463, speed 64.88 f/s\n",
      "486058: done 7482 episodes, mean reward 0.463, speed 69.84 f/s\n",
      "486428: done 7484 episodes, mean reward 0.465, speed 68.87 f/s\n",
      "486546: done 7487 episodes, mean reward 0.454, speed 69.52 f/s\n",
      "487062: done 7490 episodes, mean reward 0.459, speed 70.25 f/s\n",
      "487266: done 7491 episodes, mean reward 0.459, speed 68.47 f/s\n",
      "487472: done 7493 episodes, mean reward 0.456, speed 68.47 f/s\n",
      "487637: done 7494 episodes, mean reward 0.445, speed 70.49 f/s\n",
      "487819: done 7495 episodes, mean reward 0.446, speed 70.59 f/s\n",
      "488107: done 7497 episodes, mean reward 0.442, speed 68.82 f/s\n",
      "488366: done 7498 episodes, mean reward 0.447, speed 66.78 f/s\n",
      "488678: done 7500 episodes, mean reward 0.452, speed 69.46 f/s\n",
      "488808: done 7502 episodes, mean reward 0.448, speed 70.61 f/s\n",
      "489685: done 7504 episodes, mean reward 0.452, speed 68.97 f/s\n",
      "489786: done 7505 episodes, mean reward 0.450, speed 70.78 f/s\n",
      "489864: done 7506 episodes, mean reward 0.445, speed 72.05 f/s\n",
      "Test done in 82.78 sec, reward 2.634, steps 1001\n",
      "490026: done 7507 episodes, mean reward 0.445, speed 1.90 f/s\n",
      "490116: done 7508 episodes, mean reward 0.444, speed 68.47 f/s\n",
      "490209: done 7510 episodes, mean reward 0.442, speed 67.89 f/s\n",
      "490518: done 7512 episodes, mean reward 0.437, speed 71.57 f/s\n",
      "490662: done 7513 episodes, mean reward 0.439, speed 69.21 f/s\n",
      "491364: done 7515 episodes, mean reward 0.443, speed 69.93 f/s\n",
      "491510: done 7516 episodes, mean reward 0.421, speed 68.49 f/s\n",
      "491765: done 7517 episodes, mean reward 0.425, speed 69.92 f/s\n",
      "492056: done 7518 episodes, mean reward 0.427, speed 68.17 f/s\n",
      "492585: done 7519 episodes, mean reward 0.431, speed 67.83 f/s\n",
      "492965: done 7520 episodes, mean reward 0.415, speed 67.81 f/s\n",
      "493130: done 7521 episodes, mean reward 0.411, speed 69.94 f/s\n",
      "493333: done 7522 episodes, mean reward 0.409, speed 70.79 f/s\n",
      "493492: done 7523 episodes, mean reward 0.410, speed 66.32 f/s\n",
      "493643: done 7524 episodes, mean reward 0.413, speed 67.60 f/s\n",
      "493771: done 7525 episodes, mean reward 0.413, speed 71.21 f/s\n",
      "493840: done 7526 episodes, mean reward 0.412, speed 67.63 f/s\n",
      "494094: done 7527 episodes, mean reward 0.409, speed 68.62 f/s\n",
      "494297: done 7528 episodes, mean reward 0.413, speed 69.64 f/s\n",
      "495145: done 7529 episodes, mean reward 0.432, speed 69.04 f/s\n",
      "495226: done 7531 episodes, mean reward 0.425, speed 70.34 f/s\n",
      "495747: done 7532 episodes, mean reward 0.437, speed 71.59 f/s\n",
      "495894: done 7533 episodes, mean reward 0.438, speed 71.42 f/s\n",
      "496166: done 7534 episodes, mean reward 0.427, speed 68.60 f/s\n",
      "496635: done 7535 episodes, mean reward 0.436, speed 68.14 f/s\n",
      "496874: done 7536 episodes, mean reward 0.441, speed 67.66 f/s\n",
      "496975: done 7537 episodes, mean reward 0.443, speed 70.10 f/s\n",
      "497407: done 7538 episodes, mean reward 0.454, speed 68.92 f/s\n",
      "497567: done 7540 episodes, mean reward 0.450, speed 70.39 f/s\n",
      "497751: done 7541 episodes, mean reward 0.455, speed 69.82 f/s\n",
      "498766: done 7543 episodes, mean reward 0.479, speed 69.22 f/s\n",
      "498993: done 7544 episodes, mean reward 0.485, speed 70.48 f/s\n",
      "499138: done 7546 episodes, mean reward 0.480, speed 69.15 f/s\n",
      "499240: done 7547 episodes, mean reward 0.479, speed 67.82 f/s\n",
      "499629: done 7549 episodes, mean reward 0.476, speed 70.95 f/s\n",
      "499948: done 7550 episodes, mean reward 0.479, speed 70.13 f/s\n",
      "Test done in 83.04 sec, reward 2.633, steps 1001\n",
      "500085: done 7552 episodes, mean reward 0.474, speed 1.61 f/s\n",
      "500392: done 7554 episodes, mean reward 0.478, speed 70.04 f/s\n",
      "500519: done 7556 episodes, mean reward 0.474, speed 68.72 f/s\n",
      "500684: done 7557 episodes, mean reward 0.476, speed 71.02 f/s\n",
      "500944: done 7558 episodes, mean reward 0.481, speed 67.73 f/s\n",
      "501338: done 7559 episodes, mean reward 0.487, speed 69.47 f/s\n",
      "502278: done 7560 episodes, mean reward 0.509, speed 70.28 f/s\n",
      "502532: done 7561 episodes, mean reward 0.509, speed 67.97 f/s\n",
      "502601: done 7562 episodes, mean reward 0.491, speed 67.22 f/s\n",
      "502765: done 7564 episodes, mean reward 0.490, speed 71.08 f/s\n",
      "503046: done 7565 episodes, mean reward 0.497, speed 69.24 f/s\n",
      "503231: done 7566 episodes, mean reward 0.485, speed 68.64 f/s\n",
      "503658: done 7567 episodes, mean reward 0.490, speed 68.55 f/s\n",
      "504162: done 7569 episodes, mean reward 0.487, speed 68.36 f/s\n",
      "504304: done 7570 episodes, mean reward 0.488, speed 70.39 f/s\n",
      "504395: done 7571 episodes, mean reward 0.486, speed 67.94 f/s\n",
      "504630: done 7573 episodes, mean reward 0.490, speed 68.69 f/s\n",
      "505208: done 7574 episodes, mean reward 0.503, speed 67.99 f/s\n",
      "505431: done 7575 episodes, mean reward 0.505, speed 69.61 f/s\n",
      "505881: done 7576 episodes, mean reward 0.510, speed 70.14 f/s\n",
      "506139: done 7577 episodes, mean reward 0.513, speed 69.46 f/s\n",
      "506395: done 7578 episodes, mean reward 0.516, speed 69.73 f/s\n",
      "507169: done 7579 episodes, mean reward 0.528, speed 68.27 f/s\n",
      "507346: done 7582 episodes, mean reward 0.517, speed 69.94 f/s\n",
      "507417: done 7583 episodes, mean reward 0.519, speed 69.15 f/s\n",
      "507772: done 7584 episodes, mean reward 0.519, speed 69.50 f/s\n",
      "508100: done 7586 episodes, mean reward 0.525, speed 68.31 f/s\n",
      "508240: done 7588 episodes, mean reward 0.526, speed 71.72 f/s\n",
      "508330: done 7589 episodes, mean reward 0.528, speed 69.43 f/s\n",
      "508406: done 7591 episodes, mean reward 0.512, speed 69.14 f/s\n",
      "508571: done 7592 episodes, mean reward 0.515, speed 70.41 f/s\n",
      "508807: done 7594 episodes, mean reward 0.513, speed 69.98 f/s\n",
      "509102: done 7596 episodes, mean reward 0.516, speed 68.66 f/s\n",
      "509380: done 7597 episodes, mean reward 0.516, speed 67.06 f/s\n",
      "509698: done 7598 episodes, mean reward 0.517, speed 68.43 f/s\n",
      "Test done in 70.88 sec, reward 2.237, steps 854\n",
      "510030: done 7599 episodes, mean reward 0.525, speed 4.39 f/s\n",
      "510500: done 7600 episodes, mean reward 0.530, speed 69.31 f/s\n",
      "510648: done 7601 episodes, mean reward 0.533, speed 69.55 f/s\n",
      "510814: done 7605 episodes, mean reward 0.509, speed 68.08 f/s\n",
      "510899: done 7606 episodes, mean reward 0.509, speed 71.09 f/s\n",
      "511165: done 7608 episodes, mean reward 0.510, speed 70.56 f/s\n",
      "511326: done 7609 episodes, mean reward 0.513, speed 69.93 f/s\n",
      "511892: done 7611 episodes, mean reward 0.525, speed 69.68 f/s\n",
      "512477: done 7612 episodes, mean reward 0.534, speed 69.19 f/s\n",
      "513089: done 7613 episodes, mean reward 0.546, speed 67.69 f/s\n",
      "513161: done 7614 episodes, mean reward 0.546, speed 67.20 f/s\n",
      "513304: done 7615 episodes, mean reward 0.534, speed 67.81 f/s\n",
      "514159: done 7616 episodes, mean reward 0.552, speed 68.72 f/s\n",
      "514258: done 7618 episodes, mean reward 0.540, speed 65.98 f/s\n",
      "514745: done 7619 episodes, mean reward 0.539, speed 69.45 f/s\n",
      "514846: done 7621 episodes, mean reward 0.528, speed 70.51 f/s\n",
      "514955: done 7625 episodes, mean reward 0.514, speed 67.41 f/s\n",
      "515465: done 7627 episodes, mean reward 0.519, speed 68.14 f/s\n",
      "516184: done 7629 episodes, mean reward 0.510, speed 68.93 f/s\n",
      "516878: done 7630 episodes, mean reward 0.528, speed 67.65 f/s\n",
      "517159: done 7631 episodes, mean reward 0.534, speed 68.43 f/s\n",
      "517539: done 7633 episodes, mean reward 0.526, speed 71.92 f/s\n",
      "517718: done 7634 episodes, mean reward 0.524, speed 68.83 f/s\n",
      "517942: done 7635 episodes, mean reward 0.517, speed 69.49 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518071: done 7636 episodes, mean reward 0.514, speed 70.24 f/s\n",
      "518638: done 7638 episodes, mean reward 0.515, speed 68.96 f/s\n",
      "518728: done 7639 episodes, mean reward 0.516, speed 67.56 f/s\n",
      "518835: done 7640 episodes, mean reward 0.516, speed 69.91 f/s\n",
      "518979: done 7641 episodes, mean reward 0.515, speed 69.81 f/s\n",
      "519062: done 7642 episodes, mean reward 0.517, speed 67.95 f/s\n",
      "519823: done 7643 episodes, mean reward 0.510, speed 68.03 f/s\n",
      "519902: done 7645 episodes, mean reward 0.504, speed 66.68 f/s\n",
      "Test done in 49.78 sec, reward 1.555, steps 605\n",
      "521003: done 7647 episodes, mean reward 0.528, speed 16.69 f/s\n",
      "521104: done 7648 episodes, mean reward 0.530, speed 71.07 f/s\n",
      "521841: done 7649 episodes, mean reward 0.541, speed 69.14 f/s\n",
      "521931: done 7650 episodes, mean reward 0.535, speed 72.64 f/s\n",
      "522020: done 7651 episodes, mean reward 0.537, speed 71.53 f/s\n",
      "522243: done 7652 episodes, mean reward 0.540, speed 68.49 f/s\n",
      "522347: done 7654 episodes, mean reward 0.534, speed 70.27 f/s\n",
      "522468: done 7656 episodes, mean reward 0.534, speed 71.26 f/s\n",
      "522571: done 7658 episodes, mean reward 0.525, speed 67.55 f/s\n",
      "522731: done 7660 episodes, mean reward 0.494, speed 69.57 f/s\n",
      "522820: done 7661 episodes, mean reward 0.490, speed 68.68 f/s\n",
      "523366: done 7662 episodes, mean reward 0.503, speed 68.79 f/s\n",
      "523531: done 7663 episodes, mean reward 0.506, speed 68.61 f/s\n",
      "523645: done 7664 episodes, mean reward 0.506, speed 67.60 f/s\n",
      "524033: done 7665 episodes, mean reward 0.509, speed 69.65 f/s\n",
      "524533: done 7667 episodes, mean reward 0.507, speed 70.30 f/s\n",
      "524602: done 7668 episodes, mean reward 0.507, speed 64.92 f/s\n",
      "524842: done 7669 episodes, mean reward 0.502, speed 68.50 f/s\n",
      "524944: done 7670 episodes, mean reward 0.501, speed 70.06 f/s\n",
      "525702: done 7671 episodes, mean reward 0.518, speed 68.99 f/s\n",
      "525811: done 7674 episodes, mean reward 0.499, speed 68.46 f/s\n",
      "526198: done 7675 episodes, mean reward 0.503, speed 66.75 f/s\n",
      "526761: done 7676 episodes, mean reward 0.506, speed 69.23 f/s\n",
      "526897: done 7677 episodes, mean reward 0.503, speed 69.72 f/s\n",
      "527093: done 7679 episodes, mean reward 0.481, speed 69.42 f/s\n",
      "527243: done 7680 episodes, mean reward 0.484, speed 71.75 f/s\n",
      "528056: done 7681 episodes, mean reward 0.505, speed 69.30 f/s\n",
      "528519: done 7682 episodes, mean reward 0.514, speed 69.33 f/s\n",
      "528646: done 7683 episodes, mean reward 0.515, speed 63.99 f/s\n",
      "528751: done 7685 episodes, mean reward 0.508, speed 65.60 f/s\n",
      "528839: done 7686 episodes, mean reward 0.502, speed 70.15 f/s\n",
      "528971: done 7687 episodes, mean reward 0.504, speed 66.80 f/s\n",
      "529090: done 7689 episodes, mean reward 0.503, speed 69.63 f/s\n",
      "529214: done 7690 episodes, mean reward 0.505, speed 71.35 f/s\n",
      "529763: done 7692 episodes, mean reward 0.514, speed 67.86 f/s\n",
      "529890: done 7693 episodes, mean reward 0.516, speed 66.05 f/s\n",
      "Test done in 76.92 sec, reward 2.442, steps 928\n",
      "530983: done 7694 episodes, mean reward 0.540, speed 11.78 f/s\n",
      "531412: done 7695 episodes, mean reward 0.551, speed 68.92 f/s\n",
      "531554: done 7696 episodes, mean reward 0.547, speed 68.20 f/s\n",
      "531732: done 7697 episodes, mean reward 0.545, speed 69.44 f/s\n",
      "531973: done 7699 episodes, mean reward 0.534, speed 67.35 f/s\n",
      "532195: done 7700 episodes, mean reward 0.528, speed 70.03 f/s\n",
      "532295: done 7701 episodes, mean reward 0.526, speed 69.45 f/s\n",
      "532415: done 7702 episodes, mean reward 0.529, speed 70.66 f/s\n",
      "532635: done 7704 episodes, mean reward 0.533, speed 68.99 f/s\n",
      "533124: done 7705 episodes, mean reward 0.543, speed 68.46 f/s\n",
      "533442: done 7706 episodes, mean reward 0.550, speed 68.98 f/s\n",
      "534312: done 7708 episodes, mean reward 0.566, speed 68.58 f/s\n",
      "534402: done 7709 episodes, mean reward 0.564, speed 69.23 f/s\n",
      "534492: done 7710 episodes, mean reward 0.565, speed 72.15 f/s\n",
      "534562: done 7711 episodes, mean reward 0.554, speed 66.13 f/s\n",
      "534690: done 7712 episodes, mean reward 0.542, speed 70.08 f/s\n",
      "535286: done 7713 episodes, mean reward 0.541, speed 69.26 f/s\n",
      "535453: done 7715 episodes, mean reward 0.540, speed 68.66 f/s\n",
      "535579: done 7716 episodes, mean reward 0.522, speed 67.81 f/s\n",
      "535942: done 7718 episodes, mean reward 0.528, speed 68.65 f/s\n",
      "536368: done 7719 episodes, mean reward 0.527, speed 67.91 f/s\n",
      "536963: done 7721 episodes, mean reward 0.540, speed 70.35 f/s\n",
      "537065: done 7722 episodes, mean reward 0.542, speed 67.50 f/s\n",
      "537142: done 7724 episodes, mean reward 0.543, speed 68.94 f/s\n",
      "537304: done 7726 episodes, mean reward 0.544, speed 66.68 f/s\n",
      "537693: done 7727 episodes, mean reward 0.542, speed 69.50 f/s\n",
      "538219: done 7728 episodes, mean reward 0.554, speed 68.99 f/s\n",
      "538455: done 7729 episodes, mean reward 0.543, speed 67.86 f/s\n",
      "538562: done 7730 episodes, mean reward 0.528, speed 69.19 f/s\n",
      "538644: done 7732 episodes, mean reward 0.521, speed 73.22 f/s\n",
      "538810: done 7733 episodes, mean reward 0.517, speed 69.16 f/s\n",
      "538938: done 7735 episodes, mean reward 0.510, speed 69.77 f/s\n",
      "539106: done 7737 episodes, mean reward 0.509, speed 68.63 f/s\n",
      "539385: done 7738 episodes, mean reward 0.503, speed 67.86 f/s\n",
      "539580: done 7740 episodes, mean reward 0.503, speed 68.55 f/s\n",
      "539721: done 7742 episodes, mean reward 0.501, speed 69.28 f/s\n",
      "539925: done 7743 episodes, mean reward 0.487, speed 69.79 f/s\n",
      "Test done in 66.52 sec, reward 2.134, steps 813\n",
      "540134: done 7744 episodes, mean reward 0.492, speed 3.01 f/s\n",
      "540565: done 7745 episodes, mean reward 0.502, speed 71.99 f/s\n",
      "540801: done 7746 episodes, mean reward 0.507, speed 70.29 f/s\n",
      "541002: done 7747 episodes, mean reward 0.485, speed 69.11 f/s\n",
      "541352: done 7748 episodes, mean reward 0.491, speed 68.49 f/s\n",
      "542064: done 7750 episodes, mean reward 0.488, speed 69.91 f/s\n",
      "542926: done 7752 episodes, mean reward 0.502, speed 69.46 f/s\n",
      "543122: done 7753 episodes, mean reward 0.506, speed 67.14 f/s\n",
      "543414: done 7754 episodes, mean reward 0.512, speed 69.73 f/s\n",
      "543676: done 7755 episodes, mean reward 0.519, speed 70.10 f/s\n",
      "544028: done 7756 episodes, mean reward 0.525, speed 69.13 f/s\n",
      "544098: done 7757 episodes, mean reward 0.526, speed 66.41 f/s\n",
      "544220: done 7758 episodes, mean reward 0.528, speed 66.51 f/s\n",
      "544385: done 7759 episodes, mean reward 0.531, speed 69.67 f/s\n",
      "544793: done 7761 episodes, mean reward 0.536, speed 68.95 f/s\n",
      "545000: done 7763 episodes, mean reward 0.523, speed 67.95 f/s\n",
      "545318: done 7764 episodes, mean reward 0.529, speed 67.91 f/s\n",
      "545456: done 7766 episodes, mean reward 0.521, speed 70.55 f/s\n",
      "545621: done 7767 episodes, mean reward 0.513, speed 70.49 f/s\n",
      "545856: done 7768 episodes, mean reward 0.517, speed 70.15 f/s\n",
      "546339: done 7770 episodes, mean reward 0.521, speed 69.69 f/s\n",
      "546525: done 7772 episodes, mean reward 0.505, speed 68.40 f/s\n",
      "546789: done 7773 episodes, mean reward 0.512, speed 70.11 f/s\n",
      "547348: done 7774 episodes, mean reward 0.524, speed 68.62 f/s\n",
      "547477: done 7776 episodes, mean reward 0.503, speed 68.16 f/s\n",
      "547920: done 7777 episodes, mean reward 0.512, speed 69.82 f/s\n",
      "548137: done 7778 episodes, mean reward 0.516, speed 69.84 f/s\n",
      "548792: done 7780 episodes, mean reward 0.526, speed 68.69 f/s\n",
      "548882: done 7781 episodes, mean reward 0.507, speed 70.82 f/s\n",
      "549104: done 7782 episodes, mean reward 0.500, speed 67.65 f/s\n",
      "549230: done 7783 episodes, mean reward 0.500, speed 67.50 f/s\n",
      "549361: done 7785 episodes, mean reward 0.501, speed 68.56 f/s\n",
      "549451: done 7786 episodes, mean reward 0.502, speed 70.37 f/s\n",
      "549902: done 7787 episodes, mean reward 0.510, speed 69.49 f/s\n",
      "Test done in 80.51 sec, reward 2.567, steps 976\n",
      "550287: done 7788 episodes, mean reward 0.520, speed 4.47 f/s\n",
      "550511: done 7789 episodes, mean reward 0.524, speed 69.03 f/s\n",
      "550638: done 7790 episodes, mean reward 0.524, speed 70.25 f/s\n",
      "550781: done 7791 episodes, mean reward 0.526, speed 69.11 f/s\n",
      "550884: done 7792 episodes, mean reward 0.516, speed 66.52 f/s\n",
      "550951: done 7793 episodes, mean reward 0.515, speed 66.12 f/s\n",
      "552001: done 7795 episodes, mean reward 0.503, speed 69.54 f/s\n",
      "552229: done 7796 episodes, mean reward 0.506, speed 67.99 f/s\n",
      "552327: done 7798 episodes, mean reward 0.502, speed 70.36 f/s\n",
      "552568: done 7799 episodes, mean reward 0.503, speed 71.68 f/s\n",
      "552878: done 7800 episodes, mean reward 0.506, speed 69.04 f/s\n",
      "553025: done 7801 episodes, mean reward 0.507, speed 68.52 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553190: done 7802 episodes, mean reward 0.509, speed 68.67 f/s\n",
      "553451: done 7803 episodes, mean reward 0.514, speed 68.51 f/s\n",
      "553788: done 7804 episodes, mean reward 0.519, speed 71.02 f/s\n",
      "553903: done 7805 episodes, mean reward 0.509, speed 67.99 f/s\n",
      "554159: done 7806 episodes, mean reward 0.507, speed 70.21 f/s\n",
      "554388: done 7808 episodes, mean reward 0.490, speed 66.62 f/s\n",
      "554505: done 7810 episodes, mean reward 0.489, speed 71.79 f/s\n",
      "554874: done 7811 episodes, mean reward 0.496, speed 68.52 f/s\n",
      "555124: done 7812 episodes, mean reward 0.500, speed 68.40 f/s\n",
      "555297: done 7813 episodes, mean reward 0.489, speed 70.63 f/s\n",
      "555626: done 7815 episodes, mean reward 0.493, speed 68.62 f/s\n",
      "555735: done 7816 episodes, mean reward 0.492, speed 71.82 f/s\n",
      "555938: done 7817 episodes, mean reward 0.496, speed 68.97 f/s\n",
      "556588: done 7819 episodes, mean reward 0.494, speed 70.81 f/s\n",
      "556754: done 7820 episodes, mean reward 0.497, speed 68.54 f/s\n",
      "557213: done 7821 episodes, mean reward 0.494, speed 68.45 f/s\n",
      "557543: done 7822 episodes, mean reward 0.501, speed 68.97 f/s\n",
      "557646: done 7825 episodes, mean reward 0.501, speed 67.29 f/s\n",
      "557910: done 7827 episodes, mean reward 0.494, speed 70.08 f/s\n",
      "558496: done 7828 episodes, mean reward 0.495, speed 68.17 f/s\n",
      "558585: done 7829 episodes, mean reward 0.491, speed 67.85 f/s\n",
      "558837: done 7830 episodes, mean reward 0.495, speed 68.29 f/s\n",
      "559104: done 7832 episodes, mean reward 0.500, speed 69.84 f/s\n",
      "559908: done 7833 episodes, mean reward 0.517, speed 69.39 f/s\n",
      "Test done in 73.03 sec, reward 2.314, steps 881\n",
      "560056: done 7834 episodes, mean reward 0.519, speed 1.97 f/s\n",
      "560581: done 7835 episodes, mean reward 0.531, speed 68.21 f/s\n",
      "561309: done 7837 episodes, mean reward 0.546, speed 69.45 f/s\n",
      "561670: done 7839 episodes, mean reward 0.547, speed 67.89 f/s\n",
      "561749: done 7841 episodes, mean reward 0.544, speed 69.43 f/s\n",
      "561851: done 7842 episodes, mean reward 0.544, speed 69.97 f/s\n",
      "561995: done 7843 episodes, mean reward 0.542, speed 67.64 f/s\n",
      "562257: done 7844 episodes, mean reward 0.543, speed 67.70 f/s\n",
      "562423: done 7845 episodes, mean reward 0.536, speed 69.96 f/s\n",
      "562652: done 7846 episodes, mean reward 0.535, speed 67.98 f/s\n",
      "562965: done 7847 episodes, mean reward 0.538, speed 69.24 f/s\n",
      "563065: done 7848 episodes, mean reward 0.531, speed 71.04 f/s\n",
      "563460: done 7850 episodes, mean reward 0.523, speed 68.43 f/s\n",
      "563773: done 7851 episodes, mean reward 0.529, speed 69.04 f/s\n",
      "564110: done 7852 episodes, mean reward 0.517, speed 68.62 f/s\n",
      "564180: done 7853 episodes, mean reward 0.514, speed 67.82 f/s\n",
      "564478: done 7855 episodes, mean reward 0.508, speed 69.07 f/s\n",
      "564605: done 7856 episodes, mean reward 0.501, speed 68.65 f/s\n",
      "564721: done 7857 episodes, mean reward 0.503, speed 68.02 f/s\n",
      "565504: done 7858 episodes, mean reward 0.520, speed 71.13 f/s\n",
      "565572: done 7859 episodes, mean reward 0.518, speed 65.58 f/s\n",
      "565671: done 7860 episodes, mean reward 0.519, speed 70.35 f/s\n",
      "566222: done 7862 episodes, mean reward 0.522, speed 69.50 f/s\n",
      "566586: done 7864 episodes, mean reward 0.519, speed 69.73 f/s\n",
      "567069: done 7865 episodes, mean reward 0.531, speed 67.54 f/s\n",
      "568070: done 7866 episodes, mean reward 0.555, speed 69.37 f/s\n",
      "568166: done 7867 episodes, mean reward 0.554, speed 70.12 f/s\n",
      "568516: done 7868 episodes, mean reward 0.557, speed 70.64 f/s\n",
      "568639: done 7870 episodes, mean reward 0.547, speed 67.59 f/s\n",
      "569107: done 7871 episodes, mean reward 0.559, speed 68.37 f/s\n",
      "569713: done 7872 episodes, mean reward 0.570, speed 68.41 f/s\n",
      "Test done in 63.77 sec, reward 2.023, steps 772\n",
      "570210: done 7874 episodes, mean reward 0.562, speed 7.01 f/s\n",
      "571211: done 7875 episodes, mean reward 0.586, speed 68.60 f/s\n",
      "571321: done 7876 episodes, mean reward 0.587, speed 69.83 f/s\n",
      "571574: done 7877 episodes, mean reward 0.582, speed 68.49 f/s\n",
      "572638: done 7880 episodes, mean reward 0.587, speed 70.32 f/s\n",
      "573000: done 7881 episodes, mean reward 0.594, speed 68.48 f/s\n",
      "573106: done 7882 episodes, mean reward 0.591, speed 68.39 f/s\n",
      "573271: done 7883 episodes, mean reward 0.592, speed 69.12 f/s\n",
      "573418: done 7884 episodes, mean reward 0.594, speed 69.79 f/s\n",
      "573965: done 7886 episodes, mean reward 0.604, speed 68.53 f/s\n",
      "574353: done 7888 episodes, mean reward 0.592, speed 68.99 f/s\n",
      "574642: done 7889 episodes, mean reward 0.594, speed 67.68 f/s\n",
      "574732: done 7890 episodes, mean reward 0.593, speed 73.14 f/s\n",
      "575003: done 7891 episodes, mean reward 0.596, speed 68.24 f/s\n",
      "575262: done 7892 episodes, mean reward 0.600, speed 68.93 f/s\n",
      "575537: done 7893 episodes, mean reward 0.606, speed 68.30 f/s\n",
      "575644: done 7894 episodes, mean reward 0.607, speed 68.34 f/s\n",
      "575885: done 7896 episodes, mean reward 0.581, speed 68.95 f/s\n",
      "576126: done 7897 episodes, mean reward 0.586, speed 67.24 f/s\n",
      "576233: done 7898 episodes, mean reward 0.587, speed 69.01 f/s\n",
      "576393: done 7899 episodes, mean reward 0.585, speed 67.57 f/s\n",
      "577079: done 7900 episodes, mean reward 0.595, speed 69.80 f/s\n",
      "577944: done 7901 episodes, mean reward 0.613, speed 70.21 f/s\n",
      "578231: done 7903 episodes, mean reward 0.609, speed 67.18 f/s\n",
      "578332: done 7904 episodes, mean reward 0.603, speed 67.88 f/s\n",
      "578515: done 7905 episodes, mean reward 0.605, speed 68.89 f/s\n",
      "578904: done 7907 episodes, mean reward 0.608, speed 69.54 f/s\n",
      "579316: done 7908 episodes, mean reward 0.613, speed 67.67 f/s\n",
      "579458: done 7909 episodes, mean reward 0.617, speed 65.84 f/s\n",
      "579793: done 7910 episodes, mean reward 0.623, speed 69.36 f/s\n",
      "Test done in 74.49 sec, reward 2.375, steps 904\n",
      "580092: done 7911 episodes, mean reward 0.622, speed 3.79 f/s\n",
      "580239: done 7912 episodes, mean reward 0.619, speed 68.62 f/s\n",
      "580359: done 7913 episodes, mean reward 0.617, speed 68.96 f/s\n",
      "580502: done 7914 episodes, mean reward 0.620, speed 68.88 f/s\n",
      "580783: done 7916 episodes, mean reward 0.616, speed 69.40 f/s\n",
      "581495: done 7917 episodes, mean reward 0.631, speed 68.02 f/s\n",
      "581729: done 7919 episodes, mean reward 0.620, speed 69.28 f/s\n",
      "582177: done 7920 episodes, mean reward 0.627, speed 68.63 f/s\n",
      "582341: done 7922 episodes, mean reward 0.611, speed 66.91 f/s\n",
      "582998: done 7923 episodes, mean reward 0.627, speed 68.33 f/s\n",
      "583290: done 7924 episodes, mean reward 0.634, speed 68.00 f/s\n",
      "584067: done 7925 episodes, mean reward 0.653, speed 69.25 f/s\n",
      "584585: done 7926 episodes, mean reward 0.665, speed 68.26 f/s\n",
      "584894: done 7927 episodes, mean reward 0.667, speed 69.50 f/s\n",
      "584999: done 7928 episodes, mean reward 0.655, speed 69.32 f/s\n",
      "585821: done 7929 episodes, mean reward 0.674, speed 68.77 f/s\n",
      "586081: done 7930 episodes, mean reward 0.675, speed 68.63 f/s\n",
      "586789: done 7931 episodes, mean reward 0.692, speed 70.18 f/s\n",
      "586859: done 7932 episodes, mean reward 0.688, speed 67.82 f/s\n",
      "586927: done 7933 episodes, mean reward 0.668, speed 66.13 f/s\n",
      "587031: done 7935 episodes, mean reward 0.654, speed 69.64 f/s\n",
      "587258: done 7937 episodes, mean reward 0.640, speed 68.63 f/s\n",
      "587778: done 7938 episodes, mean reward 0.653, speed 67.66 f/s\n",
      "587912: done 7941 episodes, mean reward 0.646, speed 72.28 f/s\n",
      "588456: done 7944 episodes, mean reward 0.647, speed 70.85 f/s\n",
      "588630: done 7946 episodes, mean reward 0.642, speed 69.15 f/s\n",
      "588758: done 7948 episodes, mean reward 0.635, speed 67.30 f/s\n",
      "588885: done 7949 episodes, mean reward 0.636, speed 69.86 f/s\n",
      "589297: done 7950 episodes, mean reward 0.638, speed 68.61 f/s\n",
      "589739: done 7951 episodes, mean reward 0.641, speed 69.48 f/s\n",
      "Test done in 74.67 sec, reward 2.376, steps 905\n",
      "590010: done 7952 episodes, mean reward 0.639, speed 3.44 f/s\n",
      "590155: done 7954 episodes, mean reward 0.639, speed 71.22 f/s\n",
      "590258: done 7955 episodes, mean reward 0.635, speed 68.85 f/s\n",
      "590531: done 7956 episodes, mean reward 0.640, speed 68.32 f/s\n",
      "590623: done 7957 episodes, mean reward 0.639, speed 69.20 f/s\n",
      "590941: done 7958 episodes, mean reward 0.626, speed 67.24 f/s\n",
      "591553: done 7960 episodes, mean reward 0.638, speed 69.54 f/s\n",
      "591883: done 7961 episodes, mean reward 0.645, speed 70.15 f/s\n",
      "591988: done 7962 episodes, mean reward 0.635, speed 68.44 f/s\n",
      "592282: done 7964 episodes, mean reward 0.634, speed 71.02 f/s\n",
      "592517: done 7965 episodes, mean reward 0.627, speed 68.28 f/s\n",
      "593006: done 7966 episodes, mean reward 0.614, speed 68.90 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593088: done 7968 episodes, mean reward 0.604, speed 68.47 f/s\n",
      "594098: done 7970 episodes, mean reward 0.627, speed 69.66 f/s\n",
      "594187: done 7971 episodes, mean reward 0.616, speed 68.45 f/s\n",
      "594257: done 7972 episodes, mean reward 0.603, speed 67.20 f/s\n",
      "594650: done 7973 episodes, mean reward 0.612, speed 67.88 f/s\n",
      "595367: done 7974 episodes, mean reward 0.619, speed 70.46 f/s\n",
      "595564: done 7975 episodes, mean reward 0.597, speed 66.14 f/s\n",
      "595937: done 7977 episodes, mean reward 0.597, speed 68.82 f/s\n",
      "596081: done 7978 episodes, mean reward 0.601, speed 68.86 f/s\n",
      "596265: done 7979 episodes, mean reward 0.604, speed 71.04 f/s\n",
      "596594: done 7980 episodes, mean reward 0.587, speed 68.75 f/s\n",
      "596924: done 7981 episodes, mean reward 0.586, speed 68.66 f/s\n",
      "597988: done 7983 episodes, mean reward 0.607, speed 68.40 f/s\n",
      "598062: done 7984 episodes, mean reward 0.606, speed 69.09 f/s\n",
      "598461: done 7986 episodes, mean reward 0.602, speed 70.33 f/s\n",
      "598658: done 7988 episodes, mean reward 0.597, speed 70.42 f/s\n",
      "598751: done 7990 episodes, mean reward 0.589, speed 65.63 f/s\n",
      "599004: done 7991 episodes, mean reward 0.589, speed 70.05 f/s\n",
      "599399: done 7992 episodes, mean reward 0.592, speed 70.06 f/s\n",
      "599487: done 7993 episodes, mean reward 0.587, speed 71.60 f/s\n",
      "599683: done 7994 episodes, mean reward 0.589, speed 70.00 f/s\n",
      "Test done in 80.65 sec, reward 2.566, steps 976\n",
      "600103: done 7995 episodes, mean reward 0.599, speed 4.83 f/s\n",
      "600644: done 7996 episodes, mean reward 0.608, speed 68.98 f/s\n",
      "601298: done 7997 episodes, mean reward 0.620, speed 69.86 f/s\n",
      "601482: done 7998 episodes, mean reward 0.622, speed 68.05 f/s\n",
      "601891: done 7999 episodes, mean reward 0.628, speed 67.78 f/s\n",
      "601993: done 8000 episodes, mean reward 0.613, speed 67.94 f/s\n",
      "602497: done 8002 episodes, mean reward 0.603, speed 68.79 f/s\n",
      "603512: done 8004 episodes, mean reward 0.621, speed 68.68 f/s\n",
      "603662: done 8006 episodes, mean reward 0.618, speed 69.30 f/s\n",
      "604091: done 8007 episodes, mean reward 0.621, speed 69.61 f/s\n",
      "604588: done 8009 episodes, mean reward 0.619, speed 69.88 f/s\n",
      "604668: done 8010 episodes, mean reward 0.612, speed 72.00 f/s\n",
      "605036: done 8012 episodes, mean reward 0.610, speed 68.50 f/s\n",
      "605238: done 8013 episodes, mean reward 0.612, speed 67.28 f/s\n",
      "605567: done 8014 episodes, mean reward 0.617, speed 69.52 f/s\n",
      "605803: done 8015 episodes, mean reward 0.622, speed 68.49 f/s\n",
      "606073: done 8016 episodes, mean reward 0.623, speed 69.24 f/s\n",
      "606256: done 8017 episodes, mean reward 0.608, speed 69.44 f/s\n",
      "607036: done 8018 episodes, mean reward 0.627, speed 68.68 f/s\n",
      "607217: done 8019 episodes, mean reward 0.627, speed 66.56 f/s\n",
      "607817: done 8020 episodes, mean reward 0.631, speed 68.72 f/s\n",
      "607925: done 8022 episodes, mean reward 0.629, speed 70.32 f/s\n",
      "608564: done 8023 episodes, mean reward 0.629, speed 69.18 f/s\n",
      "608990: done 8024 episodes, mean reward 0.633, speed 68.61 f/s\n",
      "609072: done 8026 episodes, mean reward 0.601, speed 71.91 f/s\n",
      "609250: done 8027 episodes, mean reward 0.598, speed 68.67 f/s\n",
      "609385: done 8028 episodes, mean reward 0.598, speed 65.89 f/s\n",
      "609720: done 8029 episodes, mean reward 0.586, speed 70.60 f/s\n",
      "Test done in 71.31 sec, reward 2.272, steps 864\n",
      "610044: done 8032 episodes, mean reward 0.567, speed 4.26 f/s\n",
      "610225: done 8033 episodes, mean reward 0.570, speed 70.41 f/s\n",
      "610341: done 8035 episodes, mean reward 0.570, speed 70.40 f/s\n",
      "610487: done 8036 episodes, mean reward 0.573, speed 68.90 f/s\n",
      "610668: done 8037 episodes, mean reward 0.573, speed 70.00 f/s\n",
      "610829: done 8038 episodes, mean reward 0.563, speed 69.37 f/s\n",
      "611120: done 8039 episodes, mean reward 0.570, speed 69.15 f/s\n",
      "611696: done 8040 episodes, mean reward 0.585, speed 68.10 f/s\n",
      "612131: done 8041 episodes, mean reward 0.594, speed 70.37 f/s\n",
      "612411: done 8042 episodes, mean reward 0.601, speed 68.01 f/s\n",
      "612631: done 8043 episodes, mean reward 0.605, speed 69.79 f/s\n",
      "613155: done 8044 episodes, mean reward 0.606, speed 69.30 f/s\n",
      "613417: done 8045 episodes, mean reward 0.612, speed 71.23 f/s\n",
      "613696: done 8047 episodes, mean reward 0.613, speed 69.95 f/s\n",
      "614473: done 8049 episodes, mean reward 0.630, speed 69.03 f/s\n",
      "614821: done 8050 episodes, mean reward 0.628, speed 68.00 f/s\n",
      "614923: done 8051 episodes, mean reward 0.619, speed 68.82 f/s\n",
      "615597: done 8052 episodes, mean reward 0.630, speed 68.63 f/s\n",
      "615705: done 8053 episodes, mean reward 0.632, speed 68.83 f/s\n",
      "615806: done 8054 episodes, mean reward 0.632, speed 68.61 f/s\n",
      "615983: done 8055 episodes, mean reward 0.634, speed 69.68 f/s\n",
      "616356: done 8056 episodes, mean reward 0.636, speed 68.96 f/s\n",
      "616489: done 8058 episodes, mean reward 0.629, speed 69.24 f/s\n",
      "616708: done 8060 episodes, mean reward 0.619, speed 68.48 f/s\n",
      "617200: done 8061 episodes, mean reward 0.623, speed 69.12 f/s\n",
      "617383: done 8063 episodes, mean reward 0.624, speed 68.89 f/s\n",
      "617712: done 8064 episodes, mean reward 0.626, speed 67.97 f/s\n",
      "617957: done 8067 episodes, mean reward 0.611, speed 69.46 f/s\n",
      "618042: done 8068 episodes, mean reward 0.613, speed 65.18 f/s\n",
      "618209: done 8069 episodes, mean reward 0.617, speed 70.07 f/s\n",
      "618822: done 8070 episodes, mean reward 0.607, speed 70.07 f/s\n",
      "619070: done 8071 episodes, mean reward 0.612, speed 69.60 f/s\n",
      "619656: done 8072 episodes, mean reward 0.625, speed 68.13 f/s\n",
      "619727: done 8073 episodes, mean reward 0.617, speed 67.94 f/s\n",
      "619868: done 8074 episodes, mean reward 0.602, speed 68.44 f/s\n",
      "Test done in 47.77 sec, reward 1.510, steps 582\n",
      "620175: done 8075 episodes, mean reward 0.605, speed 5.88 f/s\n",
      "620243: done 8077 episodes, mean reward 0.597, speed 67.24 f/s\n",
      "620317: done 8079 episodes, mean reward 0.590, speed 71.76 f/s\n",
      "620875: done 8082 episodes, mean reward 0.585, speed 67.61 f/s\n",
      "621188: done 8083 episodes, mean reward 0.567, speed 70.04 f/s\n",
      "621599: done 8084 episodes, mean reward 0.576, speed 69.61 f/s\n",
      "621828: done 8086 episodes, mean reward 0.571, speed 68.83 f/s\n",
      "621932: done 8087 episodes, mean reward 0.573, speed 68.77 f/s\n",
      "622872: done 8088 episodes, mean reward 0.593, speed 69.92 f/s\n",
      "623091: done 8090 episodes, mean reward 0.596, speed 67.84 f/s\n",
      "623560: done 8091 episodes, mean reward 0.602, speed 68.32 f/s\n",
      "623916: done 8093 episodes, mean reward 0.599, speed 69.59 f/s\n",
      "624073: done 8094 episodes, mean reward 0.597, speed 69.74 f/s\n",
      "624610: done 8095 episodes, mean reward 0.601, speed 68.78 f/s\n",
      "624826: done 8096 episodes, mean reward 0.592, speed 69.51 f/s\n",
      "625129: done 8097 episodes, mean reward 0.583, speed 67.06 f/s\n",
      "625326: done 8098 episodes, mean reward 0.583, speed 71.08 f/s\n",
      "625466: done 8099 episodes, mean reward 0.576, speed 68.82 f/s\n",
      "625683: done 8100 episodes, mean reward 0.579, speed 66.68 f/s\n",
      "625753: done 8101 episodes, mean reward 0.580, speed 67.82 f/s\n",
      "625852: done 8102 episodes, mean reward 0.570, speed 68.30 f/s\n",
      "625941: done 8103 episodes, mean reward 0.572, speed 67.94 f/s\n",
      "626316: done 8104 episodes, mean reward 0.555, speed 69.26 f/s\n",
      "626558: done 8105 episodes, mean reward 0.560, speed 69.46 f/s\n",
      "626851: done 8106 episodes, mean reward 0.566, speed 68.86 f/s\n",
      "627088: done 8107 episodes, mean reward 0.561, speed 68.45 f/s\n",
      "627168: done 8110 episodes, mean reward 0.548, speed 66.68 f/s\n",
      "627274: done 8111 episodes, mean reward 0.549, speed 70.13 f/s\n",
      "627376: done 8112 episodes, mean reward 0.544, speed 72.36 f/s\n",
      "627534: done 8113 episodes, mean reward 0.543, speed 68.50 f/s\n",
      "627681: done 8115 episodes, mean reward 0.532, speed 67.60 f/s\n",
      "627955: done 8116 episodes, mean reward 0.532, speed 68.19 f/s\n",
      "628259: done 8118 episodes, mean reward 0.515, speed 71.20 f/s\n",
      "628766: done 8119 episodes, mean reward 0.524, speed 69.10 f/s\n",
      "628917: done 8121 episodes, mean reward 0.511, speed 67.53 f/s\n",
      "629019: done 8122 episodes, mean reward 0.512, speed 69.96 f/s\n",
      "629161: done 8123 episodes, mean reward 0.499, speed 69.27 f/s\n",
      "629629: done 8124 episodes, mean reward 0.500, speed 68.95 f/s\n",
      "629868: done 8125 episodes, mean reward 0.505, speed 69.86 f/s\n",
      "Test done in 65.09 sec, reward 2.058, steps 787\n",
      "630054: done 8126 episodes, mean reward 0.507, speed 2.74 f/s\n",
      "630462: done 8128 episodes, mean reward 0.510, speed 69.42 f/s\n",
      "630963: done 8130 episodes, mean reward 0.513, speed 67.97 f/s\n",
      "631091: done 8131 episodes, mean reward 0.516, speed 70.91 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631181: done 8132 episodes, mean reward 0.511, speed 72.23 f/s\n",
      "632199: done 8134 episodes, mean reward 0.532, speed 68.78 f/s\n",
      "632288: done 8135 episodes, mean reward 0.532, speed 71.55 f/s\n",
      "632726: done 8136 episodes, mean reward 0.540, speed 69.00 f/s\n",
      "632882: done 8138 episodes, mean reward 0.535, speed 68.19 f/s\n",
      "632972: done 8139 episodes, mean reward 0.530, speed 71.05 f/s\n",
      "633211: done 8140 episodes, mean reward 0.521, speed 70.59 f/s\n",
      "633409: done 8141 episodes, mean reward 0.514, speed 67.65 f/s\n",
      "633574: done 8142 episodes, mean reward 0.511, speed 68.09 f/s\n",
      "634027: done 8144 episodes, mean reward 0.504, speed 68.27 f/s\n",
      "634128: done 8145 episodes, mean reward 0.500, speed 68.12 f/s\n",
      "634252: done 8146 episodes, mean reward 0.501, speed 70.05 f/s\n",
      "634623: done 8147 episodes, mean reward 0.505, speed 70.44 f/s\n",
      "634940: done 8148 episodes, mean reward 0.512, speed 69.16 f/s\n",
      "635315: done 8149 episodes, mean reward 0.503, speed 69.78 f/s\n",
      "635625: done 8151 episodes, mean reward 0.499, speed 67.45 f/s\n",
      "635727: done 8152 episodes, mean reward 0.484, speed 70.83 f/s\n",
      "635849: done 8153 episodes, mean reward 0.484, speed 69.90 f/s\n",
      "636772: done 8154 episodes, mean reward 0.506, speed 68.24 f/s\n",
      "637442: done 8155 episodes, mean reward 0.518, speed 70.55 f/s\n",
      "638078: done 8156 episodes, mean reward 0.525, speed 68.32 f/s\n",
      "638178: done 8158 episodes, mean reward 0.524, speed 69.33 f/s\n",
      "638461: done 8159 episodes, mean reward 0.531, speed 70.75 f/s\n",
      "638754: done 8161 episodes, mean reward 0.520, speed 68.31 f/s\n",
      "639379: done 8162 episodes, mean reward 0.534, speed 69.29 f/s\n",
      "639468: done 8163 episodes, mean reward 0.533, speed 69.78 f/s\n",
      "Test done in 56.22 sec, reward 1.781, steps 685\n",
      "640068: done 8165 episodes, mean reward 0.540, speed 9.25 f/s\n",
      "640706: done 8167 episodes, mean reward 0.551, speed 68.87 f/s\n",
      "641281: done 8168 episodes, mean reward 0.564, speed 69.80 f/s\n",
      "641693: done 8169 episodes, mean reward 0.570, speed 69.84 f/s\n",
      "641823: done 8170 episodes, mean reward 0.557, speed 68.65 f/s\n",
      "641962: done 8172 episodes, mean reward 0.539, speed 69.61 f/s\n",
      "642228: done 8174 episodes, mean reward 0.540, speed 69.71 f/s\n",
      "642334: done 8176 episodes, mean reward 0.533, speed 69.91 f/s\n",
      "642476: done 8178 episodes, mean reward 0.534, speed 66.35 f/s\n",
      "642754: done 8179 episodes, mean reward 0.541, speed 68.87 f/s\n",
      "642863: done 8180 episodes, mean reward 0.544, speed 70.15 f/s\n",
      "643098: done 8182 episodes, mean reward 0.535, speed 69.61 f/s\n",
      "643338: done 8183 episodes, mean reward 0.533, speed 69.67 f/s\n",
      "643819: done 8184 episodes, mean reward 0.535, speed 68.92 f/s\n",
      "644048: done 8185 episodes, mean reward 0.541, speed 70.19 f/s\n",
      "644137: done 8186 episodes, mean reward 0.538, speed 70.34 f/s\n",
      "644227: done 8187 episodes, mean reward 0.538, speed 66.96 f/s\n",
      "644350: done 8189 episodes, mean reward 0.515, speed 70.27 f/s\n",
      "644654: done 8191 episodes, mean reward 0.506, speed 68.44 f/s\n",
      "644973: done 8193 episodes, mean reward 0.505, speed 69.32 f/s\n",
      "645184: done 8195 episodes, mean reward 0.492, speed 69.03 f/s\n",
      "645274: done 8196 episodes, mean reward 0.488, speed 70.94 f/s\n",
      "645384: done 8197 episodes, mean reward 0.483, speed 67.61 f/s\n",
      "645752: done 8199 episodes, mean reward 0.484, speed 68.08 f/s\n",
      "645858: done 8200 episodes, mean reward 0.481, speed 70.00 f/s\n",
      "646169: done 8202 episodes, mean reward 0.484, speed 68.54 f/s\n",
      "646601: done 8203 episodes, mean reward 0.493, speed 69.11 f/s\n",
      "647335: done 8204 episodes, mean reward 0.503, speed 69.02 f/s\n",
      "647519: done 8205 episodes, mean reward 0.501, speed 68.97 f/s\n",
      "648520: done 8206 episodes, mean reward 0.520, speed 68.98 f/s\n",
      "648995: done 8208 episodes, mean reward 0.526, speed 68.84 f/s\n",
      "649142: done 8209 episodes, mean reward 0.529, speed 70.84 f/s\n",
      "649815: done 8210 episodes, mean reward 0.546, speed 67.75 f/s\n",
      "649905: done 8211 episodes, mean reward 0.546, speed 67.58 f/s\n",
      "Test done in 53.52 sec, reward 1.680, steps 652\n",
      "650224: done 8213 episodes, mean reward 0.547, speed 5.48 f/s\n",
      "650314: done 8214 episodes, mean reward 0.549, speed 67.96 f/s\n",
      "650415: done 8215 episodes, mean reward 0.548, speed 68.55 f/s\n",
      "650782: done 8217 episodes, mean reward 0.549, speed 69.72 f/s\n",
      "651187: done 8219 episodes, mean reward 0.540, speed 67.63 f/s\n",
      "651520: done 8220 episodes, mean reward 0.547, speed 69.43 f/s\n",
      "651664: done 8221 episodes, mean reward 0.548, speed 73.37 f/s\n",
      "651862: done 8223 episodes, mean reward 0.547, speed 68.61 f/s\n",
      "651970: done 8224 episodes, mean reward 0.538, speed 68.61 f/s\n",
      "652267: done 8226 episodes, mean reward 0.535, speed 69.67 f/s\n",
      "652423: done 8227 episodes, mean reward 0.537, speed 69.17 f/s\n",
      "652832: done 8228 episodes, mean reward 0.539, speed 69.26 f/s\n",
      "652909: done 8230 episodes, mean reward 0.528, speed 67.34 f/s\n",
      "653129: done 8231 episodes, mean reward 0.531, speed 72.11 f/s\n",
      "653835: done 8232 episodes, mean reward 0.547, speed 70.30 f/s\n",
      "654259: done 8233 episodes, mean reward 0.558, speed 68.29 f/s\n",
      "654474: done 8234 episodes, mean reward 0.538, speed 70.11 f/s\n",
      "654612: done 8235 episodes, mean reward 0.540, speed 71.65 f/s\n",
      "654835: done 8236 episodes, mean reward 0.534, speed 70.06 f/s\n",
      "655053: done 8237 episodes, mean reward 0.538, speed 67.46 f/s\n",
      "655136: done 8238 episodes, mean reward 0.537, speed 69.03 f/s\n",
      "655296: done 8239 episodes, mean reward 0.539, speed 66.82 f/s\n",
      "655747: done 8240 episodes, mean reward 0.545, speed 68.09 f/s\n",
      "655891: done 8241 episodes, mean reward 0.544, speed 72.13 f/s\n",
      "656032: done 8243 episodes, mean reward 0.542, speed 69.39 f/s\n",
      "656119: done 8244 episodes, mean reward 0.534, speed 67.99 f/s\n",
      "656576: done 8246 episodes, mean reward 0.539, speed 69.03 f/s\n",
      "656822: done 8248 episodes, mean reward 0.527, speed 68.88 f/s\n",
      "657371: done 8250 episodes, mean reward 0.531, speed 69.40 f/s\n",
      "657461: done 8251 episodes, mean reward 0.526, speed 73.47 f/s\n",
      "657564: done 8252 episodes, mean reward 0.525, speed 69.26 f/s\n",
      "657645: done 8254 episodes, mean reward 0.500, speed 67.07 f/s\n",
      "657972: done 8255 episodes, mean reward 0.491, speed 69.10 f/s\n",
      "658965: done 8257 episodes, mean reward 0.500, speed 68.27 f/s\n",
      "659729: done 8258 episodes, mean reward 0.518, speed 68.15 f/s\n",
      "Test done in 45.76 sec, reward 1.434, steps 556\n",
      "660023: done 8259 episodes, mean reward 0.519, speed 5.88 f/s\n",
      "661004: done 8260 episodes, mean reward 0.544, speed 68.08 f/s\n",
      "661259: done 8261 episodes, mean reward 0.544, speed 69.19 f/s\n",
      "661406: done 8262 episodes, mean reward 0.532, speed 70.28 f/s\n",
      "661507: done 8263 episodes, mean reward 0.533, speed 65.73 f/s\n",
      "661867: done 8264 episodes, mean reward 0.542, speed 68.39 f/s\n",
      "662293: done 8265 episodes, mean reward 0.538, speed 69.05 f/s\n",
      "662820: done 8266 episodes, mean reward 0.550, speed 68.63 f/s\n",
      "662983: done 8268 episodes, mean reward 0.525, speed 70.21 f/s\n",
      "663110: done 8269 episodes, mean reward 0.517, speed 68.59 f/s\n",
      "663312: done 8270 episodes, mean reward 0.519, speed 67.84 f/s\n",
      "663441: done 8271 episodes, mean reward 0.522, speed 72.06 f/s\n",
      "663897: done 8273 episodes, mean reward 0.530, speed 69.49 f/s\n",
      "664898: done 8274 episodes, mean reward 0.550, speed 70.44 f/s\n",
      "664975: done 8275 episodes, mean reward 0.552, speed 68.45 f/s\n",
      "665386: done 8276 episodes, mean reward 0.561, speed 68.28 f/s\n",
      "665589: done 8277 episodes, mean reward 0.565, speed 66.75 f/s\n",
      "665736: done 8278 episodes, mean reward 0.567, speed 66.65 f/s\n",
      "665844: done 8279 episodes, mean reward 0.562, speed 66.67 f/s\n",
      "666873: done 8282 episodes, mean reward 0.580, speed 68.87 f/s\n",
      "667161: done 8283 episodes, mean reward 0.581, speed 68.18 f/s\n",
      "667357: done 8284 episodes, mean reward 0.574, speed 71.83 f/s\n",
      "667712: done 8285 episodes, mean reward 0.577, speed 68.67 f/s\n",
      "667797: done 8287 episodes, mean reward 0.575, speed 69.72 f/s\n",
      "667962: done 8288 episodes, mean reward 0.579, speed 66.90 f/s\n",
      "668215: done 8289 episodes, mean reward 0.582, speed 68.52 f/s\n",
      "668851: done 8290 episodes, mean reward 0.598, speed 67.29 f/s\n",
      "669185: done 8291 episodes, mean reward 0.600, speed 69.89 f/s\n",
      "669321: done 8292 episodes, mean reward 0.603, speed 67.67 f/s\n",
      "669805: done 8294 episodes, mean reward 0.607, speed 69.41 f/s\n",
      "Test done in 82.80 sec, reward 2.633, steps 1001\n",
      "670026: done 8296 episodes, mean reward 0.607, speed 2.57 f/s\n",
      "670172: done 8297 episodes, mean reward 0.608, speed 69.00 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670267: done 8299 episodes, mean reward 0.600, speed 68.95 f/s\n",
      "670555: done 8300 episodes, mean reward 0.605, speed 68.07 f/s\n",
      "671186: done 8301 episodes, mean reward 0.621, speed 69.18 f/s\n",
      "671288: done 8302 episodes, mean reward 0.616, speed 69.30 f/s\n",
      "672289: done 8303 episodes, mean reward 0.631, speed 69.45 f/s\n",
      "672450: done 8304 episodes, mean reward 0.617, speed 67.60 f/s\n",
      "672558: done 8305 episodes, mean reward 0.615, speed 67.58 f/s\n",
      "672837: done 8306 episodes, mean reward 0.596, speed 69.37 f/s\n",
      "672979: done 8307 episodes, mean reward 0.598, speed 70.61 f/s\n",
      "673569: done 8309 episodes, mean reward 0.599, speed 69.18 f/s\n",
      "674206: done 8311 episodes, mean reward 0.595, speed 70.44 f/s\n",
      "675007: done 8313 episodes, mean reward 0.607, speed 69.45 f/s\n",
      "675266: done 8314 episodes, mean reward 0.611, speed 69.13 f/s\n",
      "675352: done 8315 episodes, mean reward 0.611, speed 68.89 f/s\n",
      "675555: done 8317 episodes, mean reward 0.607, speed 67.61 f/s\n",
      "675822: done 8318 episodes, mean reward 0.613, speed 67.93 f/s\n",
      "676616: done 8321 episodes, mean reward 0.611, speed 70.36 f/s\n",
      "677367: done 8322 episodes, mean reward 0.631, speed 69.60 f/s\n",
      "678368: done 8323 episodes, mean reward 0.652, speed 69.29 f/s\n",
      "678871: done 8324 episodes, mean reward 0.663, speed 69.52 f/s\n",
      "678970: done 8325 episodes, mean reward 0.665, speed 72.29 f/s\n",
      "679088: done 8327 episodes, mean reward 0.658, speed 67.91 f/s\n",
      "679227: done 8329 episodes, mean reward 0.649, speed 69.53 f/s\n",
      "679545: done 8330 episodes, mean reward 0.657, speed 69.31 f/s\n",
      "679685: done 8331 episodes, mean reward 0.654, speed 71.53 f/s\n",
      "679785: done 8333 episodes, mean reward 0.627, speed 73.21 f/s\n",
      "Test done in 74.38 sec, reward 2.373, steps 903\n",
      "680316: done 8334 episodes, mean reward 0.635, speed 6.46 f/s\n",
      "680898: done 8335 episodes, mean reward 0.646, speed 69.57 f/s\n",
      "681207: done 8336 episodes, mean reward 0.649, speed 68.06 f/s\n",
      "681614: done 8339 episodes, mean reward 0.648, speed 70.23 f/s\n",
      "681742: done 8340 episodes, mean reward 0.639, speed 68.67 f/s\n",
      "681838: done 8342 episodes, mean reward 0.638, speed 68.97 f/s\n",
      "682221: done 8343 episodes, mean reward 0.645, speed 68.62 f/s\n",
      "682316: done 8344 episodes, mean reward 0.645, speed 68.34 f/s\n",
      "682406: done 8346 episodes, mean reward 0.636, speed 70.60 f/s\n",
      "682628: done 8347 episodes, mean reward 0.641, speed 71.35 f/s\n",
      "682876: done 8349 episodes, mean reward 0.641, speed 69.02 f/s\n",
      "683112: done 8353 episodes, mean reward 0.629, speed 68.54 f/s\n",
      "683411: done 8354 episodes, mean reward 0.636, speed 68.28 f/s\n",
      "683514: done 8355 episodes, mean reward 0.630, speed 72.54 f/s\n",
      "683658: done 8356 episodes, mean reward 0.633, speed 70.43 f/s\n",
      "683937: done 8357 episodes, mean reward 0.615, speed 69.99 f/s\n",
      "684225: done 8359 episodes, mean reward 0.595, speed 69.58 f/s\n",
      "684581: done 8360 episodes, mean reward 0.578, speed 68.23 f/s\n",
      "684659: done 8362 episodes, mean reward 0.569, speed 65.96 f/s\n",
      "684768: done 8363 episodes, mean reward 0.569, speed 68.28 f/s\n",
      "685388: done 8364 episodes, mean reward 0.576, speed 68.52 f/s\n",
      "685759: done 8365 episodes, mean reward 0.574, speed 70.46 f/s\n",
      "685846: done 8366 episodes, mean reward 0.562, speed 69.25 f/s\n",
      "686306: done 8368 episodes, mean reward 0.570, speed 70.18 f/s\n",
      "686450: done 8369 episodes, mean reward 0.571, speed 70.59 f/s\n",
      "686600: done 8371 episodes, mean reward 0.566, speed 69.83 f/s\n",
      "686702: done 8372 episodes, mean reward 0.567, speed 71.08 f/s\n",
      "687246: done 8374 episodes, mean reward 0.544, speed 69.84 f/s\n",
      "687468: done 8375 episodes, mean reward 0.548, speed 67.32 f/s\n",
      "687867: done 8377 episodes, mean reward 0.542, speed 69.81 f/s\n",
      "688159: done 8378 episodes, mean reward 0.545, speed 68.97 f/s\n",
      "688363: done 8380 episodes, mean reward 0.547, speed 69.68 f/s\n",
      "688526: done 8381 episodes, mean reward 0.551, speed 70.32 f/s\n",
      "688604: done 8383 episodes, mean reward 0.520, speed 68.58 f/s\n",
      "688689: done 8384 episodes, mean reward 0.517, speed 67.92 f/s\n",
      "688887: done 8385 episodes, mean reward 0.513, speed 67.87 f/s\n",
      "689286: done 8387 episodes, mean reward 0.521, speed 67.86 f/s\n",
      "Test done in 79.18 sec, reward 2.528, steps 961\n",
      "690437: done 8388 episodes, mean reward 0.546, speed 11.99 f/s\n",
      "690744: done 8390 episodes, mean reward 0.531, speed 68.64 f/s\n",
      "691013: done 8392 episodes, mean reward 0.526, speed 69.43 f/s\n",
      "691605: done 8393 episodes, mean reward 0.541, speed 69.26 f/s\n",
      "691772: done 8395 episodes, mean reward 0.532, speed 67.79 f/s\n",
      "691891: done 8397 episodes, mean reward 0.526, speed 69.28 f/s\n",
      "692398: done 8398 episodes, mean reward 0.539, speed 69.27 f/s\n",
      "692742: done 8399 episodes, mean reward 0.546, speed 69.27 f/s\n",
      "693117: done 8400 episodes, mean reward 0.548, speed 71.80 f/s\n",
      "693281: done 8401 episodes, mean reward 0.536, speed 69.62 f/s\n",
      "693706: done 8403 episodes, mean reward 0.518, speed 70.79 f/s\n",
      "694080: done 8404 episodes, mean reward 0.523, speed 66.84 f/s\n",
      "694217: done 8406 episodes, mean reward 0.517, speed 68.44 f/s\n",
      "694382: done 8407 episodes, mean reward 0.517, speed 68.05 f/s\n",
      "694643: done 8408 episodes, mean reward 0.524, speed 71.48 f/s\n",
      "694850: done 8410 episodes, mean reward 0.513, speed 68.95 f/s\n",
      "695028: done 8411 episodes, mean reward 0.502, speed 68.05 f/s\n",
      "696029: done 8412 episodes, mean reward 0.528, speed 68.83 f/s\n",
      "696425: done 8413 episodes, mean reward 0.519, speed 69.51 f/s\n",
      "696516: done 8414 episodes, mean reward 0.514, speed 69.26 f/s\n",
      "696719: done 8415 episodes, mean reward 0.517, speed 65.39 f/s\n",
      "696911: done 8417 episodes, mean reward 0.517, speed 68.14 f/s\n",
      "697666: done 8420 episodes, mean reward 0.529, speed 68.84 f/s\n",
      "697878: done 8423 episodes, mean reward 0.469, speed 70.62 f/s\n",
      "698152: done 8424 episodes, mean reward 0.463, speed 68.84 f/s\n",
      "698462: done 8425 episodes, mean reward 0.468, speed 70.25 f/s\n",
      "699086: done 8426 episodes, mean reward 0.484, speed 69.40 f/s\n",
      "699251: done 8427 episodes, mean reward 0.486, speed 70.07 f/s\n",
      "699668: done 8429 episodes, mean reward 0.494, speed 70.21 f/s\n",
      "699961: done 8430 episodes, mean reward 0.493, speed 69.65 f/s\n",
      "Test done in 76.74 sec, reward 2.445, steps 930\n",
      "700227: done 8431 episodes, mean reward 0.497, speed 3.30 f/s\n",
      "701206: done 8433 episodes, mean reward 0.520, speed 68.66 f/s\n",
      "701335: done 8434 episodes, mean reward 0.509, speed 67.94 f/s\n",
      "701553: done 8435 episodes, mean reward 0.500, speed 67.86 f/s\n",
      "701833: done 8436 episodes, mean reward 0.499, speed 68.92 f/s\n",
      "701923: done 8437 episodes, mean reward 0.501, speed 69.02 f/s\n",
      "702310: done 8438 episodes, mean reward 0.510, speed 68.05 f/s\n",
      "702557: done 8439 episodes, mean reward 0.507, speed 71.42 f/s\n",
      "702793: done 8440 episodes, mean reward 0.510, speed 68.95 f/s\n",
      "703067: done 8442 episodes, mean reward 0.514, speed 67.14 f/s\n",
      "703136: done 8443 episodes, mean reward 0.506, speed 67.63 f/s\n",
      "703345: done 8446 episodes, mean reward 0.507, speed 68.28 f/s\n",
      "704084: done 8448 episodes, mean reward 0.519, speed 67.65 f/s\n",
      "704344: done 8449 episodes, mean reward 0.520, speed 68.82 f/s\n",
      "704452: done 8450 episodes, mean reward 0.522, speed 68.41 f/s\n",
      "705453: done 8451 episodes, mean reward 0.547, speed 68.61 f/s\n",
      "705753: done 8452 episodes, mean reward 0.555, speed 68.03 f/s\n",
      "706754: done 8453 episodes, mean reward 0.577, speed 69.41 f/s\n",
      "706950: done 8454 episodes, mean reward 0.574, speed 71.11 f/s\n",
      "707215: done 8456 episodes, mean reward 0.574, speed 69.62 f/s\n",
      "707297: done 8458 episodes, mean reward 0.569, speed 68.34 f/s\n",
      "707540: done 8459 episodes, mean reward 0.568, speed 70.48 f/s\n",
      "708119: done 8460 episodes, mean reward 0.574, speed 70.44 f/s\n",
      "708371: done 8461 episodes, mean reward 0.579, speed 68.67 f/s\n",
      "708594: done 8462 episodes, mean reward 0.584, speed 70.29 f/s\n",
      "708892: done 8463 episodes, mean reward 0.589, speed 69.17 f/s\n",
      "709609: done 8464 episodes, mean reward 0.592, speed 69.65 f/s\n",
      "709794: done 8465 episodes, mean reward 0.587, speed 66.56 f/s\n",
      "709907: done 8467 episodes, mean reward 0.587, speed 69.21 f/s\n",
      "Test done in 66.21 sec, reward 2.086, steps 800\n",
      "710016: done 8469 episodes, mean reward 0.574, speed 1.61 f/s\n",
      "710352: done 8470 episodes, mean reward 0.582, speed 71.31 f/s\n",
      "710773: done 8471 episodes, mean reward 0.590, speed 69.08 f/s\n",
      "710900: done 8472 episodes, mean reward 0.591, speed 70.73 f/s\n",
      "711189: done 8474 episodes, mean reward 0.585, speed 70.62 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711426: done 8475 episodes, mean reward 0.586, speed 67.98 f/s\n",
      "711836: done 8478 episodes, mean reward 0.578, speed 69.27 f/s\n",
      "712135: done 8482 episodes, mean reward 0.576, speed 69.27 f/s\n",
      "712503: done 8483 episodes, mean reward 0.584, speed 68.63 f/s\n",
      "712888: done 8484 episodes, mean reward 0.592, speed 68.59 f/s\n",
      "712985: done 8486 episodes, mean reward 0.588, speed 67.98 f/s\n",
      "713075: done 8487 episodes, mean reward 0.581, speed 66.20 f/s\n",
      "713825: done 8488 episodes, mean reward 0.572, speed 69.99 f/s\n",
      "714267: done 8489 episodes, mean reward 0.583, speed 67.58 f/s\n",
      "714350: done 8491 episodes, mean reward 0.576, speed 67.94 f/s\n",
      "714447: done 8493 episodes, mean reward 0.557, speed 72.19 f/s\n",
      "715448: done 8494 episodes, mean reward 0.582, speed 68.69 f/s\n",
      "715601: done 8495 episodes, mean reward 0.584, speed 71.74 f/s\n",
      "715690: done 8496 episodes, mean reward 0.585, speed 68.67 f/s\n",
      "716492: done 8497 episodes, mean reward 0.604, speed 69.07 f/s\n",
      "716670: done 8498 episodes, mean reward 0.595, speed 69.12 f/s\n",
      "716779: done 8499 episodes, mean reward 0.589, speed 67.60 f/s\n",
      "716879: done 8500 episodes, mean reward 0.582, speed 65.31 f/s\n",
      "717159: done 8501 episodes, mean reward 0.585, speed 0.03 f/s\n",
      "717922: done 8502 episodes, mean reward 0.605, speed 67.90 f/s\n",
      "718085: done 8504 episodes, mean reward 0.589, speed 69.54 f/s\n",
      "718251: done 8505 episodes, mean reward 0.592, speed 68.29 f/s\n",
      "718412: done 8506 episodes, mean reward 0.593, speed 69.40 f/s\n",
      "718844: done 8507 episodes, mean reward 0.600, speed 68.09 f/s\n",
      "719215: done 8508 episodes, mean reward 0.603, speed 68.28 f/s\n",
      "719959: done 8509 episodes, mean reward 0.621, speed 66.79 f/s\n",
      "Test done in 69.26 sec, reward 2.194, steps 836\n",
      "720073: done 8510 episodes, mean reward 0.621, speed 1.61 f/s\n",
      "720657: done 8511 episodes, mean reward 0.632, speed 68.32 f/s\n",
      "720875: done 8512 episodes, mean reward 0.611, speed 67.01 f/s\n",
      "720980: done 8513 episodes, mean reward 0.603, speed 69.07 f/s\n",
      "721717: done 8516 episodes, mean reward 0.614, speed 69.39 f/s\n",
      "722320: done 8517 episodes, mean reward 0.626, speed 68.48 f/s\n",
      "722521: done 8518 episodes, mean reward 0.631, speed 66.72 f/s\n",
      "723037: done 8520 episodes, mean reward 0.625, speed 69.21 f/s\n",
      "723278: done 8521 episodes, mean reward 0.630, speed 68.61 f/s\n",
      "723444: done 8522 episodes, mean reward 0.634, speed 68.82 f/s\n",
      "724445: done 8523 episodes, mean reward 0.657, speed 68.93 f/s\n",
      "724654: done 8526 episodes, mean reward 0.630, speed 70.94 f/s\n",
      "725390: done 8527 episodes, mean reward 0.645, speed 68.69 f/s\n",
      "725791: done 8529 episodes, mean reward 0.645, speed 68.93 f/s\n",
      "726614: done 8530 episodes, mean reward 0.659, speed 69.85 f/s\n",
      "726856: done 8531 episodes, mean reward 0.658, speed 68.72 f/s\n",
      "727399: done 8532 episodes, mean reward 0.671, speed 68.49 f/s\n",
      "727773: done 8533 episodes, mean reward 0.657, speed 69.44 f/s\n",
      "727862: done 8534 episodes, mean reward 0.656, speed 69.87 f/s\n",
      "728090: done 8536 episodes, mean reward 0.649, speed 68.30 f/s\n",
      "728724: done 8537 episodes, mean reward 0.663, speed 70.81 f/s\n",
      "728827: done 8538 episodes, mean reward 0.656, speed 70.69 f/s\n",
      "729543: done 8539 episodes, mean reward 0.668, speed 69.17 f/s\n",
      "729822: done 8540 episodes, mean reward 0.669, speed 69.03 f/s\n",
      "Test done in 82.89 sec, reward 2.634, steps 1001\n",
      "730490: done 8541 episodes, mean reward 0.686, speed 7.21 f/s\n",
      "730631: done 8542 episodes, mean reward 0.683, speed 69.07 f/s\n",
      "731217: done 8543 episodes, mean reward 0.696, speed 68.78 f/s\n",
      "731411: done 8546 episodes, mean reward 0.696, speed 69.69 f/s\n",
      "731514: done 8547 episodes, mean reward 0.698, speed 69.17 f/s\n",
      "731631: done 8548 episodes, mean reward 0.683, speed 71.30 f/s\n",
      "731758: done 8549 episodes, mean reward 0.679, speed 69.02 f/s\n",
      "732263: done 8550 episodes, mean reward 0.690, speed 69.24 f/s\n",
      "732370: done 8551 episodes, mean reward 0.667, speed 69.86 f/s\n",
      "733416: done 8553 episodes, mean reward 0.659, speed 69.28 f/s\n",
      "734020: done 8554 episodes, mean reward 0.670, speed 67.70 f/s\n",
      "734318: done 8555 episodes, mean reward 0.677, speed 70.55 f/s\n",
      "734437: done 8557 episodes, mean reward 0.673, speed 68.80 f/s\n",
      "734532: done 8558 episodes, mean reward 0.674, speed 67.16 f/s\n",
      "734875: done 8559 episodes, mean reward 0.677, speed 67.84 f/s\n",
      "735029: done 8560 episodes, mean reward 0.666, speed 69.78 f/s\n",
      "735121: done 8563 episodes, mean reward 0.649, speed 70.55 f/s\n",
      "735301: done 8564 episodes, mean reward 0.634, speed 68.66 f/s\n",
      "735429: done 8565 episodes, mean reward 0.633, speed 70.78 f/s\n",
      "735531: done 8566 episodes, mean reward 0.634, speed 70.64 f/s\n",
      "736249: done 8567 episodes, mean reward 0.652, speed 68.96 f/s\n",
      "736409: done 8568 episodes, mean reward 0.655, speed 69.47 f/s\n",
      "736773: done 8570 episodes, mean reward 0.653, speed 69.16 f/s\n",
      "736947: done 8572 episodes, mean reward 0.643, speed 68.42 f/s\n",
      "737258: done 8574 episodes, mean reward 0.643, speed 68.71 f/s\n",
      "738289: done 8576 episodes, mean reward 0.664, speed 69.16 f/s\n",
      "738433: done 8577 episodes, mean reward 0.667, speed 69.33 f/s\n",
      "738826: done 8578 episodes, mean reward 0.668, speed 68.57 f/s\n",
      "739004: done 8579 episodes, mean reward 0.673, speed 67.53 f/s\n",
      "Test done in 80.30 sec, reward 2.567, steps 976\n",
      "740259: done 8580 episodes, mean reward 0.704, speed 12.72 f/s\n",
      "740469: done 8581 episodes, mean reward 0.708, speed 69.14 f/s\n",
      "740606: done 8583 episodes, mean reward 0.696, speed 69.86 f/s\n",
      "740752: done 8584 episodes, mean reward 0.690, speed 66.96 f/s\n",
      "740842: done 8585 episodes, mean reward 0.692, speed 68.60 f/s\n",
      "741115: done 8586 episodes, mean reward 0.697, speed 68.66 f/s\n",
      "742116: done 8587 episodes, mean reward 0.721, speed 69.13 f/s\n",
      "742514: done 8588 episodes, mean reward 0.712, speed 67.94 f/s\n",
      "742632: done 8589 episodes, mean reward 0.704, speed 69.53 f/s\n",
      "742797: done 8590 episodes, mean reward 0.708, speed 67.98 f/s\n",
      "743171: done 8591 episodes, mean reward 0.716, speed 68.45 f/s\n",
      "744088: done 8592 episodes, mean reward 0.739, speed 69.72 f/s\n",
      "744218: done 8594 episodes, mean reward 0.715, speed 67.82 f/s\n",
      "744454: done 8596 episodes, mean reward 0.714, speed 69.08 f/s\n",
      "745075: done 8597 episodes, mean reward 0.710, speed 67.89 f/s\n",
      "745157: done 8599 episodes, mean reward 0.704, speed 66.16 f/s\n",
      "745755: done 8600 episodes, mean reward 0.718, speed 70.21 f/s\n",
      "745855: done 8601 episodes, mean reward 0.713, speed 68.41 f/s\n",
      "745970: done 8603 episodes, mean reward 0.695, speed 69.32 f/s\n",
      "746259: done 8604 episodes, mean reward 0.699, speed 68.06 f/s\n",
      "746559: done 8605 episodes, mean reward 0.703, speed 68.76 f/s\n",
      "746845: done 8606 episodes, mean reward 0.706, speed 69.93 f/s\n",
      "747287: done 8607 episodes, mean reward 0.706, speed 68.58 f/s\n",
      "747593: done 8608 episodes, mean reward 0.704, speed 68.94 f/s\n",
      "747661: done 8609 episodes, mean reward 0.686, speed 67.82 f/s\n",
      "747750: done 8611 episodes, mean reward 0.669, speed 67.80 f/s\n",
      "748171: done 8613 episodes, mean reward 0.672, speed 69.93 f/s\n",
      "748777: done 8615 episodes, mean reward 0.687, speed 69.62 f/s\n",
      "749189: done 8616 episodes, mean reward 0.679, speed 69.05 f/s\n",
      "749402: done 8617 episodes, mean reward 0.669, speed 69.31 f/s\n",
      "749565: done 8618 episodes, mean reward 0.668, speed 69.71 f/s\n",
      "749644: done 8620 episodes, mean reward 0.656, speed 67.73 f/s\n",
      "Test done in 60.52 sec, reward 1.920, steps 735\n",
      "750071: done 8621 episodes, mean reward 0.660, speed 6.39 f/s\n",
      "750223: done 8623 episodes, mean reward 0.633, speed 67.39 f/s\n",
      "750693: done 8624 episodes, mean reward 0.645, speed 69.94 f/s\n",
      "750766: done 8626 episodes, mean reward 0.642, speed 67.41 f/s\n",
      "751078: done 8627 episodes, mean reward 0.631, speed 68.05 f/s\n",
      "751208: done 8628 episodes, mean reward 0.634, speed 70.20 f/s\n",
      "751311: done 8630 episodes, mean reward 0.605, speed 69.87 f/s\n",
      "751508: done 8631 episodes, mean reward 0.604, speed 68.22 f/s\n",
      "751590: done 8633 episodes, mean reward 0.582, speed 69.49 f/s\n",
      "751681: done 8634 episodes, mean reward 0.582, speed 71.52 f/s\n",
      "752139: done 8636 episodes, mean reward 0.588, speed 70.88 f/s\n",
      "752240: done 8638 episodes, mean reward 0.571, speed 65.50 f/s\n",
      "752474: done 8640 episodes, mean reward 0.550, speed 68.94 f/s\n",
      "752551: done 8643 episodes, mean reward 0.516, speed 71.74 f/s\n",
      "752656: done 8644 episodes, mean reward 0.518, speed 70.92 f/s\n",
      "753309: done 8647 episodes, mean reward 0.528, speed 68.28 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753722: done 8648 episodes, mean reward 0.536, speed 69.93 f/s\n",
      "753981: done 8650 episodes, mean reward 0.526, speed 70.14 f/s\n",
      "754531: done 8652 episodes, mean reward 0.536, speed 69.01 f/s\n",
      "755080: done 8654 episodes, mean reward 0.509, speed 69.78 f/s\n",
      "755363: done 8656 episodes, mean reward 0.507, speed 70.28 f/s\n",
      "755450: done 8657 episodes, mean reward 0.507, speed 71.12 f/s\n",
      "755768: done 8658 episodes, mean reward 0.513, speed 68.43 f/s\n",
      "755984: done 8659 episodes, mean reward 0.509, speed 69.42 f/s\n",
      "756515: done 8661 episodes, mean reward 0.518, speed 68.83 f/s\n",
      "756880: done 8663 episodes, mean reward 0.525, speed 69.42 f/s\n",
      "756986: done 8664 episodes, mean reward 0.524, speed 69.09 f/s\n",
      "757768: done 8666 episodes, mean reward 0.538, speed 70.20 f/s\n",
      "758040: done 8667 episodes, mean reward 0.525, speed 70.40 f/s\n",
      "758186: done 8668 episodes, mean reward 0.525, speed 69.09 f/s\n",
      "758289: done 8669 episodes, mean reward 0.527, speed 69.54 f/s\n",
      "758444: done 8670 episodes, mean reward 0.524, speed 67.43 f/s\n",
      "758628: done 8671 episodes, mean reward 0.527, speed 70.29 f/s\n",
      "758712: done 8672 episodes, mean reward 0.526, speed 68.96 f/s\n",
      "758814: done 8673 episodes, mean reward 0.528, speed 67.84 f/s\n",
      "759020: done 8676 episodes, mean reward 0.499, speed 69.65 f/s\n",
      "759561: done 8677 episodes, mean reward 0.510, speed 69.47 f/s\n",
      "759745: done 8678 episodes, mean reward 0.504, speed 70.56 f/s\n",
      "Test done in 62.29 sec, reward 1.978, steps 756\n",
      "760264: done 8679 episodes, mean reward 0.513, speed 7.43 f/s\n",
      "760391: done 8680 episodes, mean reward 0.484, speed 68.08 f/s\n",
      "760772: done 8681 episodes, mean reward 0.490, speed 71.41 f/s\n",
      "760958: done 8682 episodes, mean reward 0.493, speed 70.00 f/s\n",
      "761250: done 8684 episodes, mean reward 0.495, speed 68.62 f/s\n",
      "761502: done 8685 episodes, mean reward 0.500, speed 68.65 f/s\n",
      "761952: done 8686 episodes, mean reward 0.504, speed 68.58 f/s\n",
      "762264: done 8687 episodes, mean reward 0.486, speed 67.98 f/s\n",
      "762577: done 8689 episodes, mean reward 0.480, speed 67.80 f/s\n",
      "762820: done 8690 episodes, mean reward 0.482, speed 70.21 f/s\n",
      "763476: done 8691 episodes, mean reward 0.490, speed 69.51 f/s\n",
      "763905: done 8693 episodes, mean reward 0.476, speed 69.20 f/s\n",
      "764260: done 8694 episodes, mean reward 0.483, speed 69.35 f/s\n",
      "764521: done 8695 episodes, mean reward 0.490, speed 69.79 f/s\n",
      "764775: done 8696 episodes, mean reward 0.490, speed 69.67 f/s\n",
      "764878: done 8697 episodes, mean reward 0.477, speed 66.10 f/s\n",
      "765209: done 8698 episodes, mean reward 0.485, speed 68.57 f/s\n",
      "765337: done 8699 episodes, mean reward 0.487, speed 68.37 f/s\n",
      "766338: done 8700 episodes, mean reward 0.497, speed 69.99 f/s\n",
      "766628: done 8701 episodes, mean reward 0.502, speed 68.17 f/s\n",
      "766995: done 8702 episodes, mean reward 0.511, speed 67.42 f/s\n",
      "767438: done 8704 episodes, mean reward 0.513, speed 68.94 f/s\n",
      "768130: done 8706 episodes, mean reward 0.516, speed 68.10 f/s\n",
      "768367: done 8707 episodes, mean reward 0.511, speed 68.03 f/s\n",
      "768698: done 8709 episodes, mean reward 0.510, speed 69.29 f/s\n",
      "768844: done 8710 episodes, mean reward 0.513, speed 71.03 f/s\n",
      "769009: done 8711 episodes, mean reward 0.516, speed 69.65 f/s\n",
      "769427: done 8713 episodes, mean reward 0.516, speed 69.13 f/s\n",
      "Test done in 65.90 sec, reward 2.091, steps 798\n",
      "770242: done 8714 episodes, mean reward 0.537, speed 10.47 f/s\n",
      "770480: done 8715 episodes, mean reward 0.528, speed 67.47 f/s\n",
      "770720: done 8716 episodes, mean reward 0.523, speed 69.37 f/s\n",
      "770869: done 8717 episodes, mean reward 0.522, speed 67.95 f/s\n",
      "771469: done 8718 episodes, mean reward 0.533, speed 68.68 f/s\n",
      "771969: done 8719 episodes, mean reward 0.545, speed 68.96 f/s\n",
      "772506: done 8720 episodes, mean reward 0.559, speed 67.70 f/s\n",
      "772842: done 8721 episodes, mean reward 0.557, speed 71.68 f/s\n",
      "773283: done 8722 episodes, mean reward 0.568, speed 68.16 f/s\n",
      "773898: done 8723 episodes, mean reward 0.581, speed 68.87 f/s\n",
      "774101: done 8724 episodes, mean reward 0.574, speed 68.05 f/s\n",
      "774341: done 8726 episodes, mean reward 0.579, speed 71.29 f/s\n",
      "775052: done 8727 episodes, mean reward 0.589, speed 68.89 f/s\n",
      "775160: done 8728 episodes, mean reward 0.589, speed 69.43 f/s\n",
      "775688: done 8729 episodes, mean reward 0.601, speed 67.45 f/s\n",
      "775921: done 8731 episodes, mean reward 0.601, speed 68.71 f/s\n",
      "776922: done 8732 episodes, mean reward 0.626, speed 68.41 f/s\n",
      "777847: done 8734 episodes, mean reward 0.647, speed 69.01 f/s\n",
      "778766: done 8736 episodes, mean reward 0.659, speed 69.05 f/s\n",
      "779767: done 8737 episodes, mean reward 0.685, speed 68.02 f/s\n",
      "Test done in 69.19 sec, reward 2.197, steps 838\n",
      "780087: done 8738 episodes, mean reward 0.692, speed 4.32 f/s\n",
      "780299: done 8740 episodes, mean reward 0.692, speed 71.90 f/s\n",
      "780594: done 8741 episodes, mean reward 0.699, speed 68.54 f/s\n",
      "780722: done 8742 episodes, mean reward 0.702, speed 70.15 f/s\n",
      "780833: done 8744 episodes, mean reward 0.702, speed 70.70 f/s\n",
      "781619: done 8745 episodes, mean reward 0.722, speed 68.64 f/s\n",
      "782264: done 8747 episodes, mean reward 0.722, speed 68.35 f/s\n",
      "782467: done 8748 episodes, mean reward 0.717, speed 66.82 f/s\n",
      "782699: done 8750 episodes, mean reward 0.716, speed 67.59 f/s\n",
      "783530: done 8751 episodes, mean reward 0.737, speed 68.35 f/s\n",
      "783989: done 8752 episodes, mean reward 0.736, speed 69.08 f/s\n",
      "784056: done 8753 episodes, mean reward 0.737, speed 65.83 f/s\n",
      "784193: done 8754 episodes, mean reward 0.727, speed 66.73 f/s\n",
      "784676: done 8756 episodes, mean reward 0.732, speed 68.58 f/s\n",
      "785214: done 8757 episodes, mean reward 0.744, speed 69.48 f/s\n",
      "785282: done 8758 episodes, mean reward 0.738, speed 63.83 f/s\n",
      "785382: done 8759 episodes, mean reward 0.735, speed 68.39 f/s\n",
      "785462: done 8761 episodes, mean reward 0.723, speed 66.81 f/s\n",
      "785589: done 8762 episodes, mean reward 0.726, speed 67.65 f/s\n",
      "785861: done 8764 episodes, mean reward 0.721, speed 69.27 f/s\n",
      "786000: done 8765 episodes, mean reward 0.724, speed 68.54 f/s\n",
      "786145: done 8766 episodes, mean reward 0.708, speed 67.25 f/s\n",
      "786463: done 8767 episodes, mean reward 0.710, speed 66.65 f/s\n",
      "786816: done 8768 episodes, mean reward 0.715, speed 67.75 f/s\n",
      "786913: done 8769 episodes, mean reward 0.715, speed 72.05 f/s\n",
      "787002: done 8770 episodes, mean reward 0.713, speed 67.56 f/s\n",
      "787262: done 8771 episodes, mean reward 0.715, speed 70.08 f/s\n",
      "787384: done 8772 episodes, mean reward 0.716, speed 68.02 f/s\n",
      "788021: done 8773 episodes, mean reward 0.730, speed 68.11 f/s\n",
      "788811: done 8774 episodes, mean reward 0.750, speed 69.97 f/s\n",
      "789528: done 8775 episodes, mean reward 0.768, speed 68.51 f/s\n",
      "789715: done 8776 episodes, mean reward 0.770, speed 69.41 f/s\n",
      "789972: done 8777 episodes, mean reward 0.762, speed 68.89 f/s\n",
      "Test done in 60.30 sec, reward 1.915, steps 732\n",
      "790111: done 8778 episodes, mean reward 0.761, speed 2.23 f/s\n",
      "791112: done 8779 episodes, mean reward 0.774, speed 69.03 f/s\n",
      "791339: done 8780 episodes, mean reward 0.776, speed 69.14 f/s\n",
      "791670: done 8781 episodes, mean reward 0.774, speed 69.32 f/s\n",
      "791772: done 8782 episodes, mean reward 0.772, speed 69.90 f/s\n",
      "791842: done 8783 episodes, mean reward 0.772, speed 67.30 f/s\n",
      "792795: done 8785 episodes, mean reward 0.784, speed 68.95 f/s\n",
      "793279: done 8786 episodes, mean reward 0.785, speed 69.86 f/s\n",
      "793900: done 8787 episodes, mean reward 0.793, speed 68.82 f/s\n",
      "794045: done 8788 episodes, mean reward 0.795, speed 69.64 f/s\n",
      "794952: done 8789 episodes, mean reward 0.812, speed 68.94 f/s\n",
      "795621: done 8790 episodes, mean reward 0.823, speed 70.69 f/s\n",
      "795902: done 8791 episodes, mean reward 0.812, speed 68.15 f/s\n",
      "796077: done 8792 episodes, mean reward 0.816, speed 69.14 f/s\n",
      "796224: done 8793 episodes, mean reward 0.810, speed 70.12 f/s\n",
      "796298: done 8794 episodes, mean reward 0.802, speed 67.20 f/s\n",
      "796473: done 8795 episodes, mean reward 0.799, speed 69.44 f/s\n",
      "796924: done 8796 episodes, mean reward 0.805, speed 68.97 f/s\n",
      "797012: done 8797 episodes, mean reward 0.804, speed 69.12 f/s\n",
      "797275: done 8798 episodes, mean reward 0.802, speed 70.59 f/s\n",
      "797389: done 8800 episodes, mean reward 0.775, speed 71.30 f/s\n",
      "798000: done 8801 episodes, mean reward 0.783, speed 68.61 f/s\n",
      "798257: done 8802 episodes, mean reward 0.780, speed 69.33 f/s\n",
      "799091: done 8804 episodes, mean reward 0.790, speed 70.20 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799266: done 8807 episodes, mean reward 0.770, speed 67.73 f/s\n",
      "799578: done 8808 episodes, mean reward 0.778, speed 69.65 f/s\n",
      "Test done in 65.76 sec, reward 2.079, steps 798\n",
      "800115: done 8809 episodes, mean reward 0.782, speed 7.31 f/s\n",
      "800523: done 8810 episodes, mean reward 0.789, speed 69.18 f/s\n",
      "800966: done 8813 episodes, mean reward 0.785, speed 69.04 f/s\n",
      "801152: done 8814 episodes, mean reward 0.768, speed 68.49 f/s\n",
      "801355: done 8815 episodes, mean reward 0.768, speed 69.68 f/s\n",
      "801712: done 8817 episodes, mean reward 0.767, speed 68.13 f/s\n",
      "801925: done 8818 episodes, mean reward 0.756, speed 72.00 f/s\n",
      "802254: done 8819 episodes, mean reward 0.752, speed 72.03 f/s\n",
      "802331: done 8821 episodes, mean reward 0.730, speed 69.97 f/s\n",
      "802488: done 8822 episodes, mean reward 0.723, speed 68.80 f/s\n",
      "803029: done 8824 episodes, mean reward 0.715, speed 68.39 f/s\n",
      "804030: done 8825 episodes, mean reward 0.739, speed 68.70 f/s\n",
      "804223: done 8826 episodes, mean reward 0.738, speed 67.73 f/s\n",
      "804300: done 8829 episodes, mean reward 0.705, speed 72.97 f/s\n",
      "804534: done 8831 episodes, mean reward 0.705, speed 68.53 f/s\n",
      "804700: done 8832 episodes, mean reward 0.683, speed 69.83 f/s\n",
      "804791: done 8833 episodes, mean reward 0.684, speed 69.50 f/s\n",
      "805128: done 8834 episodes, mean reward 0.669, speed 69.11 f/s\n",
      "805237: done 8835 episodes, mean reward 0.672, speed 70.05 f/s\n",
      "805719: done 8836 episodes, mean reward 0.661, speed 68.39 f/s\n",
      "806151: done 8837 episodes, mean reward 0.646, speed 67.91 f/s\n",
      "806275: done 8838 episodes, mean reward 0.640, speed 68.46 f/s\n",
      "806378: done 8840 episodes, mean reward 0.637, speed 69.52 f/s\n",
      "806490: done 8841 episodes, mean reward 0.633, speed 67.22 f/s\n",
      "806672: done 8843 episodes, mean reward 0.634, speed 67.72 f/s\n",
      "806920: done 8844 episodes, mean reward 0.637, speed 68.88 f/s\n",
      "807331: done 8845 episodes, mean reward 0.627, speed 69.72 f/s\n",
      "807753: done 8846 episodes, mean reward 0.637, speed 69.44 f/s\n",
      "808175: done 8848 episodes, mean reward 0.627, speed 67.96 f/s\n",
      "808580: done 8849 episodes, mean reward 0.637, speed 70.18 f/s\n",
      "809102: done 8850 episodes, mean reward 0.646, speed 69.65 f/s\n",
      "809241: done 8852 episodes, mean reward 0.615, speed 68.80 f/s\n",
      "809704: done 8853 episodes, mean reward 0.626, speed 68.54 f/s\n",
      "809789: done 8855 episodes, mean reward 0.625, speed 68.89 f/s\n",
      "Test done in 49.09 sec, reward 1.544, steps 598\n",
      "810066: done 8857 episodes, mean reward 0.605, speed 5.21 f/s\n",
      "810300: done 8859 episodes, mean reward 0.607, speed 68.79 f/s\n",
      "810563: done 8861 episodes, mean reward 0.612, speed 68.53 f/s\n",
      "810742: done 8863 episodes, mean reward 0.612, speed 67.82 f/s\n",
      "811209: done 8864 episodes, mean reward 0.619, speed 68.28 f/s\n",
      "811651: done 8865 episodes, mean reward 0.627, speed 68.57 f/s\n",
      "812174: done 8866 episodes, mean reward 0.637, speed 68.60 f/s\n",
      "812653: done 8867 episodes, mean reward 0.641, speed 68.36 f/s\n",
      "812734: done 8869 episodes, mean reward 0.632, speed 66.39 f/s\n",
      "812911: done 8870 episodes, mean reward 0.634, speed 67.38 f/s\n",
      "813677: done 8871 episodes, mean reward 0.647, speed 69.06 f/s\n",
      "813963: done 8873 episodes, mean reward 0.634, speed 70.08 f/s\n",
      "814506: done 8874 episodes, mean reward 0.628, speed 69.12 f/s\n",
      "814722: done 8875 episodes, mean reward 0.615, speed 68.27 f/s\n",
      "815723: done 8876 episodes, mean reward 0.636, speed 68.90 f/s\n",
      "816020: done 8878 episodes, mean reward 0.634, speed 68.51 f/s\n",
      "816102: done 8880 episodes, mean reward 0.604, speed 67.65 f/s\n",
      "816740: done 8881 episodes, mean reward 0.612, speed 69.81 f/s\n",
      "816824: done 8882 episodes, mean reward 0.612, speed 67.45 f/s\n",
      "816897: done 8884 episodes, mean reward 0.610, speed 72.82 f/s\n",
      "816983: done 8886 episodes, mean reward 0.576, speed 67.84 f/s\n",
      "817233: done 8888 episodes, mean reward 0.562, speed 69.95 f/s\n",
      "818006: done 8890 episodes, mean reward 0.541, speed 68.18 f/s\n",
      "818646: done 8891 episodes, mean reward 0.551, speed 69.06 f/s\n",
      "818913: done 8892 episodes, mean reward 0.553, speed 69.32 f/s\n",
      "819093: done 8893 episodes, mean reward 0.554, speed 69.71 f/s\n",
      "819178: done 8895 episodes, mean reward 0.551, speed 67.86 f/s\n",
      "819466: done 8897 episodes, mean reward 0.544, speed 70.53 f/s\n",
      "819746: done 8898 episodes, mean reward 0.544, speed 67.68 f/s\n",
      "Test done in 62.13 sec, reward 1.970, steps 754\n",
      "820183: done 8899 episodes, mean reward 0.554, speed 6.37 f/s\n",
      "820538: done 8900 episodes, mean reward 0.563, speed 68.33 f/s\n",
      "820864: done 8902 episodes, mean reward 0.549, speed 70.15 f/s\n",
      "821141: done 8904 episodes, mean reward 0.535, speed 69.06 f/s\n",
      "821375: done 8905 episodes, mean reward 0.540, speed 70.28 f/s\n",
      "821768: done 8906 episodes, mean reward 0.550, speed 70.28 f/s\n",
      "822014: done 8908 episodes, mean reward 0.546, speed 70.00 f/s\n",
      "822578: done 8909 episodes, mean reward 0.548, speed 70.61 f/s\n",
      "823064: done 8911 episodes, mean reward 0.549, speed 67.42 f/s\n",
      "823239: done 8913 episodes, mean reward 0.543, speed 69.06 f/s\n",
      "823870: done 8914 episodes, mean reward 0.554, speed 68.59 f/s\n",
      "824775: done 8915 episodes, mean reward 0.572, speed 71.35 f/s\n",
      "824978: done 8916 episodes, mean reward 0.577, speed 69.28 f/s\n",
      "825144: done 8917 episodes, mean reward 0.573, speed 69.24 f/s\n",
      "825271: done 8918 episodes, mean reward 0.571, speed 70.48 f/s\n",
      "826272: done 8919 episodes, mean reward 0.588, speed 69.14 f/s\n",
      "826394: done 8920 episodes, mean reward 0.590, speed 66.82 f/s\n",
      "827456: done 8922 episodes, mean reward 0.614, speed 68.67 f/s\n",
      "827693: done 8923 episodes, mean reward 0.619, speed 68.82 f/s\n",
      "828192: done 8924 episodes, mean reward 0.619, speed 68.23 f/s\n",
      "828269: done 8925 episodes, mean reward 0.595, speed 70.57 f/s\n",
      "828921: done 8927 episodes, mean reward 0.607, speed 69.41 f/s\n",
      "829093: done 8929 episodes, mean reward 0.611, speed 67.07 f/s\n",
      "829296: done 8930 episodes, mean reward 0.615, speed 69.43 f/s\n",
      "829573: done 8931 episodes, mean reward 0.617, speed 69.08 f/s\n",
      "829689: done 8934 episodes, mean reward 0.604, speed 68.72 f/s\n",
      "Test done in 79.19 sec, reward 2.501, steps 952\n",
      "830304: done 8936 episodes, mean reward 0.605, speed 6.99 f/s\n",
      "830527: done 8937 episodes, mean reward 0.599, speed 69.80 f/s\n",
      "830978: done 8938 episodes, mean reward 0.608, speed 69.92 f/s\n",
      "831462: done 8939 episodes, mean reward 0.619, speed 69.21 f/s\n",
      "831602: done 8940 episodes, mean reward 0.621, speed 68.67 f/s\n",
      "831803: done 8941 episodes, mean reward 0.623, speed 69.80 f/s\n",
      "832022: done 8942 episodes, mean reward 0.627, speed 67.90 f/s\n",
      "832427: done 8943 episodes, mean reward 0.634, speed 67.97 f/s\n",
      "832593: done 8944 episodes, mean reward 0.632, speed 69.71 f/s\n",
      "832661: done 8945 episodes, mean reward 0.624, speed 64.38 f/s\n",
      "832943: done 8947 episodes, mean reward 0.620, speed 69.68 f/s\n",
      "833660: done 8948 episodes, mean reward 0.628, speed 69.17 f/s\n",
      "833916: done 8950 episodes, mean reward 0.610, speed 68.14 f/s\n",
      "834006: done 8951 episodes, mean reward 0.611, speed 67.35 f/s\n",
      "834704: done 8952 episodes, mean reward 0.627, speed 70.01 f/s\n",
      "834804: done 8953 episodes, mean reward 0.618, speed 69.50 f/s\n",
      "835136: done 8954 episodes, mean reward 0.626, speed 69.16 f/s\n",
      "835656: done 8955 episodes, mean reward 0.638, speed 68.27 f/s\n",
      "835927: done 8956 episodes, mean reward 0.644, speed 72.02 f/s\n",
      "836226: done 8957 episodes, mean reward 0.645, speed 68.01 f/s\n",
      "836553: done 8958 episodes, mean reward 0.653, speed 68.40 f/s\n",
      "836892: done 8959 episodes, mean reward 0.657, speed 69.59 f/s\n",
      "837109: done 8960 episodes, mean reward 0.661, speed 69.56 f/s\n",
      "837291: done 8961 episodes, mean reward 0.660, speed 66.31 f/s\n",
      "837392: done 8962 episodes, mean reward 0.661, speed 66.74 f/s\n",
      "837536: done 8964 episodes, mean reward 0.649, speed 67.66 f/s\n",
      "838194: done 8965 episodes, mean reward 0.654, speed 68.93 f/s\n",
      "838584: done 8966 episodes, mean reward 0.651, speed 70.67 f/s\n",
      "838709: done 8968 episodes, mean reward 0.641, speed 70.44 f/s\n",
      "838932: done 8969 episodes, mean reward 0.645, speed 69.73 f/s\n",
      "839279: done 8970 episodes, mean reward 0.650, speed 68.81 f/s\n",
      "839671: done 8972 episodes, mean reward 0.639, speed 69.10 f/s\n",
      "839784: done 8973 episodes, mean reward 0.637, speed 67.22 f/s\n",
      "Test done in 76.36 sec, reward 2.432, steps 926\n",
      "840048: done 8975 episodes, mean reward 0.623, speed 3.29 f/s\n",
      "840632: done 8976 episodes, mean reward 0.612, speed 69.12 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840915: done 8977 episodes, mean reward 0.618, speed 69.45 f/s\n",
      "841054: done 8979 episodes, mean reward 0.614, speed 71.12 f/s\n",
      "842055: done 8980 episodes, mean reward 0.639, speed 67.82 f/s\n",
      "842284: done 8981 episodes, mean reward 0.629, speed 65.82 f/s\n",
      "842541: done 8982 episodes, mean reward 0.633, speed 70.57 f/s\n",
      "843111: done 8983 episodes, mean reward 0.647, speed 69.26 f/s\n",
      "843281: done 8985 episodes, mean reward 0.651, speed 71.81 f/s\n",
      "843522: done 8986 episodes, mean reward 0.656, speed 66.91 f/s\n",
      "843699: done 8987 episodes, mean reward 0.659, speed 65.73 f/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24364/1662194233.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_crt_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                     \u001b[0mtwinq_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                     SAC_ENTROPY_ALPHA, device)\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mtb_tracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ref_v\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_vals_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\deepRLND\\deep-reinforcement-learning\\p3_collab-compet\\lib\\common.py\u001b[0m in \u001b[0;36munpack_batch_sac\u001b[1;34m(batch, val_net, twinq_net, policy_net, gamma, ent_alpha, device)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \"\"\"\n\u001b[0;32m     49\u001b[0m     \u001b[0mstates_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_q_v\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0munpack_batch_a2c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# references for the critic network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\deepRLND\\deep-reinforcement-learning\\p3_collab-compet\\lib\\common.py\u001b[0m in \u001b[0;36munpack_batch_a2c\u001b[1;34m(batch, net, last_val_gamma, device)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mnot_done_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mlast_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mstates_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mptan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mactions_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ptan\\agent.py\u001b[0m in \u001b[0;36mfloat32_preprocessor\u001b[1;34m(states)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfloat32_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mnp_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## the following code cell implimentation of SAC algorithm was taken and customly adjusted from one of my favorite books in Deep Reinforcement Learning by Maxim Lapan Chapter 19(i own a copy) GitHub link: https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition.git\n",
    "import os\n",
    "import ptan\n",
    "import gym\n",
    "import math\n",
    "import time\n",
    "import pybullet_envs\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "from lib import model, common, test_net\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distrib\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "LR_ACTS = 1e-4\n",
    "LR_VALS = 1e-4\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "SAC_ENTROPY_ALPHA = 0.1\n",
    "\n",
    "TEST_ITERS = 10000\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--cuda\", default=True, action='store_true', help='Enable CUDA')\n",
    "#     parser.add_argument(\"-n\", \"--name\", default='A', help=\"Name of the run\")\n",
    "#     parser.add_argument(\"-e\", \"--env\", default=ENV_ID, help=\"Environment id, default=\" + ENV_ID)\n",
    "# args = parser.parse_args()\n",
    "device = torch.device(\"cuda\" if True else \"cpu\")\n",
    "\n",
    "save_path = os.path.join(\"saves\", \"sac-\" + 'tennis')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "tennis_env = TennismEnv(env)\n",
    "test_env = TennismEnv(env)\n",
    "\n",
    "act_net = model.ModelActor(\n",
    "    state_size*2,\n",
    "    action_size*2).to(device)\n",
    "crt_net = model.ModelCritic(\n",
    "    state_size*2\n",
    ").to(device)\n",
    "twinq_net = model.ModelSACTwinQ(\n",
    "    state_size*2,\n",
    "    action_size*2).to(device)\n",
    "print(act_net)\n",
    "print(crt_net)\n",
    "print(twinq_net)\n",
    "\n",
    "tgt_crt_net = ptan.agent.TargetNet(crt_net)\n",
    "\n",
    "writer = SummaryWriter(comment=\"-sac_\" + 'tennis')\n",
    "agent = model.AgentDDPG(act_net, device=device)\n",
    "exp_source = ExperienceSourceFirstLast(\n",
    "    tennis_env, agent, gamma=GAMMA, steps_count=1)\n",
    "buffer = ExperienceReplayBuffer(\n",
    "    exp_source, buffer_size=REPLAY_SIZE)\n",
    "act_opt = optim.Adam(act_net.parameters(), lr=LR_ACTS)\n",
    "crt_opt = optim.Adam(crt_net.parameters(), lr=LR_VALS)\n",
    "twinq_opt = optim.Adam(twinq_net.parameters(), lr=LR_VALS)\n",
    "\n",
    "frame_idx = 0\n",
    "best_reward = None\n",
    "with RewardTracker(writer) as tracker:\n",
    "    with TBMeanTracker(\n",
    "            writer, batch_size=10) as tb_tracker:\n",
    "        while True:\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            rewards_steps = exp_source.pop_rewards_steps()\n",
    "            if rewards_steps:\n",
    "                rewards, steps = zip(*rewards_steps)\n",
    "                tb_tracker.track(\"episode_steps\", steps[0], frame_idx)\n",
    "                tracker.reward(rewards[0], frame_idx)\n",
    "\n",
    "            if len(buffer) < REPLAY_INITIAL:\n",
    "                continue\n",
    "\n",
    "            batch = buffer.sample(BATCH_SIZE)\n",
    "            states_v, actions_v, ref_vals_v, ref_q_v = \\\n",
    "                common.unpack_batch_sac(\n",
    "                    batch, tgt_crt_net.target_model,\n",
    "                    twinq_net, act_net, GAMMA,\n",
    "                    SAC_ENTROPY_ALPHA, device)\n",
    "\n",
    "            tb_tracker.track(\"ref_v\", ref_vals_v.mean(), frame_idx)\n",
    "            tb_tracker.track(\"ref_q\", ref_q_v.mean(), frame_idx)\n",
    "\n",
    "            # train TwinQ\n",
    "            twinq_opt.zero_grad()\n",
    "            q1_v, q2_v = twinq_net(states_v, actions_v)\n",
    "#             print(q1_v.shape, ref_q_v.shape)\n",
    "            q1_loss_v = F.mse_loss(q1_v.repeat(1,2),\n",
    "                                   ref_q_v.detach())\n",
    "            q2_loss_v = F.mse_loss(q2_v.repeat(1,2),\n",
    "                                   ref_q_v.detach())\n",
    "            q_loss_v = q1_loss_v + q2_loss_v\n",
    "            q_loss_v.backward()\n",
    "            twinq_opt.step()\n",
    "            tb_tracker.track(\"loss_q1\", q1_loss_v, frame_idx)\n",
    "            tb_tracker.track(\"loss_q2\", q2_loss_v, frame_idx)\n",
    "\n",
    "            # Critic\n",
    "            crt_opt.zero_grad()\n",
    "            val_v = crt_net(states_v)\n",
    "            v_loss_v = F.mse_loss(val_v.squeeze(),\n",
    "                                  ref_vals_v.detach())\n",
    "            v_loss_v.backward()\n",
    "            crt_opt.step()\n",
    "            tb_tracker.track(\"loss_v\", v_loss_v, frame_idx)\n",
    "\n",
    "            # Actor\n",
    "            act_opt.zero_grad()\n",
    "            acts_v = act_net(states_v)\n",
    "            q_out_v, _ = twinq_net(states_v, acts_v)\n",
    "            act_loss = -q_out_v.mean()\n",
    "            act_loss.backward()\n",
    "            act_opt.step()\n",
    "            tb_tracker.track(\"loss_act\", act_loss, frame_idx)\n",
    "\n",
    "            tgt_crt_net.alpha_sync(alpha=1 - 1e-3)\n",
    "\n",
    "            if frame_idx % TEST_ITERS == 0:\n",
    "                ts = time.time()\n",
    "                rewards, steps = [], []\n",
    "                for test_i in range(4):\n",
    "                    reward, step = test_net(act_net, test_env, device=device)\n",
    "                    rewards.append(float(np.mean(reward)))\n",
    "                    steps.append(step)\n",
    "                rewards = float(np.mean(rewards))\n",
    "                steps = int(np.mean(steps))\n",
    "                print(\"Test done in %.2f sec, reward %.3f, steps %d\" % (\n",
    "                    time.time() - ts, rewards, steps))\n",
    "                writer.add_scalar(\"test_reward\", rewards, frame_idx)\n",
    "                writer.add_scalar(\"test_steps\", steps, frame_idx)\n",
    "                if best_reward is None or best_reward < rewards:\n",
    "                    if best_reward is not None:\n",
    "                        print(\"Best reward updated: %.3f -> %.3f\" % (best_reward, rewards))\n",
    "                        name = \"best_%+.3f_%d.dat\" % (rewards, frame_idx)\n",
    "                        fname = os.path.join(save_path, name)\n",
    "                        torch.save(act_net.state_dict(), fname)\n",
    "                    best_reward = rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_log.txt\", 'r') as tr_log:\n",
    "    training_log = tr_log.read()\n",
    "\n",
    "episode_ids=[]\n",
    "episode_id=[]\n",
    "\n",
    "test_reward=[]\n",
    "for log in training_log.split('\\n'):\n",
    "    if 'Test done in' in log:\n",
    "        test_reward.append(float(log.split(',')[1].split(' ')[-1]))\n",
    "        episode_id.append(int(episode_ids[-1].split('episodes')[0].rstrip().split(' ')[-1]))\n",
    "    episode_ids.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.plot(episode_id, test_reward)\n",
    "plt.xscale('linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (average over agents) from episode 0: 2.600000038743019\n",
      "Score (average over agents) from episode 1: 2.650000039488077\n",
      "Score (average over agents) from episode 2: 2.650000039488077\n",
      "Score (average over agents) from episode 3: 2.600000038743019\n",
      "Score (average over agents) from episode 4: 2.650000039488077\n",
      "Score (average over agents) from episode 5: 2.650000039488077\n",
      "Score (average over agents) from episode 6: 2.600000038743019\n",
      "Score (average over agents) from episode 7: 2.650000039488077\n",
      "Score (average over agents) from episode 8: 2.650000039488077\n",
      "Score (average over agents) from episode 9: 2.600000038743019\n",
      "Score (average over agents) from episode 10: 2.650000039488077\n",
      "Score (average over agents) from episode 11: 2.650000039488077\n",
      "Score (average over agents) from episode 12: 2.600000038743019\n",
      "Score (average over agents) from episode 13: 2.650000039488077\n",
      "Score (average over agents) from episode 14: 2.650000039488077\n",
      "Score (average over agents) from episode 15: 2.600000038743019\n",
      "Score (average over agents) from episode 16: 2.650000039488077\n",
      "Score (average over agents) from episode 17: 2.650000039488077\n",
      "Score (average over agents) from episode 18: 2.600000038743019\n",
      "Score (average over agents) from episode 19: 2.650000039488077\n",
      "Score (average over agents) from episode 20: 2.650000039488077\n",
      "Score (average over agents) from episode 21: 2.600000038743019\n",
      "Score (average over agents) from episode 22: 2.650000039488077\n",
      "Score (average over agents) from episode 23: 2.650000039488077\n",
      "Score (average over agents) from episode 24: 2.600000038743019\n",
      "Score (average over agents) from episode 25: 2.650000039488077\n",
      "Score (average over agents) from episode 26: 2.650000039488077\n",
      "Score (average over agents) from episode 27: 2.600000038743019\n",
      "Score (average over agents) from episode 28: 2.650000039488077\n",
      "Score (average over agents) from episode 29: 2.650000039488077\n",
      "Score (average over agents) from episode 30: 2.600000038743019\n",
      "Score (average over agents) from episode 31: 2.650000039488077\n",
      "Score (average over agents) from episode 32: 2.650000039488077\n",
      "Score (average over agents) from episode 33: 2.600000038743019\n",
      "Score (average over agents) from episode 34: 2.650000039488077\n",
      "Score (average over agents) from episode 35: 2.650000039488077\n",
      "Score (average over agents) from episode 36: 2.600000038743019\n",
      "Score (average over agents) from episode 37: 2.650000039488077\n",
      "Score (average over agents) from episode 38: 2.650000039488077\n",
      "Score (average over agents) from episode 39: 2.600000038743019\n",
      "Score (average over agents) from episode 40: 2.650000039488077\n",
      "Score (average over agents) from episode 41: 2.650000039488077\n",
      "Score (average over agents) from episode 42: 2.600000038743019\n",
      "Score (average over agents) from episode 43: 2.650000039488077\n",
      "Score (average over agents) from episode 44: 2.650000039488077\n",
      "Score (average over agents) from episode 45: 2.600000038743019\n",
      "Score (average over agents) from episode 46: 2.650000039488077\n",
      "Score (average over agents) from episode 47: 2.650000039488077\n",
      "Score (average over agents) from episode 48: 2.600000038743019\n",
      "Score (average over agents) from episode 49: 2.650000039488077\n",
      "Score (average over agents) from episode 50: 2.650000039488077\n",
      "Score (average over agents) from episode 51: 2.600000038743019\n",
      "Score (average over agents) from episode 52: 2.650000039488077\n",
      "Score (average over agents) from episode 53: 2.650000039488077\n",
      "Score (average over agents) from episode 54: 2.600000038743019\n",
      "Score (average over agents) from episode 55: 2.650000039488077\n",
      "Score (average over agents) from episode 56: 2.650000039488077\n",
      "Score (average over agents) from episode 57: 2.600000038743019\n",
      "Score (average over agents) from episode 58: 2.650000039488077\n",
      "Score (average over agents) from episode 59: 2.650000039488077\n",
      "Score (average over agents) from episode 60: 2.600000038743019\n",
      "Score (average over agents) from episode 61: 2.650000039488077\n",
      "Score (average over agents) from episode 62: 2.650000039488077\n",
      "Score (average over agents) from episode 63: 2.600000038743019\n",
      "Score (average over agents) from episode 64: 2.650000039488077\n",
      "Score (average over agents) from episode 65: 2.650000039488077\n",
      "Score (average over agents) from episode 66: 2.600000038743019\n",
      "Score (average over agents) from episode 67: 2.650000039488077\n",
      "Score (average over agents) from episode 68: 2.650000039488077\n",
      "Score (average over agents) from episode 69: 2.600000038743019\n",
      "Score (average over agents) from episode 70: 2.650000039488077\n",
      "Score (average over agents) from episode 71: 2.650000039488077\n",
      "Score (average over agents) from episode 72: 2.600000038743019\n",
      "Score (average over agents) from episode 73: 2.650000039488077\n",
      "Score (average over agents) from episode 74: 2.650000039488077\n",
      "Score (average over agents) from episode 75: 2.600000038743019\n",
      "Score (average over agents) from episode 76: 2.650000039488077\n",
      "Score (average over agents) from episode 77: 2.650000039488077\n",
      "Score (average over agents) from episode 78: 2.600000038743019\n",
      "Score (average over agents) from episode 79: 2.650000039488077\n",
      "Score (average over agents) from episode 80: 2.600000038743019\n",
      "Score (average over agents) from episode 81: 2.650000039488077\n",
      "Score (average over agents) from episode 82: 2.650000039488077\n",
      "Score (average over agents) from episode 83: 2.650000039488077\n",
      "Score (average over agents) from episode 84: 2.600000038743019\n",
      "Score (average over agents) from episode 85: 2.650000039488077\n",
      "Score (average over agents) from episode 86: 2.650000039488077\n",
      "Score (average over agents) from episode 87: 2.600000038743019\n",
      "Score (average over agents) from episode 88: 2.650000039488077\n",
      "Score (average over agents) from episode 89: 2.650000039488077\n",
      "Score (average over agents) from episode 90: 2.600000038743019\n",
      "Score (average over agents) from episode 91: 2.650000039488077\n",
      "Score (average over agents) from episode 92: 2.650000039488077\n",
      "Score (average over agents) from episode 93: 2.600000038743019\n",
      "Score (average over agents) from episode 94: 2.650000039488077\n",
      "Score (average over agents) from episode 95: 2.650000039488077\n",
      "Score (average over agents) from episode 96: 2.600000038743019\n",
      "Score (average over agents) from episode 97: 2.650000039488077\n",
      "Score (average over agents) from episode 98: 2.650000039488077\n",
      "Score (average over agents) from episode 99: 2.600000038743019\n",
      "Score (average over agents) from episode 100: 2.650000039488077\n",
      "Score (average over agents) from episode 101: 2.650000039488077\n",
      "Score (average over agents) from episode 102: 2.600000038743019\n",
      "Score (average over agents) from episode 103: 2.650000039488077\n",
      "Score (average over agents) from episode 104: 2.650000039488077\n",
      "Score (average over agents) from episode 105: 2.600000038743019\n",
      "Score (average over agents) from episode 106: 2.650000039488077\n",
      "Score (average over agents) from episode 107: 2.650000039488077\n",
      "Score (average over agents) from episode 108: 2.600000038743019\n",
      "Score (average over agents) from episode 109: 2.650000039488077\n",
      "Score (average over agents) from episode 110: 2.650000039488077\n",
      "Score (average over agents) from episode 111: 2.600000038743019\n",
      "Score (average over agents) from episode 112: 2.650000039488077\n",
      "Score (average over agents) from episode 113: 2.650000039488077\n",
      "Score (average over agents) from episode 114: 2.600000038743019\n",
      "Score (average over agents) from episode 115: 2.650000039488077\n",
      "Score (average over agents) from episode 116: 2.650000039488077\n",
      "Score (average over agents) from episode 117: 2.600000038743019\n",
      "Score (average over agents) from episode 118: 2.650000039488077\n",
      "Score (average over agents) from episode 119: 2.650000039488077\n",
      "Score (average over agents) from episode 120: 2.600000038743019\n",
      "Score (average over agents) from episode 121: 2.650000039488077\n",
      "Score (average over agents) from episode 122: -0.004999999888241291\n",
      "Score (average over agents) from episode 123: 0.04500000085681677\n",
      "Score (average over agents) from episode 124: 2.600000038743019\n",
      "Score (average over agents) from episode 125: 2.650000039488077\n",
      "Score (average over agents) from episode 126: 2.650000039488077\n",
      "Score (average over agents) from episode 127: 2.600000038743019\n",
      "Score (average over agents) from episode 128: 2.650000039488077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (average over agents) from episode 129: 2.650000039488077\n",
      "Score (average over agents) from episode 130: 2.600000038743019\n",
      "Score (average over agents) from episode 131: 2.650000039488077\n",
      "Score (average over agents) from episode 132: 2.650000039488077\n",
      "Score (average over agents) from episode 133: 2.600000038743019\n",
      "Score (average over agents) from episode 134: 2.650000039488077\n",
      "Score (average over agents) from episode 135: 2.650000039488077\n",
      "Score (average over agents) from episode 136: 2.600000038743019\n",
      "Score (average over agents) from episode 137: 2.650000039488077\n",
      "Score (average over agents) from episode 138: 2.650000039488077\n",
      "Score (average over agents) from episode 139: 2.600000038743019\n",
      "Score (average over agents) from episode 140: 2.650000039488077\n",
      "Score (average over agents) from episode 141: 2.650000039488077\n",
      "Score (average over agents) from episode 142: 2.600000038743019\n",
      "Score (average over agents) from episode 143: 2.650000039488077\n",
      "Score (average over agents) from episode 144: 2.650000039488077\n",
      "Score (average over agents) from episode 145: 2.600000038743019\n",
      "Score (average over agents) from episode 146: 2.650000039488077\n",
      "Score (average over agents) from episode 147: 2.650000039488077\n",
      "Score (average over agents) from episode 148: 2.600000038743019\n",
      "Score (average over agents) from episode 149: 2.650000039488077\n",
      "Score (average over agents) from episode 150: 2.650000039488077\n",
      "Score (average over agents) from episode 151: 2.600000038743019\n",
      "Score (average over agents) from episode 152: 2.650000039488077\n",
      "Score (average over agents) from episode 153: 2.650000039488077\n",
      "Score (average over agents) from episode 154: 2.600000038743019\n",
      "Score (average over agents) from episode 155: 2.650000039488077\n",
      "Score (average over agents) from episode 156: 2.650000039488077\n",
      "Score (average over agents) from episode 157: 2.600000038743019\n",
      "Score (average over agents) from episode 158: 2.650000039488077\n",
      "Score (average over agents) from episode 159: 2.650000039488077\n",
      "Score (average over agents) from episode 160: 2.600000038743019\n",
      "Score (average over agents) from episode 161: 2.650000039488077\n",
      "Score (average over agents) from episode 162: 2.650000039488077\n",
      "Score (average over agents) from episode 163: 2.600000038743019\n",
      "Score (average over agents) from episode 164: 2.650000039488077\n",
      "Score (average over agents) from episode 165: 2.650000039488077\n",
      "Score (average over agents) from episode 166: 2.600000038743019\n",
      "Score (average over agents) from episode 167: 2.650000039488077\n",
      "Score (average over agents) from episode 168: 2.650000039488077\n",
      "Score (average over agents) from episode 169: 2.600000038743019\n",
      "Score (average over agents) from episode 170: 2.650000039488077\n",
      "Score (average over agents) from episode 171: 2.650000039488077\n",
      "Score (average over agents) from episode 172: 2.600000038743019\n",
      "Score (average over agents) from episode 173: 2.650000039488077\n",
      "Score (average over agents) from episode 174: 2.650000039488077\n",
      "Score (average over agents) from episode 175: 2.600000038743019\n",
      "Score (average over agents) from episode 176: 2.650000039488077\n",
      "Score (average over agents) from episode 177: 2.650000039488077\n",
      "Score (average over agents) from episode 178: 2.600000038743019\n",
      "Score (average over agents) from episode 179: 2.650000039488077\n",
      "Score (average over agents) from episode 180: -0.004999999888241291\n",
      "Score (average over agents) from episode 181: 2.600000038743019\n",
      "Score (average over agents) from episode 182: 2.650000039488077\n",
      "Score (average over agents) from episode 183: 2.600000038743019\n",
      "Score (average over agents) from episode 184: 2.650000039488077\n",
      "Score (average over agents) from episode 185: 2.650000039488077\n",
      "Score (average over agents) from episode 186: 2.600000038743019\n",
      "Score (average over agents) from episode 187: 2.650000039488077\n",
      "Score (average over agents) from episode 188: 2.650000039488077\n",
      "Score (average over agents) from episode 189: 2.600000038743019\n",
      "Score (average over agents) from episode 190: 2.650000039488077\n",
      "Score (average over agents) from episode 191: 2.650000039488077\n",
      "Score (average over agents) from episode 192: 0.04500000085681677\n",
      "Score (average over agents) from episode 193: 2.600000038743019\n",
      "Score (average over agents) from episode 194: 2.650000039488077\n",
      "Score (average over agents) from episode 195: 2.600000038743019\n",
      "Score (average over agents) from episode 196: 2.600000038743019\n",
      "Score (average over agents) from episode 197: 2.650000039488077\n",
      "Score (average over agents) from episode 198: 2.650000039488077\n",
      "Score (average over agents) from episode 199: 2.600000038743019\n",
      "Average score (over agents) for last 100 episodes 2.5278000376746057\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "best_act_net = model.ModelActor(\n",
    "    state_size*2,\n",
    "    action_size*2).to(device)\n",
    "\n",
    "best_act_net.load_state_dict(torch.load('saves/sac-tennis/best_+2.634_370000.dat'))\n",
    "best_act_net.eval()\n",
    "reward_buffer = deque(maxlen=100)\n",
    "for i in range(200):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = best_act_net(torch.tensor(states).to('cuda').flatten().view(1,-1).float()) # select an action (for each agent)\n",
    "        actions = np.clip(actions.detach().cpu().numpy(), -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    reward_buffer.append(np.mean(scores))\n",
    "    print('Score (average over agents) from episode {}: {}'.format(i, np.mean(scores)))\n",
    "print('Average score (over agents) for last 100 episodes {}'.format(np.mean(reward_buffer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
